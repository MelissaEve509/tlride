---
title: "A guided tour in targeted learning territory"
author: "David Benkeser, Antoine Chambaz, Nima Hejazi"
date: "08/06/2018"
encoding: "UTF-8"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    includes:
      in_header: dan_header.tex
  latex_engine: pdflatex
  citation_package: natbib
---

<!-- to compile the file, run

R -e "rmarkdown::render('dan.Rmd', encoding='UTF-8')"

-->


```{r setup, echo = FALSE}
knitr::opts_chunk$set(
  size = "tiny",
  warnings = FALSE,
  fig.width = 12, 
  fig.height = 4, 
  fig.path = 'img/')
```

\section{Introduction}

\tcg{This is a very first draft  of our article. The current *tentative* title
  is "A guided tour in targeted learning territory".}

\tcg{Explain our objectives and how we will meet them. Explain that the symbol
\textdbend indicates more delicate material.}

\tcg{Use sectioning a lot to ease cross-referencing.}

\tcg{Do we include exercises?}

```{r visible-setup}
set.seed(54321) ## because reproducibility matters...
suppressMessages(library(R.utils)) ## make sure it is installed
suppressMessages(library(ggplot2)) ## make sure it is installed
expit <- plogis
logit <- qlogis
```


\section{A simulation study}
\label{sec:simulation:study}

blabla

\subsection{Reproducible experiment as a law.}
\label{subsec:as:a:law}

We are interested in a reproducible experiment. The generic summary of how one
realization of the experiment unfolds, our observation, is called $O$. We view
$O$  as a  random variable  drawn from  what we  call the  law $P_{0}$  of the
experiment.  The  law $P_{0}$  is viewed  as an  element of  what we  call the
model. Denoted  by $\calM$, the model  is the collection of  \textit{all} laws
from which  $O$ can be drawn  and that meet some  constraints. The constraints
translate the knowledge  we have about the experiment. The  more we know about
the experiment,  the smaller is  $\calM$. In  all our examples,  model $\calM$
will put very few restrictions on the candidate laws.

Consider the following chunk of code:
```{r simulation}
drawFromExperiment <- function(n, full = FALSE) {
  ## preliminary
  n <- Arguments$getInteger(n, c(1, Inf))
  full <- Arguments$getLogical(full)
  ## ## 'gbar' and 'Qbar' factors
  gbar <- function(W) {
    expit(-0.3 + 2 * W - 1.5 * W^2)
  }
  Qbar <- function(AW) {
    A <- AW[, 1]
    W <- AW[, 2]
    A * cos(2 * pi * W) + (1 - A) * sin(2 * pi * W^2)
  }
  ## sampling
  ## ## context
  W <- runif(n)
  ## ## counterfactual rewards
  zeroW <- cbind(A = 0, W)
  oneW <- cbind(A = 1, W)
  Yzero <- exp( -abs(rnorm(n, mean = Qbar(zeroW), sd = 1)) )
  Yone <- exp( -abs(rnorm(n, mean = Qbar(oneW), sd = 1) )
  ## ## action undertaken
  A <- rbinom(n, size = 1, prob = gbar(W))
  ## ## actual reward
  Y <- A * Yone + (1 - A) * Yzero
  ## ## observation
  if (full) {
    obs <- cbind(W = W, Yzero = Yzero, Yone = Yone, A = A, Y = Y)
  } else {
    obs <- cbind(W = W, A = A, Y = Y)
  }
  attr(obs, "gbar") <- gbar
  attr(obs, "Qbar") <- Qbar
  attr(obs, "QW") <- dunif
  ##
  return(obs)
}
```

We can  interpret `drawFromExperiment` as a  law $P_{0}$ since we  can use the
function to sample observations  from a common law.  It is  even a little more
than that, because we can tweak the experiment, by setting its `full` argument
to  `TRUE`, in  order  to  get what  appear  as intermediary  (counterfactual)
variables  in  the regular  experiment.   The  next  chunk  of code  runs  the
(regular) experiment five times independently: 
```{r draw-five-obs} 
(five.obs <- drawFromExperiment(5))
``` 
We can view the `attributes` of object `five.obs` because, in this section, we
act  as  oracles,  \textit{i.e.},  we   know  completely  the  nature  of  the
experiment.  From a probabilistic point of view, the attributes `gbar`, `Qbar`
and  `QW` are  infinite-dimensional features  of  $P_{0}$.  There  is more  to
$P_{0}$ than  $\gbar_{0}$ (`gbar`), $\Qbar_{0}$ (`Qbar`),  formally defined by
\begin{equation}  \gbar_{0} (W)  \equiv P_{0}  (A=1|W), \quad  \Qbar_{0} (A,W)
\equiv  E_{P_{0}} (Y  | A,  W), \end{equation}  and the  marginal distribution
$Q_{0,W}$  of  $W$   under  $P_{0}$  (`QW`),  for   instance  the  conditional
distribution  (not  expectation)  of   $Y$  given  $(A,W)$,  but  $\gbar_{0}$,
$\Qbar_{0}$ and $Q_{0,W}$ will play a prominent role in our story.

\subsection{The parameter of interest, first pass.}
\label{subsec:parameter:first}

It  happens  that we  especially  care  for  a finite-dimensional  feature  of
$P_{0}$  that   we  denote  by   $\psi_{0}$.   Its  definition   involves  the
aforementioned infinite-dimensional features: 
\begin{align}
\label{eq:psi0}
  \psi_{0} 
  &\equiv E_{P_{0}} \left(\Qbar_{0}(1, W) - \Qbar_{0}(0, W)\right)\\
\notag
&= \int \left(\Qbar_{0}(1, w) - \Qbar_{0}(0, w)\right) dQ_{0,W}(w).
\end{align}
Acting as oracles, we can compute explicitely the numerical value of
$\psi_{0}$. 

Our  interest in  $\psi_{0}$ is  of  causal nature.  Taking a  closer look  at
`drawFromExperiment` reveals indeed  that the random making  of an observation
$O$ drawn  from $P_{0}$ can  be summarized by  the following causal  graph and
nonparametric system of structural equations:
```{r DAG}
## plot the causal diagram
```
and, for some  deterministic functions $f_w$, $f_a$,  $f_y$ and independent
sources of randomness $U_w$, $U_a$, $U_y$,
\begin{enumerate}
\item sample  the context where the  rest of the experiment
  will take place, $W = f_{w}(U_w)$;
\item  sample the  two counterfactual  rewards of  the two
  actions  that   can  be   undertaken,  $Y_{0}  =   f_{y}(0,  W,   U_y)$  and
  $Y_{1} = f_{y}(1, W, U_y)$;
\item\label{item:A:equals} sample   which  action is carried
  out in the given context, $A = f_{a} (W, U_a)$;
\item    define        the    corresponding    reward,
  $Y = A Y_{1} + (1-A) Y_{0}$;
\item summarize the course of the experiment  with the observation $O = (W, A,
  Y)$, thus concealing $Y_{0}$ and $Y_{1}$. 
\end{enumerate}

The above description of the  experiment `drawFormExperiment` is useful to ram
home what it  means to run the "full" experiment  by setting argument `full`
to `TRUE` in a call to  `drawFormExperiment`. Doing so triggers a modification
of the  nature of  the experiment, enforcing  that the  counterfactual rewards
$Y_{0}$ and  $Y_{1}$ be part of  the summary of the  experiment eventually. In
light  of the  above enumeration,  $\bbO \equiv  (W, Y_{0},  Y_{1}, A,  Y)$ is
output, as opposed to its summary measure $O$. This defines another experiment
and its law, that we denote $\bbP_{0}$. 

It is well known \tcg{(do we give the proof or refer to other articles?)} that
\begin{equation*}   \psi_{0}  =   E_{\bbP_{0}}  \left(Y_{1}   -  Y_{0}\right).
\end{equation*} Thus, $\psi_{0}$ compares (additively) the averages of the two
counterfactual rewards.  In other words, $\psi_{0}$  quantifies the difference
in average  of the  reward one  would get in  a world  where one  would always
enforce action $a=1$ with the reward one  would get in a world where one would
always  enforce  action  $a=0$.   This  said, it  is  worth  emphasizing  that
$\psi_{0}$ is a well defined parameter beyond its causal interpretation.

To conclude this subsection, we draw  advantage from the possibility to sample
full observations from `drawFromExperiment` by  setting its argument `full` to
`TRUE` in  order to numerically  approximate $\psi_{0}$.  By the law  of large
numbers, the following chunk of code approximates $\psi_{0}$:
```{r approx-psi-zero-a}
B <- 1e6
full.obs <- drawFromExperiment(B, full = TRUE)
(psi.hat <- mean(full.obs[, "Yone"] - full.obs[, "Yzero"]))
```
In fact,  the central limit  theorem and Slutsky's lemma  allow us to  build a
confidence interval with asymptotic level 95\% for $\psi_{0}$:
```{r approx-psi-zero-b}
sd.hat <- sd(full.obs[, "Yone"] - full.obs[, "Yzero"])
alpha <- 0.05
(psi.CI <- psi.hat + c(-1, 1) * qnorm(1 - alpha / 2) * sd.hat / sqrt(B))
```

\subsection{The parameter of interest, second pass.}
\label{subsec:parameter:second}

Suppose we  know beforehand that  $O$ drawn from  $P_{0}$ takes its  values in
$\calO \equiv  [0,1] \times \{0,1\}  \times [0,1]$ and that  $P_{0}(A=1|W)$ is
bounded  away from  zero and  one $Q_{0,W}$-almost  surely (this  is the  case
indeed).  Then  we can  define model  $\calM$ as the  set of  all laws  $P$ on
$\calO$ such that $\gbar(W) \equiv P(A=1|W)$ is bounded away from zero and one
$Q_{W}$-almost surely, where $Q_{W}$ is the marginal distribution of $W$
under $P$.  

Let us also define generically $\Qbar$ as \begin{equation*} \Qbar (A,W) \equiv
E_{P} (Y|A, W). \end{equation*} Central  to our approach is viewing $\psi_{0}$
as the  value at  $P_{0}$ of  the statistical mapping  $\Psi$ from  $\calM$ to
$[0,1]$ characterized  by \begin{align*} \Psi(P) &\equiv  E_{P} \left(\Qbar(1,
W)  - \Qbar(0,  W)\right)\\ &=  \int  \left(\Qbar(1, w)  - \Qbar(0,  w)\right)
dQ_{W}(w), \end{align*}  a clear extension of  \eqref{eq:psi0}.  For instance,
although the  law $\Pi_{0}  \in \calM$ encoded  by default  (\textit{i.e.}, with
`h=0`)  in  `drawFromAnotherExperiment`  defined below  differs  starkly  from
$P_{0}$,
```{r another_simulation}
drawFromAnotherExperiment <- function(n, h = 0) {
  ## preliminary
  n <- Arguments$getInteger(n, c(1, Inf))
  h <- Arguments$getNumeric(h)
  ## ## 'gbar' and 'Qbar' factors
  gbar <- function(W) {
    sin((1 + W) * pi / 6)
  }
  Qbar <- function(AW, hh = h) {
    A <- AW[, 1]
    W <- AW[, 2]
    expit( logit( A *  W + (1 - A) * W^2 ) +
           hh * 10 * sqrt(W) * A )
  }
  ## sampling
  ## ## context
  W <- rbeta(n, shape1 = 2, shape2 = 2)
  ## ## action undertaken
  A <- rbinom(n, size = 1, prob = gbar(W))
  ## ## reward
  QAW <- Qbar(cbind(A, W))
  Y <- rbeta(n, shape1 = 1, shape2 = (1 - QAW) / QAW)
  ## ## observation
  obs <- cbind(W = W, A = A, Y = Y)
  attr(obs, "gbar") <- gbar
  attr(obs, "Qbar") <- Qbar
  attr(obs, "QW") <- dunif
  ##
  return(obs)
}
```
parameter $\Psi(\Pi_{0})$  is well defined, and  approximated by `psi.Pi.zero`
in the following chunk of code:
```{r approx-psi-one}
obs.from.another.experiment <- drawFromAnotherExperiment(1)
integrand <- function(w) {
  Qbar <- attr(obs.from.another.experiment, "Qbar")
  QW <- attr(obs.from.another.experiment, "QW")
  ( Qbar(cbind(1, w)) - Qbar(cbind(0, w)) ) * QW(w)
}
(psi.Pi.zero <- integrate(integrand, lower = 0, upper = 1)$val)
```
(easy algebra reveals that $\Psi(\Pi_{0}) = 1/6$ indeed).

Luckily, the statistical mapping $\Psi$ is well behaved, or smooth.  Here,
this colloquial expression refers to the fact that, for each $P \in \calM$, if
$P_{h} \to_{h} P$ in  $\calM$ from a direction $s$ when  the real parameter $h
\to 0$,  then not  only $\Psi(P_{h}) \to_{h}  \Psi(P)$ (continuity),  but also
$h^{-1} [\Psi(P_{h}) - \Psi(P)] \to_{h} c$,  where the real number $c$ depends
on $P$ and $s$ (differentiability).


For   instance,   let   $\Pi_{h}   \in   \calM$  be   the   law   encoded   in
`drawFromAnotherExperiment`  with `h`  ranging over  $[-1,1]$.  We  will argue
shortly that $\Pi_{h} \to_{h} \Pi_{0}$ in $\calM$ from a direction $s$ when $h
\to 0$.  The following chunk  of code evaluates and represents $\Psi(\Pi_{h})$
for $h$ ranging in a discrete approximation of $[-1,1]$:
```{r psi-approx-psi-one}
approx <- seq(-1, 1, length.out = 1e2)
psi.Pi.h <- sapply(approx, function(t) {
  obs.from.another.experiment <- drawFromAnotherExperiment(1, h = t)
  integrand <- function(w) {
    Qbar <- attr(obs.from.another.experiment, "Qbar")
    QW <- attr(obs.from.another.experiment, "QW")
    ( Qbar(cbind(1, w)) - Qbar(cbind(0, w)) ) * QW(w)
  }
  integrate(integrand, lower = 0, upper = 1)$val  
})
slope <- (psi.Pi.h - psi.Pi.zero) / approx
slope <- slope[min(which(approx > 0))]
ggplot() +
  geom_point(data = data.frame(x = approx, y = psi.Pi.h), aes(x, y),
             color = "#CC6666") +
  geom_segment(aes(x = -1, y = psi.Pi.zero - slope, xend = 1, yend = psi.Pi.zero + slope),
               arrow = arrow(length = unit(0.03, "npc")),
               color = "#9999CC") +
  geom_vline(xintercept = 0, color = "#66CC99") +
  geom_hline(yintercept = psi.Pi.zero, color = "#66CC99") +
  labs(x = "h", y = expression(Psi(Pi[h]))) 
```

The dotted curve  represents the function $h \mapsto  \Psi(\Pi_{h})$. The blue
line represents  the tangent to the  previous curve at $h=0$,  which is indeed
differentiable around $h=0$.  It is  derived by simple geometric arguments. In
the  next  subsection,  we formalize  what  it  means  to  be smooth  for  the
statistical mapping $\Psi$. Once the presentation is complete, we will be able
to derive a  closed-form expression for the  slope of the blue  curve from the
chunk of code where `drawFromAnotherExperiment` is defined.

\subsection{\textdbend Being smooth.}
\label{subsec:parameter:third}

Let us now describe what it means  for statistical mapping $\Psi$ to be smooth
at  every $P  \in \calM$.   The description  necessitates the  introduction of
fluctuations.

For every direction\footnote{A direction is a measurable function.} $s : \calO
\to \bbR$ such that  $s \neq 0$\footnote{That is, $s(O)$ is  not equal to zero
$P$-almost surely.},  $E_{P} (s(O))  = 0$  and $s$ bounded  by, say,  $M$, for
every $t \in T  = ]-M^{-1},M^{-1}[$, we can define a law  $P_{t} \in \calM$ by
setting $P_{t}  \ll P$\footnote{That is,  $P_{t}$ is  dominated by $P$:  if an
event $A$ satisfies $P(A) = 0$, then necessarily $P_{t} (A) = 0$ too.}  and

\begin{equation}\label{eq:fluct}\frac{dP_{t}}{dP}(O)      =     1      +     t
s(O),\end{equation} 

that is, $P_{t}$ has density $(1 + t s)$ with respect to (w.r.t.) $P$. We call
$\{P_{t}  :  t  \in  T\}$  a  fluctuation of  $P$  in  direction  $s$  because

\begin{equation}\label{eq:score}(i)  \;  P_{t}|_{t=0}  =   P,  \quad  (ii)  \;
\left.\frac{d}{dt}       \log        \frac{dP_{t}}{dP}(O)\right|_{t=0}       =
s(O).\end{equation} 

The fluctuation is a one-dimensional parametric submodel of $\calM$. 

Statistical mapping $\Psi$ is smooth at  every $P \in \calM$ because, for each
$P \in \calM$, there exists  a so called efficient influence curve\footnote{It
is  a   measurable  function.}   $D^{*}(P)   :  \calO  \to  \bbR$   such  that
$E_{P}(D^{*}(P)(O)) = 0$ and, for any direction  $s$ as above, if $\{P_{t} : t
\in T\}$  is defined as in  \eqref{eq:fluct}, then the real-valued  mapping $t
\mapsto \Psi(P_{t})$ is differentiable at $t=0$, with a derivative equal to

\begin{equation}\label{eq:derivative}E_{P} \left(D^{*}(P)(O) s(O)\right).\end{equation}

Interestingly,   if  a   fluctuation   $\{P_{t}  :   t   \in  T\}$   satisfies
\eqref{eq:score} for  a direction $s$ such  that $s\neq 0$, $E_{P}(s(O))  = 0$
and  $\Var_{P}  (s(O))  <  \infty$,  then $t  \mapsto  \Psi(P_{t})$  is  still
differentiable  at  $t=0$ with  a  derivative  equal to  \eqref{eq:derivative}
(beyond fluctuations of the form \eqref{eq:fluct}).

The influence curves $D^{*}(P)$ convey valuable information about $\Psi$. For
instance,  an  important  result  from   the  theory  of  inference  based  on
semiparametric models  guarantees that if $\psi_{n}$  is a regular\footnote{We
can view  $\psi_{n}$ as the  by product of  an algorithm $\Psihat$  trained on
independent observations $O_{1}$, \ldots, $O_{n}$ drawn from $P$.  
% or, equivalently, trained on the empirical measure $P_{n} = n^{-1}
% \sum_{i=1}^{n} \Dirac(O_{i})$: $\psi_{n} = \Psihat(P_{n})$.  
The estimator is regular at $P$ (w.r.t. the maximal tangent space) if, for any
direction  $s\neq 0$  such that  $E_{P}  (s(O)) =  0$ and  $\Var_{P} (s(O))  <
\infty$ and fluctuation $\{P_{t} : t \in T\}$ satisfying \eqref{eq:score}, the
estimator $\psi_{n,1/\sqrt{n}}$ of $\Psi(P_{1/\sqrt{n}})$ obtained by training
$\Psihat$  on independent  observations  $O_{1}$, \ldots,  $O_{n}$ drawn  from
$P_{1/\sqrt{n}}$    is   such    that    $\sqrt{n}   (\psi_{n,1/\sqrt{n}}    -
\Psi(P_{1/\sqrt{n}}))$ converges  in law to  a limit  that does not  depend on
$s$.} estimator  of $\Psi(P)$  built from  $n$ independent  observations drawn
from $P$, then the asymptotic variance  of the centered and rescaled $\sqrt{n}
(\psi_{n} - \Psi(P))$ cannot be smaller  than the variance of the $P$-specific
efficient influence curve, that is,

\begin{equation}\label{eq:CR}\Var_{P}(D^{*}(P)(O)).\end{equation}

In   this   light,   an   estimator    $\psi_{n}$   of   $\Psi(P)$   is   said
\textit{asymptotically efficient} at $P$ if it is regular at $P$ and such that
$\sqrt{n} (\psi_{n} - \Psi(P))$ converges in  law to the centered Gaussian law
with variance \eqref{eq:CR}.

\subsection{The parameter of interest, third pass.}
\label{subsec:parameter:third}

It is not difficult to check \tcg{(do  we give the proof?)} that the efficient
influence curve  $D^{*}(P)$ of $\Psi$ at  $P \in \calM$ writes  as $D^{*}(P) =
D_{1}^{*} (P) +  D_{2}^{*} (P)$ where $D_{1}^{*} (P)$ and  $D_{2}^{*} (P)$ are
given by

\begin{align*}D_{1}^{*}(P) (O) &= \Qbar(1,W) - \Qbar(0,W) - \Psi(P),\\
D_{2}^{*}(P) (O) &= \frac{2A-1}{\ell\gbar(A,W)} (Y - \Qbar(A,W)),\end{align*}

with  shorthand notation $\ell\gbar(A,W) = A\gbar(W) + (1-A) (1-\gbar(W))$.  

```{r recover-slope}
eic <- function(obs, psi) {
  Qbar <- attr(obs, "Qbar")
  gbar <- attr(obs, "gbar")
  QAW <- Qbar(obs[, c("A", "W")])
  gW <- gbar(obs[, "W"])
  lgAW <- obs[, "A"] * gW + (1 - obs[, "A"]) * (1 - gW)
  ( Qbar(cbind(1, obs[, "W"])) - Qbar(cbind(0, obs[, "W"])) - psi ) +
    (2 * obs[, "A"] - 1) / lgAW * (obs[, "Y"] - QAW)
}


s <- function(obs) {
  Qbar <- attr(obs, "Qbar")
  QAW <- Qbar(obs[, c("A", "W")])
  - (1 - QAW) / QAW * 10 * sqrt(obs[, "W"]) * obs[, "A"] *
    log(1 - obs[, "Y"]) - 10 * sqrt(obs[, "W"]) * obs[, "A"]
}

obs.from.another.experiment <- drawFromAnotherExperiment(B)
(mean( eic(obs.from.another.experiment, psi = 1/6) * s(obs.from.another.experiment) ))

```
