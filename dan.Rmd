# Introduction {#introduction}

Our ambition is to present a gentle introduction to the inference of a causal
quantity whose statistical analysis is typical  and thus paves the way to more
involved analyses.   The introduction  weaves together  two main  threads, one
theoretical and the other computational.

## A causal story.  {#causal-story}

We focus on a causal story where a  random reward (a real number between 0 and
1) depends  on the action  undertaken (one among  two) and the  random context
where  the action  is performed  (summarized by  a real  number between  0 and
1).  The causal  quantity of  interest is  the average  difference of  the two
counterfactual rewards. 

We  will  build  several  estimators  and  discuss  their  respective  merits,
theoretically  and  computationally. The  construction  of  the most  involved
estimator will  unfold in  *targeted learning territory*,  at the  frontier of
machine  learning and  semiparametrics,  the statistical  theory of  inference
based on semiparametric models.

## The `guided.tour.tmle` package. {#gttmle-package}

The  computational illustrations  will  be developed  based  on the  companion
package `guided.tour.tmle`. Make  sure you have installed it,  for instance by
running the following chunk of code:

```{r install-package, eval = FALSE}
devtools::install_github("achambaz/gttmle/guided.tour.tmle")
```

Note that additional  packages are required, among  which `tidyverse` [@r4ds],
`caret` and  `ggdag`. Assuming that  these are installed  too, we can  run the
next chunk of code:

```{r visible-setup, results = FALSE}
set.seed(54321) ## because reproducibility matters...
library(tidyverse)
library(caret)
library(ggdag)
library(guided.tour.tmle)
```



## To remove at a later stage. {#remove}

```{r redo}
redo_fixed <- c(TRUE, FALSE)[1]
redo_varying <- c(TRUE, FALSE)[1]
## if  'redo_$'  then  recompute  'learned_features_$_sample_size',  otherwise
## upload it if it is not already in the environment.
if (!redo_fixed) {
  if (!exists("learned_features_fixed_sample_size")) {
    learned_features_fixed_sample_size <-
      load("data/learned_features_fixed_sample_size_new.xdr")
  }
}
if (!redo_varying) {
  if (!exists("learned_features_varying_sample_size")) {
    learned_features_varying_sample_size <-
      load("data/learned_features_varying_sample_size_new.xdr")
  }
} 
```

# A simulation study {#simulation-study}

## Reproducible experiment as a law. {#reproducible-experiment}

We are interested in a reproducible  experiment. Every time this experiment is
run, it  generates an observation that  we call $O$.  We view $O$ as  a random
variable drawn from *the law of the experiment* that we denote by $P_{0}$.  

We  view $P_{0}$  as  an element  of  *the  model* $\calM$.   The  model is  a
collection of laws.  In particular, the  model contains all laws that we think
may plausibly describe  the law of the experiment.  Thus,  the choice of model
is based on our scientific knowledge of the experiment. The more we know about
the experiment,  the smaller is  $\calM$.  In all  our examples, we  use large
models that reflect a lack of knowledge about many aspects of the experiment.

## A synthetic reproducible experiment. {#synthetic-experiment}

Instead  of considering  a  real-life reproducible  experiment,  we focus  for
pedagogical purposes  on a  *synthetic* reproducible  experiment. Thus  we can
from  now  on  take on  two  different  roles:  that  of an  *oracle*  knowing
completely the nature  of the experiment, and that of  a statistician eager to
know more about the experiment by observing some of its outputs. 

Let us run the example built into the `guided.tour.tmle` package:

```{r example-one, results = FALSE}
example(guided.tour.tmle)
```

A few objects have been defined:

```{r example-two}
ls()
```

Function `expit` implements the link function  $\expit : \bbR \to ]0,1[$ given
by  $\expit(x) \equiv  (1 +  e^{-x})^{-1}$.  Function  `logit` implements  its
inverse function  $\logit : ]0,1[  \to \bbR$  given by $\logit(p)  \equiv \log
[p/(1-p)]$.  

Let us take a look at `experiment`:

```{r view-experiment}
experiment
```

The law $P_{0}$ of the synthetic experiment `experiment` built by us generates
a generic observation $O$ that decomposes as \begin{equation*} O \equiv (W, A,
Y) \in [0,1] \times \{0,1\} \times [0,1].  \end{equation*} We interpret $W$ as
a real valued summary  measure of a random context where  an action $A$ chosen
among two is undertaken, leading to a real valued reward $Y$.

We can  sample from the experiment  (simply run `?sample_from` to  see the man
page of method `sample_from`). The next chunk of code runs the experiment five
times, independently:

```{r draw-five-obs} 
(five_obs <- sample_from(experiment, n = 5))
``` 


We can interpret `run_experiment` as a law $P_{0}$ since we can use the
function to sample observations  from a common law.  It is  even a little more
than  that, because  we  can  tweak the  experiment,  by  setting its  `ideal`
argument   to  `TRUE`,   in  order   to  get   what  appear   as  intermediary
(counterfactual) variables in the regular  experiment.  The next chunk of code
runs  the  (regular)  experiment  five times  independently  and  outputs  the
resulting observations:


We can view the `attributes` of object `five_obs` because, in this section, we
act  as  oracles,  \textit{i.e.},  we   know  completely  the  nature  of  the
experiment. In  particular, we  have included several  features of  $P_0$ that
play an important  role in our developments. The attribute  `QW` describes the
density of $W$,  of which the law  $Q_{0,W}$ is a mixture of  the uniform laws
over $[0,1]$ (weight $1/10$)  and $[11/30,14/30]$ (weight $9/10$).\footnote{We
fine-tuned (or tweaked,  or something else?)  the marginal law  of $W$ to make
it easier later on to drive home important messages.  Specifically, $\ldots{}$
(do we explain what happens?)}  The attribute `Gbar` describes the conditional
probability of action $A = 1$ given  $W$.  For each $a \in \{0,1\}$, we denote
$\Gbar_0(W)  \equiv  \Pr_{P_0}(A  =  1  |  W)$  and  $\ell\Gbar_0(a,W)  \equiv
\Pr_{P_0}(A = a | W)$.   Obviously, $\ell\Gbar_{0}(A,W) \equiv A\Gbar_{0}(W) +
(1-A) (1-\Gbar_{0}(W))$.  The attribute `qY` describes the conditional density
of $Y$ given $A$ and $W$.  For  each $y\in ]0,1[$, we denote by $q_{0,Y}(y, A,
W)$  the conditional  density  evaluated at  $y$  of $Y$  given  $A$ and  $W$.
Similarly, the  attribute `Qbar` describes  the conditional mean of  $Y$ given
$A$  and  $W$,   and  we  denote  $\Qbar_0(A,W)   =  \Exp_{P_{0}}(Y|A,W)$  the
conditional mean of $Y$ given $A$ and $W$.

# &#9881; Visualization {#exo-visualization}

1.  Run the following chunk of code.  It visualizes the conditional mean
   $\Qbar_{0}$.

```{r exercise:visualize, eval = TRUE}
some_relevant_features <- reveal(experiment)
Gbar <- some_relevant_features$Gbar
Qbar <- some_relevant_features$Qbar
QW <- some_relevant_features$QW

features <- tibble(w = seq(0, 1, length.out = 1e3)) %>%
  mutate(Qw = QW(w),
         Gw = Gbar(w),
         Q1w = Qbar(cbind(A = 1, W = w)),
         Q0w = Qbar(cbind(A = 0, W = w)),
         blip_Qw = Q1w - Q0w)

features %>% select(-Qw, -Gw) %>%
  rename("Q(1,.)" = Q1w,
         "Q(0,.)" = Q0w,
         "Q(1,.) - Q(0,.)" = blip_Qw) %>%
  gather("f", "value", -w) %>%
  ggplot() +
  geom_line(aes(x = w, y = value, color = f), size = 1) +
  labs(y = "f(w)", title = bquote("Visualizing" ~ bar(Q)[0])) +
  ylim(NA, 1)
```

2. Adapt the  above chunk of code to visualize  the marginal density $Q_{0,W}$
   and conditional probability $\Gbar_{0}$.

# The parameter of interest, first pass {#parameter-first-pass}

It happens that we especially care for a finite-dimensional feature of $P_{0}$
that  we   denote  by  $\psi_{0}$.    Its  definition  involves  two   of  the
aforementioned  infinite-dimensional  features: \begin{align}  \label{eq:psi0}
\psi_{0}  &\equiv   \int  \left(\Qbar_{0}(1,   w)  -   \Qbar_{0}(0,  w)\right)
dQ_{0,W}(w)\\  \notag &=  \Exp_{P_{0}} \left(\Exp_{P_0}(Y  \mid  A =  1, W)  -
\Exp_{P_0}(Y \mid  A = 0, W)  \right).  \end{align} Acting as  oracles, we can
compute explicitly the numerical value of $\psi_{0}$.

```{r approx-psi-0-a-one}
(psi_zero <- evaluate_psi(experiment))
```

Our  interest in  $\psi_{0}$ is  of  causal nature.  Taking a  closer look  at
`run_experiment` reveals indeed  that the random making  of an observation
$O$ drawn  from $P_{0}$ can  be summarized by  the following causal  graph and
nonparametric system of structural equations:

(ref:DAG) Causal graph  summarizing the inner causal mechanism  at play in
`run_experiment`. 

```{r DAG, out.width = '70%', fig.align = 'center', fig.width = 8, fig.height = 6, fig.cap = '(ref:DAG)'}
dagify(
  Y ~ A + Y1 + Y0, A ~ W, Y1 ~ W, Y0 ~ W,
  labels = c(Y = "Actual reward",
             A = "Action",
             Y1 = "Counterfactual reward\n of action 1",
             Y0 = "Counterfactual reward\n of action 0",
             W = "Context of action"),
  coords = list(
    x = c(W = 0, A = -1, Y1 = 1.5, Y0 = 0.25, Y = 1),
    y = c(W = 0, A = -1, Y1 = -0.5, Y0 = -0.5, Y = -1)),
  outcome = "Y",
  exposure = "A",
  latent = c("Y0", "Y1")) %>% tidy_dagitty %>%
  ggdag(text = TRUE, use_labels = "label") + theme_dag_grey()
```

and, for some deterministic functions $f_w$, $f_a$, $f_y$ and independent
sources of randomness $U_w$, $U_a$, $U_y$,
\begin{enumerate}
\item sample the  context where the counterfactual rewards  will be generated,
the action  will be undertaken  and the actual reward  will be obtained,  $W =
f_{w}(U_w)$; 
\item  sample the  two counterfactual  rewards of  the two
  actions  that   can  be   undertaken,  $Y_{0}  =   f_{y}(0,  W,   U_y)$  and
  $Y_{1} = f_{y}(1, W, U_y)$;
\item\label{item:A:equals} sample   which  action is carried
  out in the given context, $A = f_{a} (W, U_a)$;
\item    define        the    corresponding    reward,
  $Y = A Y_{1} + (1-A) Y_{0}$;
\item summarize the course of the experiment  with the observation $O = (W, A,
  Y)$, thus concealing $Y_{0}$ and $Y_{1}$. 
\end{enumerate}

The above  description of the  experiment `run_experiment` is  useful to
reinforce what it means to run the "ideal" experiment by setting argument `ideal`
to  `TRUE`  in   a  call  to  `run_experiment`.  Doing   so  triggers  a
modification   of  the   nature  of   the  experiment,   enforcing  that   the
counterfactual  rewards $Y_{0}$  and $Y_{1}$  be part  of the  summary of  the
experiment eventually.  In light  of the above  enumeration, $\bbO  \equiv (W,
Y_{0}, Y_{1}, A,  Y)$ is output, as  opposed to its summary  measure $O$. This
defines another experiment and its law, that we denote $\bbP_{0}$.

It  is   straightforward  to  show\footnote{For  $a   =  0,1$,  \begin{align*}
\Exp_{\bbP_0}(Y_a) &=  \int \Exp_{\bbP_0}(Y_a \mid  W = w) dQ_{0,W}(w)  = \int
\Exp_{\bbP_0}(Y_a \mid A = a, W =  w) dQ_{0,W}(w) \\ &= \int \Exp_{P_0}(Y \mid
A = a,  W = w) dQ_{0,W}(w) = \int  \Qbar_0(a,W) dQ_{0,W}(w).  \end{align*} The
second   equality   follows  from   the   conditional   independence  of   the
counterfactual rewards  $(Y_0,Y_1)$ and  action $A$ given  $W$ (in  words, the
\textit{randomization assumption} ``$(Y_0,Y_1) \perp A  \mid W$'' is met under
$\bbP_{0}$).   The third  equality results  from the  facts that  the observed
reward $Y$ equals the counterfactual reward $Y_a$  when $A = a$ (in words, the
\textit{consistency  assumption}  ``$Y_a =  Y  \mid  A  =  a$'' is  met  under
$\bbP_{0}$)  and that  $\Pr_{P_0}(\ell\Gbar_0(a,W) >  0) =  1$ (in  words, the
\textit{positivity assumption}  is met  under $P_{0}$ ---  this is  needed for
$\Exp_{P_0}(Y \mid A = a, W)$ to be well-defined).}  that
\begin{equation} \label{eq:psi:zero}    
\psi_{0} = \Exp_{\bbP_{0}} \left(Y_{1} - Y_{0}\right) = \Exp_{\bbP_{0}}(Y_1) -
\Exp_{\bbP_{0}}(Y_0).  \end{equation}  Thus, $\psi_{0}$ describes  the average
difference  of the  two counterfactual  rewards.  In  other words,  $\psi_{0}$
quantifies the difference  in average of the  reward one would get  in a world
where one would always enforce action $a=1$ with the reward one would get in a
world where  one would always  enforce action $a=0$.   This said, it  is worth
emphasizing  that $\psi_{0}$  is a  well-defined parameter  beyond its  causal
interpretation, and that  it describes a standardized  association between the
action $A$ and reward $Y$.

To conclude this  subsection, we use our  position as oracles to
sample observations from the  ideal experiment. We call `run_experiment`
with its  argument `ideal` set to  `TRUE` in order to  numerically approximate
$\psi_{0}$.   By  the law  of  large  numbers,  the  following code
approximates $\psi_{0}$ and shows it approximate value.

```{r approx-psi-zero-a-two}
B <- 1e6
ideal_obs <- sample_from(experiment, B, ideal = TRUE)
(psi_approx <- mean(ideal_obs[, "Yone"] - ideal_obs[, "Yzero"]))
```

The object  `psi_approx` contains  an approximation to  $\psi_0$ based  on `B`
observations from the  ideal experiment.  The random  sampling of observations
results  in uncertainty  in  the numerical  approximation  of $\psi_0$.   This
uncertainty can be  quantified by constructing a 95\%  confidence interval for
$\psi_0$. The central limit  theorem and Slutsky's lemma\footnote{Let $X_{1}$,
$\ldots$,  $X_{n}$ be  independently drawn  from a  law such  that $\sigma^{2}
\equiv \Var(X_{1})$  is finite.  Let  $m \equiv \Exp(X_{1})$  and $\bar{X}_{n}
\equiv  n^{-1} \sum_{i=1}^{n}  X_{i}$ be  the empirical  mean.  It  holds that
$\sqrt{n} (\bar{X}_{n}  - m)$ converges  in law as  $n$ grows to  the centered
Gaussian law with  variance $\sigma^{2}$.  Moreover, if  $\sigma_{n}^{2}$ is a
(positive)  consistent estimator  of  $\sigma^{2}$, then  $\sqrt{n}/\sigma_{n}
(\bar{X}_{n} - m)$ converges in law  to the standard normal law. The empirical
variance  $n^{-1}  \sum_{i=1}^{n}  (X_{i}   -  \bar{X}_{n})^{2}$  is  such  an
estimator.  In conclusion, denoting by $\Phi$ the standard normal distribution
function,  $[\bar{X}_{n} \pm  \Phi^{-1}(1-\alpha)  \sigma_{n}/\sqrt{n}]$ is  a
confidence interval for $m$ with  asymptotic level $(1-2\alpha)$.} allow us to
build such an interval as follows.

```{r approx-psi-zero-b}
sd_approx <- sd(ideal_obs[, "Yone"] - ideal_obs[, "Yzero"])
alpha <- 0.05
(psi_approx_CI <- psi_approx + c(-1, 1) * qnorm(1 - alpha / 2) * sd_approx / sqrt(B))
```

We note that the interpretation of this confidence interval is that in 95\% of 
draws of size `B` from the ideal data generating experiment, the true value of 
$\psi_0$ will be contained in the generated confidence interval. 

# &#9881; An alternative parameter of interest, first pass {#exo-alternative-parameter-first-pass}

As discussed  above, parameter $\psi_0$ \eqref{eq:psi:zero}  is the difference
in average  rewards if  we enforce  action $a  = 1$  rather than  $a =  0$. An
alternative  way to  describe  the rewards  under  different actions  involves
quantiles as opposed to averages.  

Let $Q_{0,Y}(y,  A, W) =  \int_{0}^y q_{0,Y}(u, A,  W) du$ be  the conditional
cumulative distribution of  reward $Y$ given $A$ and $W$,  evaluated at $y \in
]0,1[$, that is implied by $P_0$.  For  each action $a \in \{0,1\}$ and $c \in
]0,1[$, introduce \begin{equation}  \label{def_quantile} \gamma_{0,a,c} \equiv
\inf \left\{y  \in ]0,1[ : \int  Q_{0,Y}(y, a, w) dQ_{0,W}(w)  \ge c \right\}.
\end{equation}

It   is    not   difficult   to    check   (see   Problem   1    below)   that
\begin{equation}\label{eq:alter:gamma:zero}\gamma_{0,a,c}  = \inf\left\{y  \in
]0,1[  :  \Pr_{\bbP_{0}}(Y_a  \leq   y)  \geq  c\right\}.\end{equation}  Thus,
$\gamma_{0,a,c}$ can  be interpreted  as a covariate-adjusted  $c$-th quantile
reward     when     action     $a$    is     enforced.      The     difference
\begin{equation*}\delta_{0,c}         \equiv          \gamma_{0,1,c}         -
\gamma_{0,0,c}\end{equation*} is the $c$-th  quantile counterpart to parameter
$\psi_{0}$ \eqref{eq:psi:zero}.

1.  &#x2621; Prove \eqref{eq:alter:gamma:zero}. 

2. &#x2621; Compute the numerical value of $\gamma_{0,a,c}$ for each $(a,c)
   \in \{0,1\} \times  \{1/4, 1/2, 3/4\}$ using the  appropriate attributes of
   `five_obs`.   Based  on  these  results,  report  the  numerical  value  of
   $\delta_{0,c}$ for each $c \in \{1/4, 1/2, 3/4\}$.
   
3. Approximate  the numerical values  of $\gamma_{0,a,c}$ for each  $(a,c) \in
   \{0,1\}  \times \{1/4,  1/2,  3/4\}$ by  drawing a  large  sample from  the
   "ideal"  data experiment  and using  empirical quantile  estimates.  Deduce
   from these results  a numerical approximation to $\delta_{0,c}$  for $c \in
   \{1/4, 1/2, 3/4\}$. Confirm that  your results closely match those obtained
   in the previous problem.


# The parameter of interest, second pass {#parameter-second-pass}

Suppose we  know beforehand that  $O$ drawn from  $P_{0}$ takes its  values in
$\calO  \equiv  [0,1]  \times  \{0,1\}  \times [0,1]$  and  that  $\Gbar(W)  =
P_{0}(A=1|W)$ is bounded away from  zero and one $Q_{0,W}$-almost surely (this
is the case indeed).  Then we can define  model $\calM$ as the set of all laws
$P$ on $\calO$ such that $\Gbar(W)  \equiv P(A=1|W)$ is bounded away from zero
and one $Q_{W}$-almost surely, where $Q_{W}$ is the marginal law of $W$
under $P$.  

Let us also define generically $\Qbar$ as \begin{equation*} \Qbar (A,W) \equiv
\Exp_{P}  (Y|A,  W) \ ,   \end{equation*}  
where, for simplicity, we have suppressed the dependence of $\Qbar$ on $P$.
Central to  our  approach  is  viewing
$\psi_{0}$ as  the value  at $P_{0}$  of the  statistical mapping  $\Psi$ from
$\calM$  to  $[0,1]$  characterized  by \begin{align} \label{eq:psimap} \Psi(P)  &\equiv  \int
\left(\Qbar(1, w) - \Qbar(0, w)\right) dQ_{W}(w) \\ &= \Exp_{P} \left(\Qbar(1,
W) -  \Qbar(0, W)\right), \notag \end{align}  a clear extension  of \eqref{eq:psi0}.
For  instance,  although  the  law  $\Pi_{0} \in  \calM$  encoded  by  default
(\textit{i.e.},  with  `h=0`)  in  `run_another_experiment`  defined  below
differs starkly from $P_{0}$,

```{r another-simulation}
run_another_experiment <- function(n, h = 0) {
  ## preliminary
  n <- Arguments$getInteger(n, c(1, Inf))
  h <- Arguments$getNumeric(h)
  ## ## 'Gbar' and 'Qbar' factors
  Gbar <- function(W) {
    sin((1 + W) * pi / 6)
  }
  Qbar <- function(AW, hh = h) {
    A <- AW[, 1]
    W <- AW[, 2]
    expit( logit( A *  W + (1 - A) * W^2 ) +
           hh * 10 * sqrt(W) * A )
  }
  ## sampling
  ## ## context
  W <- runif(n, min = 1/10, max = 9/10)
  ## ## action undertaken
  A <- rbinom(n, size = 1, prob = Gbar(W))
  ## ## reward
  shape1 <- 4
  QAW <- Qbar(cbind(A, W))
  Y <- rbeta(n, shape1 = shape1, shape2 = shape1 * (1 - QAW) / QAW)
  ## ## observation
  obs <- cbind(W = W, A = A, Y = Y)
  attr(obs, "Gbar") <- Gbar
  attr(obs, "Qbar") <- Qbar
  attr(obs, "QW") <- function(x){dunif(x, min = 1/10, max = 9/10)}
  attr(obs, "shape1") <- shape1
  attr(obs, "qY") <- function(AW, Y, Qbar, shape1){
    A <- AW[,1]; W <- AW[,2]
    Qbar.AW <- do.call(Qbar, list(AW))
    dbeta(Y, shape1 = shape1, shape2 = shape1 * (1 - Qbar.AW) / Qbar.AW)
  }
  ##
  return(obs)
}
```

the  parameter  $\Psi(\Pi_{0})$  is   well  defined.  Straightforward  algebra
confirms  that $\Psi(\Pi_{0})  = 59/300$,  which is  confirmed by  our numeric
computation below.

```{r approx-psi-one}
(five_obs_another_experiment <- sample_from(another_experiment, 5, h = 0))
(psi_Pi_zero <- evaluate_psi(another_experiment, h = 0))
```

# &#9881; An alternative parameter of interest, second pass {#exo-alternative-parameter-second-pass}

As above, we define $q_{Y}(y,A,W)$ to  be the conditional density of $Y$ given
$A$ and  $W$, evaluated at $y$,  that is implied  by a generic $P  \in \calM$.
Similarly, we use $Q_{Y}$ to  denote the corresponding cumulative distribution
function.  The  covariate-adjusted $c$-th  quantile reward  for action  $a \in
\{0,1\}$ may  be viewed as  a mapping  $\Gamma_{a,c}$ from $\calM$  to $[0,1]$
characterized by \begin{equation*} \Gamma_{a,c}(P)  = \inf\left\{y \in ]0,1[ :
\int Q_{Y}(y,a,w) dQ_W(w)  \ge c \right\}.  \end{equation*}  The difference in
$c$-th quantile rewards  may similarly be viewed as a  mapping $\Delta_c$ from
$\calM$  to $[0,1]$,  characterized by  $\Delta_c(P) \equiv  \Gamma_{1,c}(P) -
\Gamma_{0,c}(P)$.

1. Compute the numerical value of $\Gamma_{a,c}(\Pi_0)$ for $(a,c) \in \{0,1\}
   \times   \{1/4,   1/2,  3/4\}$   using   the   appropriate  attributes   of
   `five_obs_from_another_experiment`.   Based on  these  results, report  the
   numerical value of $\Delta_c(\Pi_0)$ for each $c \in \{1/4, 1/2, 3/4\}$.

2. Approximate the  value of $\Gamma_{0,a,c}(\Pi_{0})$ for  $(a,c) \in \{0,1\}
   \times \{1/4, 1/2,  3/4\}$ by drawing a large sample  from the "ideal" data
   experiment  and  using empirical  quantile  estimates.   Deduce from  these
   results a numerical  approximation to $\Delta_{0,c} (\Pi_{0})$  for each $c
   \in  \{1/4, 1/2,  3/4\}$.  Confirm  that your  results closely  match those
   obtained in the previous problem.
   
3. Building upon the code you wrote to solve the previous problem, construct a
   confidence  interval   with  asymptotic  level  $95\%$   for  $\Delta_{0,c}
   (\Pi_{0})$, with  $c \in  \{1/4, 1/2, 3/4\}$.\footnote{Let  $X_{1}, \ldots,
   X_{n}$  be  independently drawn  from  a  continuous distribution  function
   $F$. Set $p \in ]0,1[$ and, assuming  that $n$ is large, find $k\geq 1$ and
   $l \geq 1$ such that  $k/n \approx p - \Phi^{-1}(1-\alpha) \sqrt{p(1-p)/n}$
   and $l/n \approx p +  \Phi^{-1}(1-\alpha) \sqrt{p(1-p)/n}$, where $\Phi$ is
   the standard  normal distribution function.  Then  $[X_{(k)},X_{(l)}]$ is a
   confidence interval for $F^{-1}(p)$ with asymptotic level $1 - 2\alpha$.}



# The parameter of interest, third pass {#parameter-third-pass}

In the previous subsection, we reoriented  our view of the target parameter to
be  that   of  a   statistical  functional   of  the   law  of   the  observed
data. Specifically, we viewed the parameter as a function of specific features
of   the   observed  data   law,   namely   $Q_{W}$   and  $\Qbar$.    It   is
straightforward\footnote{We temporarily drop the subscript $P_0$ to save space
and note,  for the same reason,  that $(2a-1)$ equals  1 if $a=1$ and  $-1$ if
$a=0$.   Now, for  each $a  = 0,1$,  \begin{align*} \Exp\left(\frac{\one\{A  =
a\}Y}{\ell\Gbar(a,W)}\right)     &=    \Exp\left(\Exp\left(\frac{\one\{A     =
a\}Y}{\ell\Gbar(a,W)} \middle| A, W  \right) \right) = \Exp\left(\frac{\one\{A
=  a\}}{\ell\Gbar(a,W)}  \Qbar(A,  W)   \right)  =  \Exp\left(\frac{\one\{A  =
a\}}{\ell\Gbar(a,W)}         \Qbar(a,          W)\right)         \\         &=
\Exp\left(\Exp\left(\frac{\one\{A = a\}}{\ell\Gbar(a,W)}  \Qbar(a, W) \middle|
W \right)  \right) =  \Exp\left(\frac{\ell\Gbar(a,W)}{\ell\Gbar(a,W)} \Qbar(a,
W) \middle|  W \right) = \Exp  \left( \Qbar(a, W) \right),  \end{align*} where
the first,  fourth and sixth  equalities follow from  the tower rule,  and the
second  and fifth  hold by  definition  of the  conditional expectation,  This
completes the proof.}   to show an equivalent representation  of the parameter
as \begin{align}  \notag \psi_{0}  &= \int  \frac{2a -  1}{\ell\Gbar_0(a,w)} y
dP_0(w,a,y)   \\   \label{eq:psi0:b}   &=   \Exp_{P_0}   \left(   \frac{2A   -
1}{\ell\Gbar_{0}(A,W)} Y \right).  \end{align}  Viewing again the parameter as
a  statistical   mapping  from  $\calM$   to  $[0,1]$,  it  also   holds  that
\begin{align} \notag  \Psi(P) &= \int \frac{2a-1}{\ell\Gbar(a,w)}  y dP(w,a,y)
\\  \label{eq:psi0:c}  &=  \Exp_{P}\left(\frac{2A -  1}{\ell\Gbar_{0}(A,W)}  Y
\right).  \end{align}

Our reason for introducing this alternative  view of the target parameter will
become clear when we discuss estimation of the target parameter. Specifically,
the  representations (\ref{eq:psi0})  and (\ref{eq:psi0:b})  naturally suggest
different estimation strategies for $\psi_0$.  The former suggests building an
estimator  of $\psi_0$  using estimators  of $\Qbar_0$  and of  $Q_{W,0}$. The
latter  suggests  building  an  estimator  of  $\psi_0$  using  estimators  of
$\ell\Gbar_0$ and of $P_0$. We return to these ideas in later sections.

# &#9881; An alternative parameter of interest, third pass {#exo-alternative-parameter-third-pass}

1.   &#x2621;  Show that  for $a'  = 0,1$,  $\gamma_{0,a',c}$ as  defined in
(\ref{def_quantile})  can be  equivalently expressed  as \begin{equation*}\inf
\left\{z \in ]0,1[  : \int \frac{\one\{a =  a'\}}{\ell\Gbar(a',W)} \one\{y \le
z\} dP_0(w,a,y) \ge c \right\}.\end{equation*}


# Smooth parameters, first pass {#smooth-first-pass}


Within  our view  of the  target  parameter as  a statistical  mapping, it  is
natural to inquire of properties this  functional enjoys.  For example, we may
be interested in asking how the value of $\Psi(P)$ changes as we consider laws
that *get nearer to* $P$ in $\calM$.  If small deviations from $P_0$ result in
large  changes in  $\Psi(P_0)$,  then we  might hypothesize  that  it will  be
difficult to  produce stable estimators  of $\psi_0$. Fortunately,  this turns
out not to be the case for the mapping  $\Psi$, and so we say that $\Psi$ is a
*smooth*   parameter   mapping.   We   formalize   this   notion  in   Section
\@ref(smooth-first-pass),  and  here  provide   an  informal  description  of
smoothness.

To discuss how $\Psi(P)$ changes for distributions *near* $P$ in the model, we 
require a more concrete definition of nearness.  To that end, consider the law 
encoded in `run_another_experiment`  as a function of the  input parameter `h`.  
Let $\Pi_{h} \in  \calM$ be the law encoded by  `run_another_experiment` for a
given `h`  $\in ]-1,1[$.  Note  that $\calP \equiv  \{\Pi_h : h  \in ]-1,1[\}$
defines  a collection  of laws,  that is,  a statistical  model.  We  say that
$\calP$ is a  *submodel* of $\calM$ because $\calP  \subset \calM$.  Moreover,
we say that this submodel is  *through $\Pi_0$* since $\Pi_{h} \to \Pi_{0}$ as
$h \to  0$.  One could  enumerate many  possible submodels in  $\calM$ through
$\Pi_0$. It turns  out that all that  matters for our purposes is  the form of
the submodel in  a neighborhood of $\Pi_0$. We informally  say that this local
behavior describes the *direction* of a submodel through $\Pi_0$. We formalize
this notion in the next subsection.

We now  have a notion of how to move through the model space $P \in \calM$ and 
can study how the value of the parameter changes as we move away from a law $P$. 
Above,  we said  that  $\Psi$  is a  smooth
parameter if  it does not change  "too much" as we  move towards $P$  in any
particular direction.  That  is, we should hope that  $\Psi$ is differentiable
along our  submodel at $P$. This idea too is formalized in the next subsection, and
we now turn to illustrating this idea numerically. The code below evaluates  how the  parameter changes  for laws  in $\calP$,  and
approximates the  derivative of  the parameter along  the submodel  $\calP$ at
$\Pi_0$.

(ref:psi-approx-psi-one) Evolution of  statistical parameter $\Psi$ along
fluctuation $\{\Pi_{h} : h \in H\}$.

```{r psi-approx-psi-one, fig.cap = '(ref:psi-approx-psi-one)'}
approx <- seq(-1, 1, length.out = 1e2)
psi_Pi_h <- sapply(approx, function(t) {
  evaluate_psi(another_experiment, h = t)
})
slope_approx <- (psi_Pi_h - psi_Pi_zero) / approx
slope_approx <- slope_approx[min(which(approx > 0))]
ggplot() +
  geom_point(data = data.frame(x = approx, y = psi_Pi_h), aes(x, y),
             color = "#CC6666") +
  geom_segment(aes(x = -1, y = psi_Pi_zero - slope_approx,
                   xend = 1, yend = psi_Pi_zero + slope_approx),
               arrow = arrow(length = unit(0.03, "npc")),
               color = "#9999CC") +
  geom_vline(xintercept = 0, color = "#66CC99") +
  geom_hline(yintercept = psi_Pi_zero, color = "#66CC99") +
  labs(x = "h", y = expression(Psi(Pi[h]))) 
```

The dotted curve  represents the function $h \mapsto  \Psi(\Pi_{h})$. The blue
line represents  the tangent to the  previous curve at $h=0$,  which indeed appears to be
differentiable around $h=0$.  In
the  next  subsection,  we derive a  closed-form expression 
for the  slope of the blue  curve from the chunk of code where 
`run_another_experiment` is defined.

<!--

\subsubsection*{\gear Exercises}

1.    Adapt  the code  from problem 1  in Section  \ref{subsec:visualizing} to
visualize $\Exp_{\Pi_h}(Y  \mid A = 1,  W)$, $\Exp_{\Pi_h}(Y \mid A  = 0, W)$,
and $\Exp_{\Pi_h}(Y \mid  A = 1, W) -  \Exp_{\Pi_h}(Y \mid A = 0,  W)$, for $h
\in \{-1/2, 0, 1/2\}$.

Define a new experiment with law $\Pi_0'$  by adapting the code used to define
`run_another_experiment`.  Leave  all aspects of the  new experiment identical
to $\Pi_0$, but set

```{r set-Q-bar, eval = FALSE}
Qbar = function(AW, hh = h){
  A <- AW[,1]
  W <- AW[,2]
  expit( logit( A * W + (1 - A) * W^2 ) + 
         hh * (2*A - 1) / ifelse(A == 1, sin((1 + W) * pi / 6), 
                                 1 - sin((1 + W) * pi / 6)) *
         (Y - A * W + (1 - A) * W^2))
}
```

2.  Repeat the  previous  problem  for this  new  experiment.  Comment on  the
   similarities  and differences  between $\Pi_0$  and $\Pi_0'$  for different
   values of $h$.

3.  Re-produce Figure  1 for  law $\Pi_0'$.  Comment on  the similarities  and
   differences between  this figure for  $\Pi_0$ and $\Pi_0'$.  In particular,
   how  does the  behavior of  the  target parameter  around $h  = 0$  compare
   between laws $\Pi_0$ and $\Pi_0'$?

# &#x2621; Being smooth, second pass {#smooth-second-pass}

Let us now formally define what it  means for statistical mapping $\Psi$ to be
smooth at every  $P \in \calM$.  For every $h  \in H \equiv ]-M^{-1},M^{-1}[$,
we can define  a law $P_{h} \in \calM$ by  setting $P_{h} \ll P$\footnote{That
is, $P_{h}$ is  dominated by $P$: if  an event $A$ satisfies $P(A)  = 0$, then
necessarily $P_{h} (A) = 0$ too.}  and

\begin{equation}\label{eq:fluct}\frac{dP_{h}}{dP}    \equiv   1    +   h    s,
\end{equation}

where $s : \calO\to  \bbR$ is a (measurable) function of  $O$ such that $s(O)$
is not equal to zero $P$-almost surely, $\Exp_{P} (s(O)) = 0$, and $s$ bounded
by $M$.   We make the observation  that \begin{equation}\label{eq:score}(i) \;
P_{h}|_{h=0}    =     P,    \quad    (ii)    \;     \left.\frac{d}{dh}    \log
\frac{dP_{h}}{dP}(O)\right|_{h=0} = s(O).\end{equation}

Because  of   \textit{(i)},  $\{P_{h}  :  h  \in   H\}$  is  a submodel through $P$
(also referred to as a \textit{fluctuation} of $P$).  As above note that the 
fluctuation is a one-dimensional submodel of
$\calM$  with univariate  parameter $h  \in  H$.  We  note that  \textit{(ii)}
indicates that  the score of this  submodel at $h =  0$ is $s$.  Thus,  we say
that the fluctuation is \textit{in the  direction} of $s$. Fluctuations of $P$
do not  necessarily take the same  form as in \eqref{eq:fluct}.  No matter how
the  fluctuation is  built, for our purposes the most important feature of the 
fluctuation is its local  shape in a neighborhood of $P$. 

We  are  now  prepared  to  provide  a  formal  definition  of  smoothness  of
statistical mappings.  We  say that a
statistical mapping $\Psi$ is smooth at every $P \in \calM$ if for each $P \in
\calM$, there exists a (measurable) function  $D^{*}(P) : \calO \to \bbR$ such
that  $\Exp_{P}(D^{*}(P)(O)) =  0$,  $\Var_{P}(D^{*}(P)(O)) < \infty$, and,  for
every  fluctuation $\{P_{h}  : h  \in H\}$  with  score $s$  at $h  = 0$,  the
real-valued mapping $h \mapsto \Psi(P_{h})$ is differentiable at $h=0$, with a
derivative               equal               to               \begin{equation}
\label{eq:derivative}\Exp_{P}\left(D^{*}(P)(O) s(O)\right).  \end{equation}

<!-- Interestingly,   if  a   fluctuation   $\{P_{h}  :   h   \in  H\}$   satisfies
\eqref{eq:score} for a direction $s$ such that $s\neq 0$, $\Exp_{P}(s(O)) = 0$
and  $\Var_{P}  (s(O))  <  \infty$,  then $h  \mapsto  \Psi(P_{h})$  is  still
differentiable  at  $h=0$ with  a  derivative  equal to  \eqref{eq:derivative}
(beyond  fluctuations of  the  form \eqref{eq:fluct}).   -->

The object $D^*(P)$ in (\ref{eq:derivative}) is called a gradient of $\Psi$ at
$P$.  

This  terminology has  a direct  parallel  to directional  derivatives in  the
calculus  of Euclidean  geometry.   Recall  that if  $f$  is a  differentiable
mapping from $\bbR^p$ to $\bbR$, then the directional derivative of $f$ at $x$
(a point in $\bbR^p$) in direction $u$  (a unit vector in $\bbR^p$) is the dot
product of the gradient of $f$  and $u$.  In words, the directional derivative
of $f$ at $x$ can be represented as  an inner product of the direction that we
approach $x$ and  the change of the  function's value at $x$.   In the present
problem, the law  $P$ is the point  at which we evaluate  the function $\Psi$,
the score $s$ of  the fluctuation is the "direction" in  which we approach the
point, and  the gradient describes the  change in the function's  value at the
point.



In general, it is possible for many gradients to exist\footnote{This may be at
first  surprising given  the previous  parallel drawn  to Euclidean  geometry.
However, it is  important to remember that the model  dictates fluctuations of
$P$ that  are valid submodels  with respect to the  full model. In  turn, this
determines  the possible  directions from  which we  may approach  $P$.  Thus,
depending  on the  direction,  \eqref{eq:derivative} may  hold with  different
choices  of  $D^*$.}.   However,  in  the  special  case  that  the  model  is
nonparametric, only a  single gradient exists, which is  sometimes referred to
as the canonical gradient. In the more general setting, the canonical gradient
may be defined as the minimizer of  $D\mapsto \Var_{P} (D(O))$ over the set of
all gradients.

```{r eic}
## 
## move around
## 
eic_experiment <- evaluate_eic(experiment)
(eic_experiment(five_obs))

eic_another_experiment <- evaluate_eic(another_experiment, h = 0)
(eic_another_experiment(five_obs_another_experiment))
```

```{r cramer-rao}
##
## move around
##
obs <- sample_from(experiment, B)
(cramer_rao_hat <- var(eic_experiment(obs)))
```

```{r cramer-rao-another-experiment}
obs_another_experiment <- sample_from(another_experiment, B, h = 0)
(cramer_rao_Pi_zero_hat <- var(eic_another_experiment(obs_another_experiment)))
(ratio <- sqrt(cramer_rao_Pi_zero_hat/cramer_rao_hat))
```

# Revisiting Section \@ref(smooth-first-pass)

It is  not difficult  (though cumbersome)  to verify that,  up to  a constant,
$\{\Pi_{h} :  h \in [-1,1]\}$ is  a fluctuation of $\Pi_{0}$  in the direction
(in the sense of \eqref{eq:fluct}) of

\begin{align*}\sigma_{0}(O)  &\equiv -  10  \sqrt{W}  A \times  \beta_{0}(A,W)
\left(\log(1    -     Y)    +     \sum_{k=0}^{3}    \left(k     +    \beta_{0}
(A,W)\right)^{-1}\right)     +      \text{constant},\\     \text{where}     \;
\beta_{0}(A,W)&\equiv                                                  \frac{1
-\Qbar_{\Pi_{0}}(A,W)}{\Qbar_{\Pi_{0}}(A,W)}.\end{align*}

Consequently,    the    slope    of     the    dotted    curve    in    Figure
\@ref(fig:psi-approx-psi-one) is equal to 

\begin{equation}\label{eq:slope:Pi}\Exp_{\Pi_{0}}      (D^{*}(\Pi_{0})     (O)
\sigma_{0}(O))\end{equation}

(since $D^{*}(\Pi_{0})$  is centered under $\Pi_{0}$,  knowing $\sigma_{0}$ up
to a constant is not problematic). 

In the following code, we check this numerically by implementing the direction
$\sigma_{0}$ with  `R` function `sigma0_run_another_experiment`, which  we use
to  numerically   approximate  \eqref{eq:slope:Pi}   (pointwise  and   with  a
confidence interval of asymptotic level 95\%):

```{r recover-slope}
vars <- eic_another_experiment(obs_another_experiment) *
  sigma0(obs_another_experiment)
sd_hat <- sd(vars)
(slope_hat <- mean(vars))
(slope_CI <- slope_hat + c(-1, 1) * qnorm(1 - alpha / 2) * sd_hat / sqrt(B))
```

Equal to  `r round(slope_approx,  3)` (rounded to  three decimal  places), the
first numerical approximation `slope_approx` is not too off.

# &#x2621; Influence functions and the efficient influence function {#influence-functions}

If an estimator $\psi_n$ of $\Psi(P)$ can be written as 

\begin{equation*}  \psi_n  =  \Psi(P) +  \frac{1}{n}\sum_{i=1}^n  \IF(O_i)  +
o_P(1/\sqrt{n})\end{equation*}

for some function  $\IF : \calO \to  \bbR$ such that $\Exp_P(\IF(O))  = 0$ and
$\Var_{P}(\IF(O))  < \infty$,  then we  say that  $\psi_n$ is  *asymptotically
linear* with  *influence function*  $\IF$.  We note  that weak  convergence of
asymptotically linear estimators is implied by the central limit theorem. That
is, if $\psi_n$  is asymptotically linear with influence  function $\IF$, then
$\sqrt(n)  (\psi_n -  \Psi(P))  = \frac{1}{\sqrt{n}}  \sum_{i=1}^n \IF(O_i)  +
o_P(1)$. The central  limit theorem implies the  the right-hand-side converges
in law to a centered Gaussian distribution with variance $\Var_P(\IF(O))$.

As  it happens,
influence  functions of  regular\footnote{We  can view  $\psi_{n}$  as the  by
product of an algorithm $\Psihat$ trained on independent observations $O_{1}$,
\ldots,  $O_{n}$  drawn from  $P$.  We say that the estimator  is  regular at  $P$ 
if, for any direction $s\neq 0$ such that $\Exp_{P} (s(O)) = 0$
and  $\Var_{P}  (s(O))  <  \infty$  and fluctuation  $\{P_{h}  :  h  \in  H\}$
satisfying   \eqref{eq:score},   the    estimator   $\psi_{n,1/\sqrt{n}}$   of
$\Psi(P_{1/\sqrt{n}})$   obtained  by   training   $\Psihat$  on   independent
observations $O_{1}$, \ldots, $O_{n}$ drawn from $P_{1/\sqrt{n}}$ is such that
$\sqrt{n} (\psi_{n,1/\sqrt{n}} - \Psi(P_{1/\sqrt{n}}))$  converges in law to a
limit that  does not  depend on  $s$.}  estimators  are intimately  related to
gradients.   In fact,  if $\psi_n$  is a  regular, asymptotically linear 
estimator  of $\Psi(P)$  with
influence  function $\IF$,  then it  must be  true that  $\Psi$ is  a smooth
at $P$ and  that $\IF$  is a  gradient of $\Psi$  at $P$.  

These results, combined with the definition of the canonical gradient, imply that  
if $\psi_{n}$ is a regular, asymptotically linear estimator of $\Psi(P)$ built
from $n$ independent observations drawn from $P$, then the asymptotic variance
of $\sqrt{n} (\psi_{n} - \Psi(P))$ cannot  be smaller than the variance of the
canonical         gradient          of         $\Psi$          at         $P$,
\begin{equation}\label{eq:CR}\Var_{P}(D^{*}(P)(O))  . \end{equation}  That is,
(\ref{eq:CR}) is  the lower bound on  the asymptotic variance of  any regular,
asymptotically linear estimator of $\Psi(P)$. This bound is referred to as the
\textit{Cramér-Rao bound}.  Any regular estimator that  achieves this variance
bound  is said  to be  \textit{asymptotically  efficient} at  $P$. Since,  the
canonical gradient  is the influence  function of an  asymptotically efficient
estimator, it  is often referred to  as the *efficient influence  function* or
the *efficient influence curve*.

It is not difficult to check \tcg{(do  we give the proof?)} that the efficient
influence function of  $\Psi$ at  $P \in  \calM$ can be written as $D^{*}(P)
\equiv D_{1}^{*}  (P) +  D_{2}^{*} (P)$ where 
\begin{equation} \label{eq:eif}
\begin{aligned} D_{1}^{*}(P) (O)  &\equiv \Qbar(1,W)  - \Qbar(0,W)  - \Psi(P),\\
D_{2}^{*}(P)     (O)     &\equiv      \frac{2A-1}{\ell\Gbar(A,W)}     (Y     -
\Qbar(A,W)).\end{aligned}
\end{equation}

# &#9881; Cramér-Rao bounds {#exo-cramer-rao}

1. Numerically approximate the Cramér-Rao bound at $P_{0}$ and at
   $\Pi_0$. With  a large  sample and  using regular  estimators, can  we more
   precisely estimate $\Psi(P_0)$ or $\Psi(\Pi_0)$?

2. Numerically approximate  the  Cramér-Rao  bound at  the  law encoded  for
   problems  2 and  3  of Section  \@ref(subsec:exo:dave:three). Compare  this
   bound with those computed in problem 1.

# Linear approximations of parameters {#linear-approximation}

In the last subsection, we learned that the stochastic behavior of a regular, 
asymptotically linear estimator of $\Psi(P)$ can be characterized by its
influence function. Moreover, we said that this influence function must in fact
be a gradient of $\Psi$ at $P$. In this section, we show that the converse
is also true: given a gradient $D^*$ of $\Psi$  at $P$, under regularity
conditions, it is  possible to construct an estimator  with influence function
equal to $D^*(P)$. This fact will suggest concrete strategies for generating efficient 
estimators of smooth parameters. In this section, we take the first step towards 
generating such estimators: linearizing the parameter. 

Drawing  another parallel  to  Euclidean geometry,  recall that  if  $f$ is  a
differentiable mapping from $\bbR^p$ to $\bbR$,  we can use a Taylor series to
approximate $f$ at a point $x_0 \in  \bbR^p$, $f(x_0) \approx f(x) + (x_0 - x)
\cdot \nabla f(x)$, where $x$ is a  point in $\bbR^p$ and $\nabla f(x)$ is the
gradient  of $f$  evaluated at  $x$.  As the  distance between  $x$ and  $x_0$
decreases, the *linear approximation* to $f(x_0)$ becomes more accurate.

Returning to  the present  problem with this  in mind, we  find that  indeed a
similar approximation  strategy may  be applied. In  particular, if  $\Psi$ is
pathwise differentiable uniformly  over directions, then for any  given $P \in
\calM$,  we  can  write  \begin{align}  \label{taylor_expansion}  \Psi(P_0)  =
\Psi(P) + (P_0 - P)  D^*(P) - \Rem_{P_0}(P), \end{align} where $\Rem_{P_0}(P)$
is a *remainder term* satisfying that \begin{align*} \frac{\Rem_{P_0}(P)}{d(P,
P_0)} \rightarrow 0 \ \mbox{as} \ d(P, P_0) \rightarrow 0 , \end{align*} where
$d$  is   a  measure   of  discrepancy  for   distributions  in   $\calM$.  In
(\ref{taylor_expansion}),  we introduced  a  new shorthand  notation. For  any
measurable function $g$  of the observed data  $O$, we will write  $P g \equiv
\Exp_P(g(O))$. Thus, (\ref{taylor_expansion}) could be equivalently written as
\begin{align*}    \Psi(P_0)    =    \Psi(P)    +    \Exp_{P_0}(D^*(P)(O))    -
\Exp_P(D^*(P)(O)) - \Rem_{P_0}(P).  \end{align*} The remainder term formalizes
the notion that if $P$ is ``close''  to $P_0$ (i.e., $d(P,P_0)$ is small) then
the linear approximation of $\Psi(P_0)$ is more accurate.

The equations for  the definition of the parameter  (\ref{eq:psimap}), form of
the  canonical   gradient  (\ref{eq:eif}),  and  linearization   of  parameter
(\ref{taylor_expansion})  combine to  determine  the remainder,  \begin{align}
\Rem_{P_0}(P) &= \Psi(P) - \Psi(P_0) - (P_0 - P)D^*(P) \notag \\ &= \Exp_{P_0}
\biggl[    \{\Gbar_0(W)    -     \Gbar(W)\}    \biggl\{\frac{\Qbar_0(1,W)    -
\Qbar(1,W)}{\ell\Gbar(1,W)} + \frac{\Qbar_0(0,W) - \Qbar(0,W)}{\ell\Gbar(0,W)}
\biggr\} \biggr]. \label{remainder} \end{align}

```{r evaluating-remainder}
# Could we add a method for evaluating the remainder?
# e.g., using integrate and/or approximating with large sample
```

# &#9881; The remainder term {#exo-remainder-term}

1.  [Pending  the  addition  of  above  method]  Use  the  available  code  to
   numerically confirm equation (\ref{taylor_expansion}) by approximating each
   term in  the equation separately  and showing approximate  equality between
   the two sides of the equation.

2. Compute $\Rem_{\Pi_0}(\Pi_h)$ for $h \in [-1,1]$ and plot your results.

# &#x2621; Double-robustness {#double-robustness}

The  efficient influence  curve $D^{*}(P)$  at  $P \in  \calM$ enjoys  a rather
remarkable property: it is double-robust.   Specifically, the remainder satisfies

\begin{equation}\label{eq:rem:two}   \Rem_{P}    (\Qbar',   \Gbar')^{2}   \leq
\|\Qbar'  - \Qbar\|_{P}^{2}  \times  \|(\Gbar' -  \Gbar)/\ell\Gbar'\|_{P}^{2}.
\end{equation}

In particular, if

\begin{equation}\label{eq:solves:eic}     \Exp_{P}    (D^{*}(P')     (O))    =
0,\end{equation}

and  \textit{either}  $\Qbar' =  \Qbar$  \textit{or}  $\Gbar' =  \Gbar$,  then
$\Rem_{P} (\Qbar', \Gbar') = 0$ hence $\Psi(P') = \Psi(P)$.  In words, if $P'$
solves  the   so  called  $P$-specific  efficient   influence  curve  equation
\eqref{eq:solves:eic} and if, in addition, $P'$ has the same $\Qbar$-component
or $\Gbar$-component as $P$, then $\Psi(P')  = \Psi(P)$ no matter how $P'$ may
differ  from  $P$ otherwise.  This  property  is  useful to  build  consistent
estimators of $\Psi(P)$.

# &#9881; Double-robustness {#exo-double-robustness}

Confirm the double-robustness of the efficient influence function by doing the following:

1. \tcg{Will modify  to reflect the final structure of  `LAW` methods, etc...}
   Replace `Gbar` in the law of  `experiment` with some other function of $W$,
   and confirm  (via numerical approximation)  that the mean of  the efficient
   influence function is zero. Repeat for several choices of `Gbar`.

2. \tcg{Will modify  to reflect the final structure of  `LAW` methods, etc...}
   Replace `Qbar` in the law of  `experiment` with some other function of $W$,
   and confirm  (via numerical approximation)  that the mean of  the efficient
   influence function is zero. Repeat for several choices of `Gbar`.


# Building estimators

The previous  section identified our  target parameter and  developed relevant
theory for understanding the properties of  certain types of estimators of the
parameter. In  this section, we illustrate  how this theory can  be applied to
develop estimators of and inference  about these quantities.  We consider that
we  have  available,  $O_{1}$,  \ldots,  $O_{n}$,  a  sample  of  observations
independently drawn  from $P_{0}$, and  the goal is  to estimate the  value of
$\psi_0$. Below  we generate `B` values  \tcg{Maybe change `B` to  `n` to keep
consistent with the notation?}

```{r draw-a-sample}
## Debug -- couldn't find obs when I tried to compile
## obs <- run_experiment(B)
```

Each of  the strategies  we discuss be  described in two  steps. In  the first
step, we obtain estimators of the quantities relevant to evaluating the target
parameter and  its efficient influence  function. Because estimation  of these
quantities is merely  an intermediate step along our path  towards an estimate
of  the  target   parameter,  we  refer  to  these   quantities  as  *nuisance
parameters*. In  the second step,  we combine  the estimators of  the nuisance
parameters in  a particular way as  to ensure that the  resultant estimator of
the target parameter enjoys desirable properties.  In this second step we will
make use of the theory developed in the previous subsection.

# Estimating nuisance parameters

The  first step  towards obtaining  estimates of  the target  parameter is  to
propose  a method  for  estimating the  nuisance parameters.  In  the case  of
$\Psi(P_0)$,  these  nuisance  parameters include  $\Gbar_0$,  $\Qbar_0$,  and
$Q_{0,W}$. We will discuss estimation of each of these quantities in turn.

In the  next code block,  we define  a function that  we will use  to estimate
$\Gbar_0$.  The function  takes  a  data set  as  input  and a  user-specified
algorithm for  estimating $\Gbar_0$  and returns the  fitted algorithm  on the
data. We will consider approaches to  estimation of $\Gbar_0$ based on machine
learning  (if the  `"ML"`  attribute  of `algorithm`  is  `TRUE`)  or not  (if
`FALSE`).

\tcg{There's a lot  to deparse in these  code chunks. I worry that  it will be
overwhelming for  the reader. Is  it possible to  simplify the syntax  at all?
E.g., could  we create  an S3  class for  algorithms? Not  sure what  the best
strategy  is, but  as  it  is I  think  most readers  will  have  a hard  time
understanding what these chunks do.}

```{r intro-est-G}
## estimate_G <- function(dat, algorithm, ...) {
##   if (!is.data.frame(dat)) {
##     dat <- as.data.frame(dat)
##   }
##   if (!attr(algorithm, "ML")) {
##     fit <- algorithm[[1]](formula = algorithm[[2]], data = dat)
##   } else {
##     fit <- algorithm[[1]](dat, ...)
##   }
##   fit$type_of_preds <- algorithm$type_of_preds
##   return(fit)
## }
```
To make this  estimation procedure more concrete, let us  provide an algorithm
for   estimation  of   $\Gbar_0$.   The  following   code   block  builds   an
algorithm. \tcg{Again, this is complicated stuff. Can we simplify?}

```{r unknown-Gbar-two}
trim_glm_fit <- caret::getModelInfo("glm")$glm$trim
working_model_G_one <- list(
  model = function(...) { 
    trim_glm_fit(glm(family = binomial(), ...))
  },
  formula = as.formula(
    paste("A ~",
          paste(c("I(W^", "I(abs(W - 5/12)^"),
                rep(seq(1/2, 3/2, by = 1/2), each = 2),
                sep = "", collapse = ") + "),
          ")")
  ),
  type_of_preds = "response"
)
attr(working_model_G_one, "ML") <- FALSE
working_model_G_one$formula
```

The next code chunk illustrates how the algorithm can be used to estimate $\Gbar_0$ based on the observed data `obs`. 

```{r show-estimateG-in-action}

```

Similarly, we can define a function and a method for estimating $\Qbar_0$.

```{r intro-estimateQ-method}
# introduce code for estimation of $\Qbar_0$ 
# and show how it works 
```

To  estimate $Q_{0,W}$,  the  distribution  of $W$,  we  will  simply use  the
empirical measure of  $W_1, \dots, W_n$, that is, the  law describing sampling
from $W_1, \dots, W_n$ with equal probability $n^{-1}$.

```{r intro-to-estimateQW}
# not sure how this is being handled in the code 
# and whether we need to introduce a function for this here?
```
In  general, we  can view  an estimator  of $\Qbar_0$  (or any  other relevant
nuisance parameter)  as an algorithm  $\hat{Q}: \calM \rightarrow  \calQ$ that
maps a distribution from the model  into the nuisance parameter space. In this
case, $\calQ$ consists of all functions mapping from $\{0,1\} \times [0,1]$ to
$\bbR$. We can be our estimate $\Qbar_n$ as the algorithm $\hat{Q}$ applied on
$P_{n}$, the empirical  measure, \textit{i.e.}, the law  consisting in drawing
one among $O_{1}$, \ldots, $O_{n}$ with equal probabilities $n^{-1}$. Thus, we
can write $\Qbar_n = \hat{Q}(P_n)$.

# &#9881; Algorithms {#exo-algorithms}

1. \tcg{Depending on how above goes,  define your own algorithm for estimating
   $\Gbar_0$ and/or $\Qbar_0$?}

\tcg{Do we  need a  remark on  simulation studies?  I.e., something  like: *In
order  to  study the  frequentist  operating  characteristics of  the  various
estimators of the  target parameter, we will use Monte  Carlo simulation. That
is,  we will  repeatedly sample  $n$ independent  observations from  $P_0$ and
build our  estimator on each  realization of  our experiment.* Don't  know how
much detail we need  to go into, but adding something like  this would give us
an excuse to get  all of the estimation of the nuisance  parameters out of the
way. Again, it all depends on how we/you want to structure the code.}

# Simple estimation strategies

Equation (\ref{eq:psi0}) suggests the following, simple estimator of $\psi_0$ \[
\psi_{n}  &\equiv   \int  \left(\Qbar_{n}(1,   w)  -   \Qbar_{n}(0,  w)\right) dQ_{n,W}(w) , 
\]
where $\Qbar_n$ is an estimate of $\Qbar_0$ and $Q_{n,W}$ is an estimate of $Q_{0,W}$. 
In words, this estimator is implemented by first estimating the regression of $Y$ on $A$ 
and $W$. The estimated regression function is marginalized with respect to the 
estimated law of $W$. This estimator is referred to as a standardization or G-computation
estimator. We refer to this estimator as a "plug-in" estimator since it can be 
obtained by "plugging in" estimates of relevant quantities into (\ref{eq:psi0}). 

A plug-in estimate of $\psi_0$ is constructed based on `B` independent draws from $P_0$.
This estimator uses a parametric estimator of $\Qbar_0$.

```{r show-g-comp-computation}

```

Alternatively, we could construct a plug-in estimate based on a more flexible regression 
technique. The following code shows construction of a plug-in estimate based on $k$-nearest
neighbors regression. 

```{r show-another-gcomp}

```

We now inquire as to the stochastic properties of such an estimator. In this section,
we study the estimators properties via Monte Carlo simulation. In particular, we 
study the bias, variance, and weak convergence of the estimators. 

```{r simulation-with-parametric-and-knn}
# maybe have three estimators? one with correct parametric model, one with misspecified parametric model and one with knn?
```

The simulation demonstrates the following key points. For correctly-specified parametric
regressions, the plug-in estimator exhibits excellent behavior: its bias disappears
faster than $n^{-1/2}$, its variance is close to that of an efficient estimator in large samples, and its sampling distribution is approximately Gaussian. However, misspecified 
parametric regressions demonstrate very poor behavior indeed. Their bias does not disappear, even in large samples and thus their sampling distribution is not centered around the truth. 
This may suggest that we should be instead using more flexible regression techniques in
order to ensure that our estimate $\Qbar_n$ is as close to $\Qbar_0$ as possible. However,
we find that, in spite of the excellent fit of $\Qbar_n$ to $\Qbar_0$, the plug-in estimate based on $k$-nearest neighbors exhibits non-negligible bias. In particular, the bias is 
not disappearing faster than $n^{-1/2}$. Thus, the center point of the sampling distribution of $n^{1/2}$ times our estimator is moving farther from the truth as sample size increases. 
In the next subsection, we analyze the plug-in estimator to understand the source
of this poor behavior. 

# &#9881; Plug-in estimate {#exo-plug-in-estimate}

1. Evaluate the plug-in estimate,  if, rather than the empirical distribution,
   we used  a maximum likelihood  estimator of $Q_{0,W}$ under  the assumption
   that $W \sim  \mbox{Normal}(\mu, \sigma^2)$ where $\mu$  and $\sigma^2$ are
   unknown.

2. \tcg{If we  asked them to implement a regression  estimator in the previous
   subsection, let's now have them use it to repeat our simulation and compute
   the G-comp estimator based on this.}

# &#x2621; Analysis of plug-in estimator {#analysis-plug-in-estimate}

In view of our estimators of $\Qbar_0$ and $\Gbar_0$ as algorithms that map from a 
distribution in $\calM$ to the relevant parameter space, we may view the estimates
$\Qbar_n \equiv \hat{Q}(P_n)$ and $\Gbar_n \equiv \hat{G}(P_n)$. Let's denote by 
$\hat{P}_n$ a distribution in $\calM$ that implies $\Qbar_{\hat{P}_n} = \Qbar_n$, 
$\Gbar_{\hat{P}_n} = \Gbar_n$ and $Q_{\hat{P}_n,W} = Q_{n,W}$. We can then apply
equation (\ref{taylor_expansion}) substituting this $\hat{P}_n$ for $P$: \begin{equation} \label{hard_to_study}
\Psi(\hat{P}_n) - \Psi(P_0) = - P_0 D*(\hat{P}_n) + R_{P_0}(\hat{P}_n) . 
\end{equation}
We remind readers of our shorthand notation, \begin{equation*}
P_0 D*(\hat{P}_n) \equiv \Exp_{P_0}\left( \frac{2A - 1}{\ell \Gbar_n(A,W)} (Y - \Qbar_n(A,W)) + \Qbar_n(1,W) - \Qbar_n(0,W) - \Psi(\hat{P}_n) \right) . 
\end{equation*}
To facilitate our study of (\ref{hard_to_study}), we perform some simple algebra, \begin{align*}
\Psi(\hat{P}_n) - \Psi(P_0) &= - P_0 D*(\hat{P}_n) + R_{P_0}(\hat{P}_n) \pm P_n D*(\hat{P}_n) \\
&= (P_n - P_0) D*(\hat{P}_n) - P_n D*(\hat{P}_n) + R_{P_0}(\hat{P}_n) \pm (P_n - P_0) D*(P_0) \\
&= (P_n - P_0) D*(P_0) - P_n D*(\hat{P}_n) + R_{P_0}(\hat{P}_n) + (P_n - P_0) (D*(\hat{P}_n) - D*(P_0)) . 
\end{align*}

The first term $(P_n - P_0) D*(P_0) \equiv n^{-1} \sum_{i=1}^n D*(P_0)(O_i)$ is 
a sum of independent transformations of the data that have mean zero and finite variance. 
Thus, the weak law of large numbers and central limit theorem imply that this term should
be well-behaved statistically.

Turning to the term $R_{P_0}(\hat{P}_n)$, we note that this term is an expectation involving
the product of two differences, one between $\Qbar_n$ and $\Qbar_0$, the other between 
$\Gbar_n$ and $\Gbar_0$. Thus, we expect that if $\Qbar_n$ and $\Gbar_n$ are good 
estimates of $\Qbar_0$ and $\Gbar_0$ respectively, then this product of differences 
should be small, and, we should hope, asymptotically negligible. Formal conditions
for negligibility of this term can be made via the triangle inequality\footnote{Add triangle inequality argument}.

Considering the final term $(P_n - P_0) (D*(\hat{P}_n) - D*(P_0)),$ we note that this 
term two involves a product of two differences, one between $P_n$ and $P_0$ and one between
$D^*(\hat{P}_n)$ and $D^*(P_0)$. However, analysis of this term is more complicated than
that of the remainder, and we require results from empirical process theory to provide 
conditions on the asymptotic negligibility of this term\footnote{Add footnote on Donsker
conditions}. We proceed assuming that such conditions are satisfied. 

Finally, we turn to the term $P_n D*(\hat{P}_n)$. This term is the source of the irregular 
behavior of the plug-in estimator. Notice that the data appear in this term twice, through $\hat{P}_n$ and through $P_n$ itself. Thus, the term involves a empirical average of a data-dependent transformation of the data. Such terms are difficult to analyze, and we 
have no reason to suspect that if $\hat{P}_n$ involves flexible regression techniques, this term will be asymptotically negligible. Indeed, we can study numerically study the behavior of this term via Monte Carlo simulation. The next code chunk analyzes its behavior. 

```{r show-rootn-times-PnDstar-blow up}

```

This simulation demonstrates that, as expected, $n^{1/2}P_n D*(\hat{P}_n) \rightarrow \infty$ and thus is the source of the irregular behavior of the plug-in estimate. 


1. \tcg{Design a simulation study to empirically evaluate $n^{1/2} R_{P_0}(\hat{P}_n)$.}

2. \tcg{Design a simulation study to empirically evaluate $n^{1/2} (P_n - P_0) (D*(\hat{P}_n) - D*(P_0))}.

3. \tcg{A "difficult" exercise. Repeat exercise 2, but let's try to break Donsker conditions. E.g., could we blow the Donsker condition up by including an increasing number of polynomials?}

## One-step corrections.

In this section, we discuss a simple strategy for correcting the behavior of the plug-in
estimate of $\psi_0$. Recall that an estimator $\psi_n$ of $\psi_0$ 
is asymptotically linear if we can write $\psi_n - \psi_0 = \frac{1}{n} \sum_{i=1}^n \mbox{IF}(O_i) + o_P(n^{-1/2})$. Now, in the previous subsection we saw that, under
certain regularity conditions, the plug-in estimator could be written as \begin{equation} \label{pre_one_step}
  \Psi(\hat{P}_n) = \Psi(P_0) + \frac{1}{n} \sum_{i=1}^n D*(P_0)(O_i) - \frac{1}{n} \sum_{i=1}^n D*(\hat{P}_n)(O_i) + o_P(n^{-1/2}). 
\end{equation}
In other words, we found that the estimator *very nearly* allowed for an asymptotically 
linear expression; however, the problematic term $\frac{1}{n} \sum_{i=1}^n D*(\hat{P}_n)(O_i)$ 
prevents our claiming asymptotic linearity. Nevertheless, (\ref{pre_one_step}) implies
that \[
  \Psi(\hat{P}_n) + \frac{1}{n} \sum_{i=1}^n D*(\hat{P}_n)(O_i) = \Psi(P_0) + \frac{1}{n} \sum_{i=1}^n D*(P_0)(O_i) + o_P(n^{-1/2}). 
\]
In other words, under the regularity conditions introduced in the previous subsection, the corrected plug-in estimator \[
  \psi_n^{+} \equiv \Psi(\hat{P}_n) + \frac{1}{n} \sum_{i=1}^n D*(\hat{P}_n)(O_i)
\]
is an asymptotically linear estimator with influence function equal to D*(P_0). That is 
to say, in light of section \ref{subsec:parameter:third}, $\psi_n^{+}$ is an *asymptotically efficient* estimate of $\psi_0$!

This simple strategy for generating asymptotically linear and efficient estimators 
is often referred to as a one-step estimator, since it generalizes the familiar
one-step Newton-Raphson estimator that is often used in the context of parametric models. 

The next code chunk demonstrates that the one-step correction indeed yields an 
efficient estimator of $\psi_0$. 

```{r show-one-step-in-simulation}

```

Recall also that the central  limit theorem provides for the weak convergence of 
asymptotically linear estimators of $\psi_0$ to a centered Gaussian distribution with 
variance $\Var_{P_0}(D*(P_0))$. The fact that the form of the asymptotic variance is
explicitly available through our knowledge of $D*$ allows for simple estimators 
of the asymptotic variance of $\psi_n^{+}$. Namely, under weak conditions, the empirical variance of $D^*(\hat{P}_n)$ will consistently estimate $\Var_{P_0}(D*(P_0))$. Denoting this estimate by $\hat{\sigma}_n^2$, by Slutsky's theorem, we can argue that $n^{1/2}(\psi_n^{+} - \psi_0) / \hat{sigma}_n$ converges in distribution to that of a centered Gaussian variate 
with unit variance. 


```{r show-weak-convergence}

```

This convergence result also paves the way for the construction of Wald-style confidence intervals with coverage probability $1-\alpha$, which take the form $\psi_n \pm n^{-1/2} \sigma_n z_{1-\alpha/2}$, where $z_{1-\alpha/2}$ is the $1 - \alpha/2$ quantile of a 
center Gaussian variate with unit variance. 

```{r show-confidence-intervals}

```

# &#x2621; Targeted minimum loss-based estimation {#TMLE}

In the previous section, we introduced the one-step corrected estimator of
$\psi_0$. This estimator added a correction term to the plug-in estimator, 
which resulted in an estimator that is asymptotically linear and efficient. 
However, the one-step estimator has lost a desirable feature of a plug-in estimator:
plug-in estimators always obey bounds on the parameter space. For example, the law 
$P_0$ stipulates that $Y$ has a conditional Beta distribution and thus $Y \in (0,1)$. 
Thus, it must also be true that $\psi_0 \in [-1,1]$. It is typically easy to generate an 
estimate $\Qbar_n$ of $\Qbar_0$ that obeys bounds on the parameter space for $\Qbar_0$. 
The plug-in estimate based on on such an estimate will also obey the bounds on the 
parameter space for $\psi_0$. That is, our plug-in estimate of $\psi_0$ is guaranteed by
its very construction be between -1 and 1. However, the one-step estimate adds 
an additional term to the plug-in and therefore may result in an estimate of $\psi_0$ 
that is outside its parameter space. Upon closer examination of the efficient influence
function $D*$, we see that this may occur more frequently when $\ell \Gbar_n(A_i,W_i)$ is 
close to zero for at least some $i$. In words, if there are observations in our 
data set that we observed under actions $A_i$ that were estimated to be unlikely given the context $W_i$. In such cases, $D*(\hat{P}_n)(O_i)$, and consequently the one-step 
correction term, may be large and cause the one-step estimate to fall outside the parameter
space. 

Another way to understand this behavior is to recognize the one-step estimator as
a plug-in estimator that is corrected *in the parameter space of $\psi_0$*. On the
other hand, we might consider performing a correction *in the parameter space of $\Qbar_0$*. 
In particular, consider a $\hat{P}_n^*$ that is compatible with the estimates $\Qbar_n^*$, $\Gbar_n$, and $Q_{n,W}$, but moreover is such that $P_n D*(\hat{P}_n^*) = 0$ (or at the very least equals to $o_P(n^{-1/2})$). If indeed, such estimates can be generated, then the 
plug-in estimator \[
  \psi_n^* \equiv \int (\Qbar_n^*(1,w) - \Qbar_n^*(0,w)) dQ_{n,W}(w)
\]
is asymptotically linear and efficient. Moreover, by virtue of its plug-in construction it 
has the additional property that in finite-samples $\psi_n^*$ will always obey
bounds on the parameter space. 

Our strategy for constructing such a plug-in estimate begins by generating an 
initial estimate $\Qbar_n$ of $\Qbar_0$. This initial estimate should strive to be
as close as possible to $\Qbar_0$. Now, the goal is to select a new regression
function "near to" $\Qbar_n$ such that $P_n D*(\hat{P}_n^*) = 0$. Furthermore, because
$\Qbar_n$ is our "best guess" for the true regression $\Qbar_0$, the new regression
that we pick should be *at least as good* of an estimate of $\Qbar_0$ as $\Qbar_n$. 
Thus, we must determine a way to move between regressions in our model *and* we
must determine a way to move to a new regression that fits the data at least as 
well as $\Qbar_n$. To move about the model space, we will make use of parametric
submodels introduced in Section \ref{subsec:being:smooth:one}. In particular, let's consider
the following parametric submodel \[
  \calQ(\Gbar_n) = \{ \Qbar_{n,\epsilon}(A,W) = \expit\left(\logit(\Qbar_n(A,w)) + \epsilon \frac{2A - 1}{\ell \Gbar_n(A,W)} \right) : \epsilon \in \bbR \} . 
\]
We note that: (i) this submodel depends on the estimate of $\Gbar_0$; (ii) the submodel is 
indexed by the univariate parameter $\epsilon$; and (iii) the submodel goes *through*
the initial estimate $\Qbar_n$ of $\Qbar_0$. The code below visualizes regressions
for several values of $\epsilon$. 

```{r plot-for-several-epsilon}
# can include the graph of Qbar_n,eps(a,W) ~ W for eps = -1, 0, 1 \times a = 0,1?
# would like to drive home the point that each epsilon is simply a different regresion
# fit.

# could also show for different Gbar_n showing that the submodel depends on Gbar_n, 
# but I actually think that's far less important. 
```

The plot shows that each $\epsilon$ corresponds to a different regression function. 
Recalling that our goal is to select an estimate of $\Qbar_0$ that is *at least as good* 
as $\Qbar_n$. This goal is now translated into selecting an $\epsilon$ that corresponds to
a regression function that fits the data *at least as well* as $\Qbar_n$. To formalize
this, we require a metric for defining how well a regression function estimates the true
regression function. For this, we estimate the average negative log-likelihood loss, defined for a candidate regression function $\Qbar as \[
  \bbD_0(\Qbar) \equiv -\Exp_{P_0}( Y \log(\Qbar(A,W)) + (1 - Y)\log(1 - \Qbar(A,W)) ) . 
\]
We note that the minimizer of $\bbD_0$ over all functions mapping from $\{0,1\} \times (0,1)$ to $\bbR$ is equal to $\Qbar_0$. Thus, $\bbD_0$ provides a concrete means of evaluating the 
fit of different candidate regression functions to the true regression function $\Qbar_0$. Because, $\bbD_0$ depends on $P_0$, it is often unknown to us in practice. Thus, we may instead compute a natural estimate of $\bbD_0$, \[
  \bbD_n(\Qbar) \equiv -\frac{1}{n} \sum_{i = 1}^n \left( Y_i \log(\Qbar(A_i, W_i)) + (1 - Y_i)\log(1 - \Qbar(A_i, W_i)) \right). 
\]
The code below computes this estimate of $\bbD_n(\Qbar_{n,\epsilon})$ over a fine grid of epsilon and plots the resulting function. 

```{r compute-negloglik-and-plot}
# basically just do a grid search over epsilon for minimizer of negative log-likelihood loss
```

The above plot shows the estimated average negative log-likelihood loss for each regression function in the parametric submodel $\calQ(\Gbar_n)$. Now, we should like to move to
the regression with the smallest estimated average negative log-likelihood, which 
was achieved at $\epsilon = $ \tcg{fill in from R output}. Because our submodel is through 
$\Qbar_n$, that is, $\Qbar_n$ is itself included in our submodel, and this newly selected
regression estimate has a better estimated average negative log-likelihood loss, we should
hope that the new estimate is at least *no further from $\Qbar_0$* than $\Qbar_n$. 

But what about the second \tcg{DAVID STOPPED HERE!}



\tcg{Make point about  one-step being able to go outside  the parameter space,
but plug-in  estimator will not do  so.  Can we  do a correction in  the model
space? Can we pick  a new $\Qbar_n$ that satisfies the  key equation? We would
like the  new $\Qbar_n$  to be  near to $\Qbar_0$  in some  sense. How  can we
define  a set  of  distributions  that are  near  to  a current  distribution?
Parametric submodels!   Define a (local  least favorable?) submodel  through a
current  estimate. Pick  one data  set and  visualize some  $\Qbar_{\epsilon}$
along that submodel. Make the point that the submodel simply defines different
regression estimates of $\Qbar_0$. Now we have a way to select a collection of
distributions near to  our initial estimate of $\Qbar_0$. Which  one should we
pick?   One that  gets us  closer to  $\Qbar_0$!  How  do we  define close  to
$\Qbar_0$?   Loss functions:  define  and introduce  log  likelihood loss  for
$\Qbar_0$. Say that the  MLE along the submodel is the  member of the submodel
that the  data think  is closest to  $\Qbar_0$. Now  we have a  way to  pick a
distribution that moves from $\Qbar_n$  towards $\Qbar_0$ in some sense. Thus,
we have  a sensible  way to  move around  our model  space. But  what submodel
should we  use? Recall  the goal is  to generate an  estimate of  $\Qbar$ that
solves  the  specified equation.   Remind  the  reader  that  at the  MLE  the
derivative of the  loss function (i.e., generalized score)  equals zero. Thus,
if we arrange the  submodel and loss function to have  derivative equal to the
desired equation  then at the  MLE the desired  equation will be  solved. Note
that  its   generally  difficult  (though  possible,   footnote  on  universal
submodel?) to  arrange a submodel with  derivative equal to the  EIF for every
fluctuation parameter  value. However, it is  easy to arrange a  submodel with
derivative  equal  to  the  EIF  at zero.   Introduce  local  least  favorable
model. Now describe the iterative process. Place a local least favorable model
through  the   current  estimate  and  look   for  a  better  fit   along  the
submodel. Eventually we  should hope that that  there is no better  fit in the
submodel than the current estimate. That is,  the MLE is zero, so the equation
(i.e.,  derivative of  the  loss) is  solved  at zero,  i.e.,  at the  current
estimate. Then we use that estimate. Show  that it only takes a single step in
the current problem to arrive at that solution. Phew, this section is going to
be  a lot!  TMLE is  hard to  explain...   Show code  for TMLE.  Show that  it
corrects  G-computation  estimator.  Compare  MSE of  one-step  TMLE?  Briefly
discuss asymptotic linearity and inference. Show confidence intervals.  }

\tcg{Big questions left in my mind. 1. Do  we talk about IPTW at all? 2. Do we
discuss double-robustness  at all? Maybe  just include an  aside on DR  in the
estimator  section (after  one-step  section?)  to mirror  the  DR section  in
Chapter 2?}


\tcg{ !!!DAVID STOPPED HERE!!! }
<!-- However,   there  is   much   more  to   double-robustness   than  the   above
straightforward  implication. Indeed,  \ref{eq:rem:one} is  useful to  build a
consistent etimator of $\Psi(P)$ that,  in addition, satisfies a central limit
theorem and thus lends itsef to the construction of confidence intervals. -->

Let $P_{n}^{0} \in \calM$  be an element of model $\calM$  of which the choice
is data-driven, based  on observing $n$ independent draws  from $P$.  Equality
\ref{eq:rem:one} reveals  that the  statistical behavior of  the corresponding
\textit{substitution}  estimator  $\psi_{n}^{0}   \equiv  \Psi(P_{n}^{0})$  is
easier  to   analyze  when   the  remainder  term   $\Rem_{P}  (\Qbar_{n}^{0},
\Gbar_{n}^{0})$ goes  to zero  at a  fast (relative to  $n$) enough  rate.  In
light of  \ref{eq:rem:two}, this happens  if the features  $\Qbar_{n}^{0}$ and
$\Gbar_{n}^{0}$ of  $P_{n}^{0}$ converge  to their  counterparts under  $P$ at
rates of which \textit{the product} is fast enough.

# Inference assuming the mechanism of action known, or not, first pass {#known-gbar-first-pass}

Let us  assume for a moment  that we know  $\Gbar_{0}$.  This may be  the case
indeed if  $P_{0}$ was a  controlled experiment.  Note that, on  the contrary,
assuming $\Qbar_{0}$ known would be difficult to justify. 

```{r known-Gbar-one-a}
Gbar <- get_feature(experiment, "Gbar")
iter <- 1e3
```

Then,  the   alternative  expression  \ref{eq:psi0:b}  suggests   to  estimate
$\psi_{0}$    with    \begin{equation}\label{eq:psi:n:b}\psi_{n}^{b}    \equiv
\Exp_{P_{n}}   \left(\frac{2A-1}{\ell\Gbar_{0}(A,W)}Y\right)   =   \frac{1}{n}
\sum_{i=1}^{n}
\left(\frac{2A_{i}-1}{\ell\Gbar_{0}(A_{i},W_{i})}Y_{i}\right).\end{equation}
Note how $P_{n}$ is substituted  for $P_{0}$ in \eqref{eq:psi:n:b} relative to
\eqref{eq:psi0:b}.

It is easy to check that $\psi_{n}^{b}$ estimates $\psi_{0}$ consistently, but
this  is too  little  to request  from an  estimator  of $\psi_{0}$.   Better,
$\psi_{n}^{b}$   also   satisfies   a   central   limit   theorem:   $\sqrt{n}
(\psi_{n}^{b} -  \psi_{0})$ converges in law  to a centered Gaussian  law with
asymptotic     variance     \begin{equation*}v^{b}     \equiv     \Var_{P_{0}}
\left(\frac{2A-1}{\ell\Gbar_{0}(A,W)}Y\right),\end{equation*}   where  $v^{b}$
can    be    consistently    estimated   by    its    empirical    counterpart
\begin{equation}\label{eq:v:n:b}      v_{n}^{b}       \equiv      \Var_{P_{n}}
\left(\frac{2A-1}{\ell\Gbar_{0}(A,W)}Y\right)  =   \frac{1}{n}  \sum_{i=1}^{n}
\left(\frac{2A_{i}-1}{\ell\Gbar_{0}(A_{i},W_{i})}Y_{i}                       -
\psi_{n}^{b}\right)^{2}.\end{equation}

Let us investigate how $\psi_{n}^{b}$ behaves  based on `obs`.  Because we are
interested  in the  \textit{law} of  $\psi_{n}^{b}$,  the next  chunk of  code
constitutes `iter =` `r iter`  independent samples of independent observations
drawn from $P_{0}$, each consisting of $n$ equal to `nrow(obs)/iter =` 
`r nrow(obs) / iter` data points, and computes the realization of $\psi_{n}^{b}$
on all samples.

Before  proceeding,   let  us  introduce   \begin{align*}\psi_{n}^{a}  &\equiv
\Exp_{P_{n}} \left(Y  | A=1\right) -  \Exp_{P_{n}} \left(Y | A=0\right)  \\ &=
\frac{1}{n_{1}}   \sum_{i=1}^{n}  \one\{A_{i}=1\}   Y_{i}  -   \frac{1}{n_{0}}
\sum_{i=1}^{n} \one\{A_{i}=0\} Y_{i}  \\&=\frac{1}{n_{1}} \sum_{i=1}^{n} A_{i}
Y_{i} - \frac{1}{n_{0}}  \sum_{i=1}^{n} (1 - A_{i})  Y_{i}, \end{align*} where
$n_{1}  = \sum_{i=1}^{n}  A_{i} =  n -  n_{0}$ is  the number  of observations
$O_{i}$    such   that    $A_{i}   =    1$.    It    is   an    estimator   of
\begin{equation*}\Exp_{P_{0}}    (Y   |    A=1)    -    \Exp_{P_{0}}   (Y    |
A=0).\end{equation*} We seize this  opportunity to demonstrate numerically the
obvious fact that $\psi_{n}^{a}$ does not estimate $\psi_{0}$.

(ref:known-Gbar-one-b) Kernel density estimators of  the law of two estimators
of $\psi_{0}$ (recentered  with respect to $\psi_{0}$,  and renormalized), one
of  them  misconceived (a),  the  other  assuming  that $\Gbar_{0}$  is  known
(b). Built based on `iter` independent realizations of each estimator.

```{r known-Gbar-one-b, fig.cap = '(ref:known-Gbar-one-b)'}
psi_hat_ab <- obs %>% as_tibble() %>%
  mutate(id = (seq_len(n()) - 1) %% iter) %>%
  mutate(lGAW = A * Gbar(W) + (1 - A) * (1 - Gbar(W))) %>% group_by(id) %>%
  summarize(est_a = mean(Y[A==1]) - mean(Y[A==0]),
            est_b = mean(Y * (2 * A - 1) / lGAW),
            std_b = sd(Y * (2 * A - 1) / lGAW) / sqrt(n()),
            clt_b = (est_b - psi_approx) / std_b) %>% 
  mutate(std_a = sd(est_a),
         clt_a = (est_a - psi_approx) / std_a) %>%
  gather("key", "value", -id) %>%
  extract(key, c("what", "type"), "([^_]+)_([ab])") %>%
  spread(what, value)

(bias_ab <- psi_hat_ab %>% group_by(type) %>% summarise(bias = mean(clt)))

fig <- ggplot() +
  geom_line(aes(x = x, y = y), 
            data = tibble(x = seq(-3, 3, length.out = 1e3),
                          y = dnorm(x)),
            linetype = 1, alpha = 0.5) +
  geom_density(aes(clt, fill = type, colour = type),
               psi_hat_ab, alpha = 0.1) +
  geom_vline(aes(xintercept = bias, colour = type),
             bias_ab, size = 1.5, alpha = 0.5)
  
fig +
  labs(y = "",
       x = expression(paste(sqrt(n/v[n]^{list(a, b)})*(psi[n]^{list(a, b)} - psi[0]))))
```

Let $v_{n}^{a}$ be $n$ times the empirical variance of the `iter` realizations
of   $\psi_{n}^{a}$.   By   the  above   chunk  of   code,  the   averages  of
$\sqrt{n/v_{n}^{a}}   (\psi_{n}^{a}  -   \psi_{0})$  and   $\sqrt{n/v_{n}^{b}}
(\psi_{n}^{b}  -  \psi_{0})$  computed  across the  realizations  of  the  two
estimators are respectively equal to 
`r round(bias_ab$bias[bias_ab$type=="a"],3)` and 
`r round(bias_ab$bias[bias_ab$type=="b"], 3)` (both rounded to three decimal
places  ---  see  `bias_ab`).   Interpreted  as amounts  of  bias,  those  two
quantities    are     represented    by     vertical    lines     in    Figure
\@ref(fig:known-Gbar-one-b). The red and blue bell-shaped curves represent the
empirical laws  of $\psi_{n}^{a}$ and $\psi_{n}^{b}$  (recentered with respect
to   $\\psi_{0}$,   and  renormalized)   as   estimated   by  kernel   density
estimation.  The latter  is close  to the  black curve,  which represents  the
standard normal density.

# Inference assuming the mechanism of action known, or not, second pass {#known-gbar-second-pass}


At  the beginning  of  Section \@ref(known-gbar-first-pass),  we assumed  that
$\Gbar_{0}$ was known.  Let us suppose now  that it is not.  The definition of
$\psi_{n}^{b}$ can be adapted to  overcome this difficulty, by substituting an
estimator of $\ell\Gbar_{0}$ for $\ell\Gbar_{0}$ in \eqref{eq:psi:n:b}.

For  simplicity,  we  consider  the  case that  $\Gbar_{0}$  is  estimated  by
minimizing   a   loss    function   on   a   single    working   model,   both
fine-tune-parameter-free.   By adopting  this  stance,  we exclude  estimating
procedures that involve penalization  (\textit{e.g.} the LASSO) or aggregation
of competing estimators (\textit{via}  stacking/super learning) -- see Section
\@ref(subsec:exo:one).   Defined  in  the  next chunk  of  code,  the  generic
function `estimate_Gbar`  fits a user-specified  working model by  minimizing the
empirical risk  associated to  the user-specified  loss function  and provided
data.

\tcg{Comment on new structure of} `estimate_Gbar` \tcg{and say a few words about}
`compute_lGbar_hatAW`.

```{r unknown-Gbar-one}
## estimate_Gbar <- function(dat, algorithm, ...) {
##   if (!is.data.frame(dat)) {
##     dat <- as.data.frame(dat)
##   }
##   if (!attr(algorithm, "ML")) {
##     fit <- algorithm[[1]](formula = algorithm[[2]], data = dat)
##   } else {
##     fit <- algorithm[[1]](dat, ...)
##   }
##   fit$type_of_preds <- algorithm$type_of_preds
##   return(fit)
## }

## compute_lGbar_hatAW <- function(A, W, Ghat, threshold = 0.05) {
##   dat <- data.frame(A = A, W = W)
##   Ghat_W <- predict(Ghat, newdata = dat, type = Ghat$type_of_preds)
##   lGAW <- A * Ghat_W + (1 - A) * (1 - Ghat_W)
##   pred <- pmin(1 - threshold, pmax(lGAW, threshold))
##   return(pred)
## }
```


Note how the  prediction of any $\ell\Gbar_{0}(A,W)$ is  manually bounded away
from 0 and 1 at the last  but one line of `compute_lGhatAW`. This is desirable
because the  \textit{inverse} of each $\ell\Gbar_{0}(A_{i},W_{i})$  appears in
the definition of $\psi_{n}^{b}$ \eqref{eq:psi:n:b}.

For sake of illustration, we choose argument `working_model_G_one` of function
`estimate_Gbar` as follows:

```{r fake}
## empty...
```

In words, we choose the so called logistic (or negative binomial) loss
function    $L_{a}$    given   by    \begin{equation}    \label{eq:logis:loss}
-L_{a}(f)(A,W) \equiv A \log f(W) + (1 - A) \log (1 - f(W)) \end{equation} for
any function $f : [0,1] \to [0,1]$ paired with the working model $\calF \equiv
\left\{f_{\theta} :  \theta \in \bbR^{5}\right\}$  where, for any  $\theta \in
\bbR^{5}$,  $\logit   f_{\theta}  (W)   \equiv  \theta_{0}   +  \sum_{j=1}^{4}
\theta_{j}  W^{j/2}$. The  working model  is well  specified: it  happens that
$\Gbar_{0}$  is the  unique minimizer  of the  risk entailed  by $L_{a}$  over
$\calF$: \begin{equation*}\Gbar_{0} = \mathop{\arg\min}_{f_{\theta} \in \calF}
\Exp_{P_{0}}   \left(L_{a}(f_{\theta})(A,W)\right).\end{equation*}  Therefore,
the estimator  $\Gbar_{n}$ output by  `estimate_Gbar` and obtained  by minimizing
the        empirical         risk        \begin{equation*}        \Exp_{P_{n}}
\left(L_{a}(f_{\theta})(A,W)\right)      =     \frac{1}{n}      \sum_{i=1}^{n}
L_{a}(f_{\theta})(A_{i},W_{i})\end{equation*}   over    $\calF$   consistently
estimates $\Gbar_{0}$.

In light of  \eqref{eq:psi:n:b}, introduce \begin{equation}\psi_{n}^{c} \equiv
\frac{1}{n}   \sum_{i=1}^{n}   \left(\frac{2A_{i}  -   1}{\ell\Gbar_{n}(A_{i},
W_{i})}   Y_{i}\right).\end{equation}   Because  $\Gbar_{n}$   minimizes   the
empirical  risk over  a finite-dimensional  and well-specified  working model,
$\sqrt{n} (\psi_{n}^{c} -  \psi_{0})$ converges in law to  a centered Gaussian
law. Let us compute $\psi_{n}^{c}$ on the  same `iter = ` `r iter` independent
samples  of  independent  observations  drawn   from  $P_{0}$  as  in  Section
\@ref(known-gbar-first-pass):

```{r unknown-Gbar-two-bis}
if (redo_fixed) {
  learned_features_fixed_sample_size <-
    obs %>% as_tibble() %>%
    mutate(id = (seq_len(n()) - 1) %% iter) %>%
    nest(-id, .key = "obs") %>%
    mutate(Gbar_hat = map(obs, ~ estimate_Gbar(., algorithm = working_model_G_one))) %>%
    mutate(lGAW = map2(Gbar_hat, obs, ~ compute_lGbar_hatAW(.y$A, .y$W, .x)))
}

psi_hat_abc <-
  learned_features_fixed_sample_size %>%
  unnest(obs, lGAW) %>%
  group_by(id) %>%
  summarize(est = mean(Y * (2 * A - 1) / lGAW)) %>%
  mutate(std = sd(est),
         clt = (est - psi_approx) / std,
         type = "c") %>%
  full_join(psi_hat_ab)

## DEBUG : This was breaking when I compiled.
(bias_abc <- psi_hat_abc %>% group_by(type) %>% summarise(bias = mean(clt)))
```

Note how we exploit the independent realizations of $\psi_{n}^{c}$ to estimate
the  asymptotic variance  of the  estimator with  $v_{n}^{c}/n$. By  the above
chunk of code,  the average of $\sqrt{n/v_{n}^{c}}  (\psi_{n}^{c} - \psi_{0})$
computed across the realizations is equal to 
`r round(bias_abc$bias[bias_abc$type=="c"],3)` (rounded to three decimal places
--- see `bias_abc`).  We represent the empirical laws of  the recentered (with
respect to  $\\psi_{0}$) and  renormalized $\psi_{n}^{a}$,  $\psi_{n}^{b}$ and
$\psi_{n}^{c}$  in   Figures  \@ref(fig:unknown-Gbar-three)   (kernel  density
estimators) and \@ref(fig:unknown-Gbar-four) (quantile-quantile plots). 
 
(ref:unknown-Gbar-three)  Kernel  density  estimators  of  the  law  of  three
estimators  of   $\psi_{0}$  (recentered  with  respect   to  $\psi_{0}$,  and
renormalized), one of them misconceived  (a), one assuming that $\Gbar_{0}$ is
known  (b) and  one that  hinges  on the  estimation of  $\Gbar_{0}$ (c).  The
present  figure includes  Figure \@ref(fig:known-Gbar-one-b)  (but the  colors
differ). Built based on `iter` independent realizations of each estimator.
 
```{r unknown-Gbar-three, fig.cap = '(ref:unknown-Gbar-three)'}
fig +
  geom_density(aes(clt, fill = type, colour = type), psi_hat_abc, alpha = 0.1) +
  geom_vline(aes(xintercept = bias, colour = type),
             bias_abc, size = 1.5, alpha = 0.5) +
  xlim(-3, 4) + 
  labs(y = "",
       x = expression(paste(sqrt(n/v[n]^{list(a, b, c)})*
                            (psi[n]^{list(a, b, c)} - psi[0]))))
```

(ref:unknown-Gbar-four)  Quantile-quantile plot  of  the  standard normal  law
against the  empirical laws  of three  estimators of  $\psi_{0}$, one  of them
misconceived (a),  one assuming  that $\Gbar_{0}$  is known  (b) and  one that
hinges  on  the  estimation  of   $\Gbar_{0}$  (c).   Built  based  on  `iter`
independent realizations of each estimator.

```{r unknown-Gbar-four, fig.cap = '(ref:unknown-Gbar-four)'}
ggplot(psi_hat_abc, aes(sample = clt, fill = type, colour = type)) +
  geom_abline(intercept = 0, slope = 1, alpha = 0.5) +
  geom_qq(alpha = 1)
```

Figures \@ref(fig:unknown-Gbar-three)  and \@ref(fig:unknown-Gbar-four) reveal
that $\psi_{n}^{c}$ behaves as well as $\psi_{n}^{b}$ --- but remember that we
did not discuss how to estimate its asymptotic variance.

# &#9881; A nice title... {#exo-a-nice-title}

1. Compute a numerical approximation of $\Exp_{P_{0}} (Y | A=1) - \Exp_{P_{0}}
(Y  | A=0)$.  How accurate  is it?  \tcg{I would  perhaps suggest  moving this
exercise and the  discussion of this parameter to earlier.  I.e., could have a
whole section  on how our target  parameter differs from this  guy. Then could
return to it in Sec. 3 with estimators?}

2. Building upon the piece of code devoted to the repeated computation of
$\psi_{n}^{b}$ and  its companion  quantities, construct  confidence intervals
for  $\psi_{0}$ of  (asymptotic)  level  $95\%$, and  check  if the  empirical
coverage is satisfactory.  Note that if  the coverage was exactly $95\%$, then
the number of confidence intervals  that would contain $\psi_{0}$ would follow
a binomial  law with parameters  `iter` and  `0.95`, and recall  that function
`binom.test` performs  an exact  test of  a simple  null hypothesis  about the
probability of success  in a Bernoulli experiment against  its three one-sided
and two-sided alternatives.

3.  The call to `compute_lGbar_hatAW` makes predictions on the same data points as
those  exploited to  learn $\Gbar_{0}$  by fitting  the user-supplied  working
model. Why  could that be problematic?  Can you think of  a simple workaround,
implement and test it?

4.  Discuss what happens when the dimension of the (still well-specified)
working model grows. You could use the following chunk of code
```{r exercises-one, eval = FALSE}
powers <- ## make sure '1/2' and '1' belong to 'powers', eg
  seq(1/4, 3, by = 1/4)
working_model_G_two <- list(
  model = function(...) {trim_glm_fit(glm(family = binomial(), ...))},
  formula = as.formula(
    paste("A ~",
          paste(c("I(W^", "I(abs(W - 5/12)^"),
                rep(powers, each = 2),
                sep = "", collapse = ") + "),
          ")")
  ),
  type_of_preds = "response"
)
attr(working_model_G_two, "ML") <- FALSE
```
play around with  argument `powers` (making sure that `1/2`  and `1` belong to
it),   and   plot   graphics   similar   to   those   presented   in   Figures
\@ref(fig:unknown-Gbar-three) and \@ref(fig:unknown-Gbar-four). 

5. Discuss  what happens when the  working model is mis-specified.   You could
use the following chunk of code:
```{r exercises-two, eval = TRUE}
transform <- c("cos", "sin", "sqrt", "log", "exp")
working_model_G_three <- list(
  model = function(...) {trim_glm_fit(glm(family = binomial(), ...))},
  formula = as.formula(
    paste("A ~",
          paste("I(", transform, sep = "", collapse = "(W)) + "),
          "(W))")
  ),
  type_of_preds = "response"
)
attr(working_model_G_three, "ML") <- FALSE
(working_model_G_three$formula)
```

6.   &#x2621;  Drawing inspiration  from \eqref{eq:v:n:b}, one  may consider
estimating the asymptotic  variance of $\psi_{n}^{c}$ with  the counterpart of
$v_{n}^{b}$ obtained  by substituting  $\ell\Gbar_{n}$ for  $\ell\Gbar_{0}$ in
\eqref{eq:v:n:b}.   By adapting  the piece  of  code devoted  to the  repeated
computation of  $\psi_{n}^{b}$ and its  companion quantities, discuss  if that
would be legitimate.

# Inference based on the estimation of the reward regression {#inference-reward-regression}

\tcg{Comment on structure of} `estimate_Qbar`, similar to that of `estimate_Gbar`.

\tcg{Demonstrating the  inference of} $\psi_{0}$ \tcg{based  on the estimation
of} $\Qbar_{0}$ \tcg{(and of the marginal  law of} $W$\tcg{).  Once based on a
(mis-specified) working model, and once based on a non-parametric algorithm.}

```{r estimating-Qbar-one}
## estimate_Q <- function(dat, algorithm, ...) {
##   if (!is.data.frame(dat)) {
##     dat <- as.data.frame(dat)
##   }
##   if (!attr(algorithm, "ML")) {
##     fit <- algorithm[[1]](formula = algorithm[[2]], data = dat)
##   } else {
##     fit <- algorithm[[1]](dat, ...)
##   }
##   fit$type_of_preds <- algorithm$type_of_preds
##   return(fit)
## }

## compute_QhatAW <- function(Y, A, W, Qhat, blip = FALSE) {
##   if (!blip) {
##     dat <- data.frame(Y = Y, A = A, W = W)
##     pred <- predict(Qhat, newdata = dat, type = Qhat$type_of_preds)
##   } else {
##     pred <- predict(Qhat, newdata = data.frame(A = 1, W = W),
##                     type = Qhat$type_of_preds) -
##       predict(Qhat, newdata = data.frame(A = 0, W = W),
##               type = Qhat$type_of_preds)
##   }
##   return(pred)  
## }

## working_model_Q_one <- list(
##   model = function(...) {trim_glm_fit(glm(family = binomial(), ...))},
##   formula = as.formula(
##     paste("Y ~ A * (",
##           paste("I(W^", seq(1/2, 3/2, by = 1/2), sep = "", collapse = ") + "),
##           "))")
##   ),
##   type_of_preds = "response"
## )
## attr(working_model_Q_one, "ML") <- FALSE
## working_model_Q_one$formula

## ## k-NN
## kknn_algo <- list(
##   algo = function(dat, ...) {
##     args <- list(...)
##     if ("Subsample" %in% names(args)) {
##       keep <- sample.int(nrow(dat), args$Subsample)
##       dat <- dat[keep, ]
##     }
##     fit <- caret::train(Y ~ I(10*A) + W, ## a tweak
##                         data = dat,
##                         method = "kknn",
##                         verbose = FALSE,
##                         ...)
##     fit$finalModel$fitted.values <- NULL
##     ## nms <- names(fit$finalModel$data)
##     ## for (ii in match(setdiff(nms, ".outcome"), nms)) {
##     ##   fit$finalModel$data[[ii]] <- NULL
##     ## }
##     fit$trainingData <- NULL    
##     return(fit)
##   },
##   type_of_preds = "raw"
## )
## attr(kknn_algo, "ML") <- TRUE
## kknn_grid <- expand.grid(kmax = 5, distance = 2, kernel = "gaussian")
## control <- trainControl(method = "cv", number = 2,
##                         predictionBounds = c(0, 1),
##                         trim = TRUE,
##                         allowParallel = TRUE)
```

(ref:estimating-Qbar-one-bis) Write caption.

```{r estimating-Qbar-one-bis, fig.cap = '(ref:estimating-Qbar-one-bis)'}
##
## not updated yet
##
if(redo_fixed) {
  learned_features_fixed_sample_size <-
    learned_features_fixed_sample_size %>% # head(n = 100) %>%
    mutate(Qbar_hat_d = map(obs, ~ estimate_Qbar(., algorithm = working_model_Q_one)),
           Qbar_hat_e = map(obs, ~ estimate_Qbar(., algorithm = kknn_algo,
                                          trControl = kknn_control,
                                          tuneGrid = kknn_grid))) %>%
    mutate(blip_QW_d = map2(Qbar_hat_d, obs,
                            ~ compute_Qbar_hatAW(.y$A, .y$W, .x, blip = TRUE)),
           blip_QW_e = map2(Qbar_hat_e, obs,
                            ~ compute_Qbar_hatAW(.y$A, .y$W, .x, blip = TRUE)))
}

psi_hat_de <- learned_features_fixed_sample_size %>%
  unnest(blip_QW_d, blip_QW_e) %>%
  group_by(id) %>%
  summarize(est_d = mean(blip_QW_d),
            est_e = mean(blip_QW_e)) %>%
  mutate(std_d = sd(est_d),
         std_e = sd(est_e),
         clt_d = (est_d - psi_approx) / std_d,
         clt_e = (est_e - psi_approx) / std_e) %>% 
  gather("key", "value", -id) %>%
  extract(key, c("what", "type"), "([^_]+)_([de])") %>%
  spread(what, value)

(bias_de <- psi_hat_de %>% group_by(type) %>% summarize(bias = mean(clt)))

fig <- ggplot() +
  geom_line(aes(x = x, y = y), 
            data = tibble(x = seq(-3, 3, length.out = 1e3),
                          y = dnorm(x)),
            linetype = 1, alpha = 0.5) +
  geom_density(aes(clt, fill = type, colour = type),
               psi_hat_de, alpha = 0.1) +
  geom_vline(aes(xintercept = bias, colour = type),
             bias_de, size = 1.5, alpha = 0.5)
  
fig +
  labs(y = "",
       x = expression(paste(sqrt(n/v[n]^{list(d, e)})*(psi[n]^{list(d, e)} - psi[0]))))
```

\tcg{No that bad!   Yet, we know that} $\sqrt{n}$ \tcg{times  bias is bound to
increase with sample size. To see this, check out the next chunks of code.}



```{r estimating-Qbar-two, eval = TRUE}
##
## not updated yet
##

sample_size <- c(4e3, 9e3)
block_size <- sum(sample_size)


if (redo_varying) {
  learned_features_varying_sample_size <- obs %>% as.tibble %>% 
    head(n = (nrow(.) %/% block_size) * block_size) %>% 
    mutate(block = label(1:nrow(.), sample_size)) %>%
    nest(-block, .key = "obs")
} 
```

First, we cut the  data set into independent sub-data sets  of sample size $n$
in $\{$ `r sample_size` $\}$.  Second, we infer $\psi_{0}$ as shown two chunks
earlier.  We thus obtain `r nrow(obs) %/% block_size` independent realizations
of each estimator derived on  data sets of `r length(sample_size)`, increasing
sample sizes.

```{r estimating-Qbar-three, eval = TRUE}
##
## not updated yet
## 
if(redo_varying) {
  learned_features_varying_sample_size <-
    learned_features_varying_sample_size %>% 
    mutate(Qbar_hat_d = map(obs, ~ estimate_Qbar(., algorithm = working_model_Q_one)),
           Qbar_hat_e = map(obs, ~ estimate_Qbar(., algorithm = kknn_algo,
                                          trControl = kknn_control,
                                          tuneGrid = kknn_grid))) %>%
    mutate(blip_QW_d = map2(Qbar_hat_d, obs,
                            ~ compute_Qbar_hatAW(.y$A, .y$W, .x, blip = TRUE)),
           blip_QW_e = map2(Qbar_hat_e, obs,
                            ~ compute_Qbar_hatAW(.y$A, .y$W, .x, blip = TRUE)))
}

root_n_bias <- learned_features_varying_sample_size %>%
  unnest(blip_QW_d, blip_QW_e) %>%
  group_by(block) %>%
  summarize(clt_d = sqrt(n()) * (mean(blip_QW_d) - psi_approx),
            clt_e = sqrt(n()) * (mean(blip_QW_e) - psi_approx)) %>%
  gather("key", "value", -block) %>%
  extract(key, c("what", "type"), "([^_]+)_([de])") %>%
  spread(what, value) %>%
  mutate(block = unlist(map(strsplit(block, "_"), ~.x[2])),
         sample_size = sample_size[as.integer(block)])
```
The  `tibble`  called  `root_n_bias`  reports  root-$n$  times  bias  for  all
combinations of  estimator and sample  size. The  next chunk of  code presents
visually our findings, see Figure \@ref(fig:estimating-Qbar-four). Note how we
include the  realizations of the  estimators derived earlier and  contained in
`psi_hat_de`   (thus  breaking   the   independence   between  components   of
`root_n_bias`, a small price to pay in this context).

(ref:estimating-Qbar-four) Evolution of root-$n$ times bias versus sample size
for  two  inference methodology  of  $\psi_{0}$  based  on the  estimation  of
$\Qbar_{0}$.   Big  dots  represent  the average  biases  and  vertical  lines
represent twice the standard error.

```{r estimating-Qbar-four, fig.width = 5, fig.height = 5, fig.cap = '(ref:estimating-Qbar-four)'}
##
## not updated yet
##
root_n_bias <- learned_features_fixed_sample_size %>%
  mutate(sample_size = B/iter) %>%  # because *fixed* sample size
  unnest(blip_QW_d, blip_QW_e) %>%
  group_by(id) %>%
  summarize(clt_d = sqrt(n()) * (mean(blip_QW_d) - psi_approx),
            clt_e = sqrt(n()) * (mean(blip_QW_e) - psi_approx),
            sample_size = sample_size[1]) %>%
  gather("key", "clt", -id, -sample_size) %>%
  extract(key, c("what", "type"), "([^_]+)_([de])") %>%
  mutate(block = "0") %>% select(-id, -what) %>%
  full_join(root_n_bias)

root_n_bias %>%
  ggplot() +
  stat_summary(aes(x = sample_size, y = clt,
                   group = interaction(sample_size, type),
                   color = type),
               fun.data = mean_se, fun.args = list(mult = 2),
               position = position_dodge(width = 250), cex = 1) +
  stat_summary(aes(x = sample_size, y = clt,
                   group = interaction(sample_size, type),
                   color = type),
               fun.data = mean_se, fun.args = list(mult = 2),
               position = position_dodge(width = 250), cex = 1,
               geom = "errorbar", width = 750) +
  stat_summary(aes(x = sample_size, y = clt,
                   color = type),
               fun.y = mean, 
               position = position_dodge(width = 250),
               geom = "polygon", fill = NA) +
  geom_point(aes(x = sample_size, y = clt,
                 group = interaction(sample_size, type),
                 color = type),
             position = position_dodge(width = 250),
             alpha = 0.1) +
  scale_x_continuous(breaks = unique(c(B / iter, sample_size))) +
  labs(x = "sample size n",
       y = expression(paste(sqrt(n) * (psi[n]^{list(d, e)} - psi[0]))))

## execute
## rm(learned_features_fixed_sample_size)
## as soon as possible!
```	

# One-step estimation {#one-step-estimation}

Function  `set_Qbar_Gbar`  implements the  change  of  the `Qbar`  and  `Gbar`
attributes of `obs` (which are accessible only by oracles).

```{r one-step-one}
##
## not updated yet
##
set_Qbar_Gbar <- function(obs, Qhat, Ghat) {
  attr(obs, "Qbar") <- function(newdata) {
    if (!is.data.frame(newdata)) {
      newdata <- as.data.frame(newdata)
    }
    predict(Qhat, newdata = newdata, type = Qhat$type_of_preds)
  }
  attr(obs, "Gbar") <- function(newdata) {
    if (!is.data.frame(newdata)) {
      newdata <- as.data.frame(newdata)
    }
    predict(Ghat, newdata = newdata, type = Ghat$type_of_preds)
  }
  return(obs)
}
eic_hat <- function(obs, Qhat, Ghat, psi_hat) {
  Qbar <- function(newdata) {
    if (!is.data.frame(newdata)) {
      newdata <- as.data.frame(newdata)
    }
    predict(Qhat, newdata = newdata, type = Qhat$type_of_preds)
  }
  Gbar <- function(newdata) {
    if (!is.data.frame(newdata)) {
      newdata <- as.data.frame(newdata)
    }
    predict(Ghat, newdata = newdata, type = Ghat$type_of_preds)
  }
  QAW <- Qbar(obs[, c("A", "W")])
  QoneW <- Qbar(cbind(A = 1, W = obs[, "W"]))
  QzeroW <- Qbar(cbind(A = 0, W = obs[, "W"]))
  GW <- Gbar(obs[, "W", drop = FALSE])
  lGAW <- obs[, "A"] * GW + (1 - obs[, "A"]) * (1 - GW)
  out <- (QoneW - QzeroW - psi_hat) + (2 * obs[, "A"] - 1) / lGAW * (obs[, "Y"] - QAW)
  out <- out[[1]]
  return(out)
}
```

We  first call  function  `eic_hat` to  compute the  values  of the  estimated
efficient influence  at the observations  in `obs`. Constructing  the one-step
estimators is then straightforward.

(ref:one-step-two) Write caption.

```{r one-step-two, fig.cap = '(ref:one-step-two)'}
##
## not updated yet
##
psi_hat_de_one_step <- learned_features_fixed_sample_size %>%
  mutate(est_d = map(blip_QW_d, mean),
         est_e = map(blip_QW_e, mean)) %>%
  mutate(eic_obs_d = pmap(list(obs, Qbar_hat_d, Gbar_hat, est_d),
                          eic_hat),
         eic_obs_e = pmap(list(obs, Qbar_hat_e, Gbar_hat, est_e),
                          eic_hat)) %>%
  unnest(blip_QW_d, eic_obs_d,
         blip_QW_e, eic_obs_e) %>%
  group_by(id) %>%
  summarize(est_d = mean(blip_QW_d) + mean(eic_obs_d),
            std_d = sd(eic_obs_d),
            clt_d = sqrt(n()) * (est_d - psi_approx) / std_d,
            est_e = mean(blip_QW_e) + mean(eic_obs_e),
            std_e = sd(eic_obs_e),
            clt_e = sqrt(n()) * (est_e - psi_approx) / std_e) %>%
  gather("key", "value", -id) %>%
  extract(key, c("what", "type"), "([^_]+)_([de])") %>%
  spread(what, value) %>%
  mutate(type = paste0(type, "_one_step"))
  
(bias_de_one_step <- psi_hat_de_one_step %>%
   group_by(type) %>% summarize(bias = mean(clt)))

ggplot() +
  geom_line(aes(x = x, y = y), 
            data = tibble(x = seq(-3, 3, length.out = 1e3),
                          y = dnorm(x)),
            linetype = 1, alpha = 0.5) +
  geom_density(aes(clt, fill = type, colour = type),
               psi_hat_de_one_step, alpha = 0.1) +
  geom_vline(aes(xintercept = bias, colour = type),
             bias_de_one_step, size = 1.5, alpha = 0.5) +  
  labs(y = "",
       x = expression(
         paste(sqrt(n/v[n]^{list(dos, eos)}) * (psi[n]^{list(dos, eos)} - psi[0]))))
```

It seems  that the one-step correction  is quite good (in  particular, compare
`bias_de` with `bias_de_one_step`):

```{r remove-soon}
##
## not updated yet
##
bind_rows(bias_de, bias_de_one_step)
```

What about the  estimation of the asymptotic variance, and  of the mean-square
errors of the estimators?

```{r enhance}
##
## not updated yet
##
psi_hat_de %>%
  full_join(psi_hat_de_one_step) %>% group_by(type) %>%
  summarize(sd = mean(std * ifelse(str_detect(type, "one_step"), 1, NA),
                      se = sd(est) * sqrt(n()),
                      mse = mean((est - psi_approx)^2) * n()))
```

The `sd`  (\textit{estimator} of the  asymptotic standard deviation)  and `se`
(\textit{empirical} standard deviation) entries for type `d_one_step` are very
similar: this indicates  that the inference of the asymptotic  variance of the
`d`-variant  of the  one-step estimator  based  on the  influence function  is
accurate.   \textit{This  comes as  a  surprise,  since theory  suggests  that
estimation should be conservative, that  is, that the estimator should produce
an  upper bound  to the  actual asymptotic  variance.}  On  the contrary,  the
influence function-based estimator of the  asymptotic variance is clearly very
conservative  for type  `e_one-step`.  As  for the  mean square  error, it  is
diminished by  the one-step  update for  type `d` and  enlarged for  type `e`,
the `d_one_step` estimator exhibiting the smallest mean square error.

# Targeted inference {#targeted-inference}


# (APPENDIX) Appendix {-}



For later$\ldots{}$

<!--```{r estimating-Qbar-appendix, eval = FALSE}
##
## not updated yet
##

working_model_Q_two <- list(
  model = function(...) {trim_glm_fit(glm(family = binomial(), ...))},
  formula = as.formula(
    paste("Y ~ A * (",
          paste("I(W^", seq(1/2, 3, by = 1/2), sep = "", collapse = ") + "),
          "))")
  ),
  type_of_preds = "response"
)
attr(working_model_Q_two, "ML") <- FALSE

## xgboost based on trees
xgb_tree_algo <- list(
  algo = function(dat, ...) {
    caret::train(Y ~ I(10*A) + W,
                 data = dat,
                 method = "xgbTree",
                 trControl = control,
                 tuneGrid = grid,
                 verbose = FALSE)  
  },
  type_of_preds = "response"
)
attr(xgb_tree_algo, "ML") <- TRUE
xgb_tree_grid <- expand.grid(nrounds = 350,
                             max_depth = c(4, 6),
                             eta = c(0.05, 0.1),
                             gamma = 0.01,
                             colsample_bytree = 0.75,
                             subsample = 0.5,
                             min_child_weight = 0)

## nonparametric kernel smoothing regression
npreg <- list(
  label = "Kernel regression",
  type = "Regression",
  library = "np",
  parameters = data.frame(parameter =
                            c("subsample", "regtype",
                              "ckertype", "ckerorder"), 
                          class = c("integer", "character",
                                    "character", "integer"), 
                          label = c("#subsample", "regtype",
                                    "ckertype", "ckerorder")),
  grid = function(x, y, len = NULL, search = "grid") {
    if (!identical(search, "grid")) {
      stop("No random search implemented.\n")
    } else {
      out <- expand.grid(subsample = c(50, 100),
                         regtype = c("lc", "ll"),
                         ckertype =
                           c("gaussian",
                             "epanechnikov",
                             "uniform"),
                         ckerorder = seq(2, 8, 2))
    } 
    return(out)
  },
  fit = function(x, y, wts, param, lev, last, classProbs, ...) {
    ny <- length(y)
    if (ny > param$subsample) {
      ## otherwise far too slow for what we intend to do here...
      keep <- sample.int(ny, param$subsample)
      x <- x[keep, ]
      y <- y[keep]
    }
    bw <- np::npregbw(xdat = as.data.frame(x), ydat = y,
                      regtype = param$regtype,
                      ckertype = param$ckertype,
                      ckerorder = param$ckerorder,
                      remin = FALSE, ftol = 0.01, tol = 0.01,
                      ...)
    np::npreg(bw)
  },
  predict = function (modelFit, newdata, preProc = NULL, submodels = NULL) {
    if (!is.data.frame(newdata)) {
      newdata <- as.data.frame(newdata)
    }
    np:::predict.npregression(modelFit, se.fit = FALSE, newdata)
  },
  sort = function(x) {
    x[order(x$regtype, x$ckerorder), ]
  },
  loop = NULL, prob = NULL, levels = NULL
)

npreg_algo <- list(
  algo = function(dat, ...) {
    caret::train(working_model_Q_one$formula,
                 data = dat,
                 method = npreg, # no quotes!
                 verbose = FALSE,
                 ...)
  },
  type_of_preds = "response"
)
attr(npreg_algo, "ML") <- TRUE
npreg_grid <- data.frame(subsample = 100,
                         regtype = "lc",
                         ckertype = "gaussian",
                         ckerorder = 4,
                         stringsAsFactors = FALSE)
```-->
