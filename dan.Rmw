---
title: "A guided tour in targeted learning territory"
author: "David Benkeser, Antoine Chambaz, Nima Hejazi"
date: "10/11/2017"
encoding: "UTF-8"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    includes:
      in_header: dan_header.tex
  latex_engine: pdflatex
  citation_package: natbib
---

<!-- to compile the file, run

R -e "rmarkdown::render('dan.Rmd', encoding='UTF-8')"

-->


```{r setup, echo = FALSE}
knitr::opts_chunk$set(
  warnings = FALSE,
  fig.width = 12, 
  fig.height = 4, 
  fig.path = 'img/')
```

\section{Introduction}

\tcb{This is a very first draft  of our article. The current *tentative* title
  is "A guided tour in targeted learning territory".}

\tcb{Explain our objectives and how we will meet them.}

\tcb{Use sectioning a lot to ease cross-referencing.}

```{r visible-setup, size = "small"}
set.seed(54321) ## because reproducibility matters...
suppressMessages(library(R.utils)) ## make sure it is installed
expit <- plogis
logit <- qlogis
```


\section{A simulation study}
\label{sec:simulation:study}

blabla

\subsection{Reproducible experiment as a law.}
\label{subsec:as:a:law}

We are interested in a reproducible experiment. The generic summary of how one
realization of the experiment unfolds, our observation, is called $O$. We view
$O$  as a  random variable  drawn from  what we  call the  law $P_{0}$  of the
experiment.  The  law $P_{0}$  is viewed  as an  element of  what we  call the
model. Denoted by $\calM$, the model is the collection of \emph{all} laws from
which  $O$ can  be  drawn  and that  meet  some  constraints. The  constraints
translate the knowledge  we have about the experiment. The  more we know about
the experiment,  the smaller is  $\calM$. In  all our examples,  model $\calM$
will put very few restrictions on the candidate laws.

Consider the following chunk of code:
```{r simulation, size = "small"}
drawFromExperiment <- function(n, full = FALSE) {
  ## preliminary
  n <- Arguments$getInteger(n, c(1, Inf))
  full <- Arguments$getLogical(full)
  ## ## 'gbar' and 'Qbar' factors
  gbar <- function(W) {
    expit(-0.3 + 2 * W - 1.5 * W^2)
  }
  Qbar <- function(AW) {
    A <- AW[, 1]
	W <- AW[, 2]
    A * cos(2 * pi * W) + (1 - A) * sin(2 * pi * W^2)
  }
  ## sampling
  ## ## context
  W <- runif(n)
  ## ## counterfactual rewards
  zeroW <- cbind(A = 0, W)
  oneW <- cbind(A = 1, W)
  Yzero <- rnorm(n, mean = Qbar(zeroW), sd = 1)
  Yone <- rnorm(n, mean = Qbar(oneW), sd = 1)
  ## ## action undertaken
  A <- rbinom(n, size = 1, prob = gbar(W))
  ## ## actual rewards
  Y <- A * Yone + (1 - A) * Yzero
  ## ## observation
  if (full) {
    obs <- cbind(W = W, Yzero = Yzero, Yone = Yone, A = A, Y = Y)
  } else {
    obs <- cbind(W = W, A = A, Y = Y)
  }
  attr(obs, "gbar") <- gbar
  attr(obs, "Qbar") <- Qbar
  attr(obs, "QW") <- dunif
  ##
  return(obs)
}
```

We can  interpret `drawFromExperiment` as a  law $P_{0}$ since we  can use the
function to sample observations  from a common law.  It is  even a little more
than that, because we can tweak the experiment, by setting its `full` argument
to  `TRUE`, in  order  to  get what  appear  as intermediary  (counterfactual)
variables  in  the regular  experiment.   The  next  chunk  of code  runs  the
(regular) experiment five times independently: 
```{r draw-five-obs, size = "small"} 
five.obs <- drawFromExperiment(5) 
five.obs
``` 
The `attributes` of the object `obs` are visible because, in this section,
we  act  as  oracles,  \emph{i.e.},  we know  completely  the  nature  of  the
experiment.  From a probabilistic point of view, the attributes `gbar`, `Qbar`
and  `QW` are  infinite-dimensional features  of  $P_{0}$.  There  is more  to
$P_{0}$ than $\gbar_{0}$ (`gbar`), $\Qbar_{0}$ (`Qbar`), formally defined by
\begin{equation}
  \gbar_{0} (W) \equiv  P_{0} (A=1|W), \quad \Qbar_{0}  (A,W) \equiv E_{P_{0}}
  (Y | A, W),
\end{equation}
and  the marginal  distribution $Q_{0,W}$  of  $W$ under  $P_{0}$ (`QW`),  for
instance the conditional distribution (not  expectation) of $Y$ given $(A,W)$,
but $\gbar_{0}$, $\Qbar_{0}$  and $Q_{0,W}$ will play a prominent  role in our
story.

\subsection{The parameter of interest, first pass.}
\label{subsec:parameter:first}

It  happens  that we  especially  care  for  a finite-dimensional  feature  of
$P_{0}$  that   we  denote  by   $\psi_{0}$.   Its  definition   involves  the
aforementioned infinite-dimensional features: 
\begin{eqnarray*}
  \psi_{0} 
  &\equiv& E_{P_{0}} \left(\Qbar_{0}(1, W) - \Qbar_{0}(0, W)\right)\\
  &=& \int \left(\Qbar_{0}(1, w) - \Qbar_{0}(0, w)\right) dQ_{0,W}(w).
\end{eqnarray*}
Acting as oracles, we can compute explicitely the numerical value of
$\psi_{0}$. 

Our  interest in  $\psi_{0}$ is  of  causal nature.  Taking a  closer look  at
`drawFromExperiment` reveals indeed  that the random making  of an observation
$O$ drawn  from $P_{0}$ can  be summarized by  the following causal  graph and
nonparametric system of structural equations:
```{r DAG}
## plot the causal diagram
```
and, for some  deterministic functions $f_w$, $f_a$,  $f_y$ and independent
sources of randomness $U_w$, $U_a$, $U_y$,
\begin{enumerate}
\item sample  the context where the  rest of the experiment
  will take place, $W = f_{w}(U_w)$;
\item  sample the  two counterfactual  rewards of  the two
  actions  that   can  be   undertaken,  $Y_{0}  =   f_{y}(0,  W,   U_y)$  and
  $Y_{1} = f_{y}(1, W, U_y)$;
\item\label{item:A:equals} sample   which  action is carried
  out in the given context, $A = f_{a} (W, U_a)$;
\item    define        the    corresponding    reward,
  $Y = A Y_{1} + (1-A) Y_{0}$;
\item summarize the course of the experiment  with the observation $O = (W, A,
  Y)$, thus concealing $Y_{0}$ and $Y_{1}$. 
\end{enumerate}

The above description of the  experiment `drawFormExperiment` is useful to ram
home what it  means to run the "full" experiment  by setting argument `full`
to `TRUE` in a call to  `drawFormExperiment`. Doing so triggers a modification
of the  nature of  the experiment, enforcing  that the  counterfactual rewards
$Y_{0}$ and  $Y_{1}$ be part of  the summary of the  experiment eventually. In
light  of the  above enumeration,  $\bbO \equiv  (W, Y_{0},  Y_{1}, A,  Y)$ is
output, as opposed to its summary measure $O$. This defines another experiment
and its law, that we denote $\bbP_{0}$. 

It is well known \tcb{(do we give the proof or refer to other articles?)} that
\begin{equation*}
  \psi_{0} = E_{\bbP_{0}} \left(Y_{1} - Y_{0}\right).
\end{equation*}
Thus, $\psi_{0}$ compares (additively) the  averages of the two counterfactual
rewards. In  other words, $\psi_{0}$  quantifies the difference in  average of
the reward  one would  get in a  world where one  would always  enforce action
$a=1$ with the reward one would get  in a world where one would always enforce
action  $a=0$.  This said,  it  is  worth  emphasizing  that $\psi_{0}$  is  a
well-defined parameter beyond its causal interpretation.

To conclude this subsection, we draw  advantage from the possibility to sample
full observations from `drawFromExperiment` by  setting its argument `full` to
`TRUE` in  order to numerically  approximate $\psi_{0}$.  By the law  of large
numbers, the following chunk of code approximates $\psi_{0}$:
```{r approx-psi-one}
B <- 1e6
full.obs <- drawFromExperiment(B, full = TRUE)
psi.hat <- mean(full.obs[, "Yone"] - full.obs[, "Yzero"])
psi.hat
```
In fact,  the central limit  theorem and Slutsky's lemma  allow us to  build a
confidence interval with asymptotic level 95\% for $\psi_{0}$:
```{r approx-psi-two}
sd.hat <- sd(full.obs[, "Yone"] - full.obs[, "Yzero"])
alpha <- 0.05
psi.CI <- psi.hat + c(-1, 1) * qnorm(1 - alpha / 2) * sd.hat / sqrt(B)
psi.CI
```

\subsection{The parameter of interest, second pass.}
\label{subsec:parameter:second}

