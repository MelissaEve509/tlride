[
["index.html", "A Ride in Targeted Learning Territory Welcome", " A Ride in Targeted Learning Territory David Benkeser (Emory University) Antoine Chambaz (Université Paris Descartes) 2018-10-18 Welcome This is either the website or the text called “A Ride in Targeted Learning Territory”. In the former case, the text can be downloaded by clicking on the dedicated button in the top part of the webpage. In the latter case, the website can be browsed here. Long Mendocino Drive (detail, Liana Steinmetz) Organization The text takes the form of a series of brief sections. The main sections combine theoretical and computational developments. The code is written in the programming language R. R is widely used among statisticians and data scientists to develop statistical software and data analysis. Regularly, a section is inserted that proposes exercizes. Each such section is indicated by the ⚙ symbol. The symbol ☡ also indicates those sections whose content is more involved. Overview After a short introduction, we present the reproducible experiment that will play a central role throughout the text. Then, we introduce the main parameter of interest. We comment upon some of its properties that are useful from a statistical perspective. This paves the way to the presentation of several estimators that are increasingly more powerful statistically. The discussion takes us into targeted learning territory. Audience The text might be of interest to students in statistics and machine learning. It might also serve as a gentle introduction to targeted learning in both its theoretical and computational aspects before delving deeper into the literature (Laan and Rose 2011), (Laan and Rose 2018). Technical details The text is written in RMarkdown with bookdown. Assuming that the knitr package is installed, you can retrieve all the R code by running knitr::purl(&quot;abcd.Rmd&quot;) "],
["1-a-ride.html", "Section 1 A ride 1.1 Introduction 1.2 A simulation study 1.3 ⚙ Visualization 1.4 ⚙ Make your own experiment", " Section 1 A ride 1.1 Introduction Our ambition is to present a gentle introduction to the inference of a causal quantity whose statistical analysis is typical and thus paves the way to more involved analyses. The introduction weaves together two main threads, one theoretical and the other computational. 1.1.1 A causal story We focus on a causal story where a random reward (a real number between 0 and 1) depends on the action undertaken (one among two) and the random context where the action is performed (summarized by a real number between 0 and 1). The causal quantity of interest is the average difference of the two counterfactual rewards. We will build several estimators and discuss their respective merits, theoretically and computationally. The construction of the most involved estimator will unfold in targeted learning territory, at the frontier of machine learning and semiparametrics, the statistical theory of inference based on semiparametric models. 1.1.2 The tlrider package The computational illustrations will be developed based on the companion package tlrider. Make sure you have installed it, for instance by running the following chunk of code: devtools::install_github(&quot;achambaz/tlride/tlrider&quot;) Note that additional packages are required, among which tidyverse (Wickham and Grolemund 2016), caret and ggdag. Assuming that these are installed too, we can run the next chunk of code: set.seed(54321) ## because reproducibility matters... library(tidyverse) library(caret) library(ggdag) library(tlrider) 1.1.3 What we will discuss … 1.2 A simulation study 1.2.1 Reproducible experiment as a law We are interested in a reproducible experiment. Every time this experiment is run, it generates an observation that we call \\(O\\). We view \\(O\\) as a random variable drawn from the law of the experiment that we denote by \\(P_{0}\\). We view \\(P_{0}\\) as an element of the model \\(\\calM\\). The model is a collection of laws. In particular, the model contains all laws that we think may plausibly describe the law of the experiment. Thus, the choice of model is based on our scientific knowledge of the experiment. The more we know about the experiment, the smaller is \\(\\calM\\). In all our examples, we use large models that reflect a lack of knowledge about many aspects of the experiment. 1.2.2 A synthetic reproducible experiment Instead of considering a real-life reproducible experiment, we focus for pedagogical purposes on a synthetic reproducible experiment. Thus we can from now on take on two different roles: that of an oracle knowing completely the nature of the experiment, and that of a statistician eager to know more about the experiment by observing some of its outputs. Let us run the example built into the tlrider package: example(tlrider) A few objects have been defined: ls() #&gt; [1] &quot;another_experiment&quot; &quot;experiment&quot; &quot;expit&quot; #&gt; [4] &quot;logit&quot; &quot;sigma0&quot; Function expit implements the link function \\(\\expit : \\bbR \\to ]0,1[\\) given by \\(\\expit(x) \\equiv (1 + e^{-x})^{-1}\\). Function logit implements its inverse function \\(\\logit : ]0,1[ \\to \\bbR\\) given by \\(\\logit(p) \\equiv \\log [p/(1-p)]\\). Let us take a look at experiment: experiment #&gt; A law for (W,A,Y) in [0,1] x {0,1} x [0,1]. #&gt; #&gt; If the law is fully characterized, you can use method #&gt; &#39;sample_from&#39; to sample from it. #&gt; #&gt; If you built the law, or if you are an _oracle_, you can #&gt; also use methods &#39;reveal&#39; to reveal its relevant features #&gt; (QW, Gbar, Qbar, qY -- see &#39;?reveal&#39;), and &#39;alter&#39; to change #&gt; some of them. #&gt; #&gt; If all its relevant features are characterized, you can #&gt; use methods &#39;evaluate_psi&#39; to obtain the value of &#39;Psi&#39; at #&gt; this law (see &#39;?evaluate_psi&#39;) and &#39;evaluate_eic&#39; to obtain #&gt; the efficient influence curve of &#39;Psi&#39; at this law (see &#39;? #&gt; evaluate_eic&#39;). The law \\(P_{0}\\) of the synthetic experiment experiment built by us generates a generic observation \\(O\\) that decomposes as \\[\\begin{equation*} O \\equiv (W, A, Y) \\in [0,1] \\times \\{0,1\\} \\times [0,1]. \\end{equation*}\\] We interpret \\(W\\) as a real valued summary measure of a random context where an action \\(A\\) chosen among two is undertaken, leading to a real valued reward \\(Y\\). We can sample from the experiment (simply run ?sample_from to see the man page of method sample_from). The next chunk of code runs the experiment five times, independently: (five_obs &lt;- sample_from(experiment, n = 5)) #&gt; W A Y #&gt; [1,] 0.429 1 0.981 #&gt; [2,] 0.454 1 0.855 #&gt; [3,] 0.377 0 0.836 #&gt; [4,] 0.461 1 0.582 #&gt; [5,] 0.419 1 0.878 1.2.3 Revealing experiment Acting as oracles, we can peek into experiment and reveal a selection of relevant features (simply run ?reveal to see the man page of method reveal). Made by us, the selection exhibits features that will play an important role in the text. relevant_features &lt;- reveal(experiment) names(relevant_features) #&gt; [1] &quot;QW&quot; &quot;Gbar&quot; &quot;Qbar&quot; &quot;qY&quot; &quot;sample_from&quot; We have an oracular knowledge of experiment and can thus comment upon the features of \\(P_{0}\\) revealed in relevant_features. QW The QW feature describes the marginal law of \\(W\\), that we call \\(Q_{0,W}\\).1 relevant_features$QW #&gt; function(W, #&gt; mixture_weights = c(1/10, 9/10, 0), #&gt; mins = c(0, 11/30, 0), #&gt; maxs = c(1, 14/30, 1)) { #&gt; out &lt;- sapply(1:length(mixture_weights), #&gt; function(ii){ #&gt; mixture_weights[ii] * #&gt; stats::dunif(W, #&gt; min = mins[ii], #&gt; max = maxs[ii]) #&gt; }) #&gt; return(rowSums(out)) #&gt; } #&gt; &lt;environment: 0xe822610&gt; It appears that \\(Q_{0,W}\\) is a mixture of the uniform laws over \\([0,1]\\) (weight \\(1/10\\)) and \\([11/30,14/30]\\) (weight \\(9/10\\)).2 Gbar The Gbar feature describes the conditional probability of action \\(A = 1\\) given \\(W\\). For each \\(a \\in \\{0,1\\}\\), we denote \\[\\begin{align*} \\Gbar_0(W) &amp;\\equiv \\Pr_{P_0}(A = 1 | W), \\\\\\ell\\Gbar_0(a,W) &amp;\\equiv \\Pr_{P_0}(A = a | W).\\end{align*}\\] Obviously, \\[\\begin{equation*}\\ell\\Gbar_{0}(A,W) \\equiv A\\Gbar_{0}(W) + (1-A) (1-\\Gbar_{0}(W)).\\end{equation*}\\] relevant_features$Gbar #&gt; function(W) { #&gt; expit(1 + 2 * W - 4 * sqrt(abs((W - 5/12)))) #&gt; } #&gt; &lt;environment: 0xe822610&gt; Note how real numbers of the form \\(1 + 2W - 4 * \\sqrt{|W - 5/12|})\\) are mapped into the interval \\([0,1]\\) by the \\(\\expit\\) link function. qY The qY feature describes the conditional density of \\(Y\\) given \\(A\\) and \\(W\\). For each \\(y\\in ]0,1[\\), we denote by \\(q_{0,Y}(y, A, W)\\) the conditional density evaluated at \\(y\\) of \\(Y\\) given \\(A\\) and \\(W\\). relevant_features$qY #&gt; function(obs, Qbar, shape10 = 2, shape11 = 3){ #&gt; A &lt;- obs[, &quot;A&quot;] #&gt; AW &lt;- obs[, c(&quot;A&quot;, &quot;W&quot;)] #&gt; QAW &lt;- Qbar(AW) #&gt; shape1 &lt;- ifelse(A == 0, shape10, shape11) #&gt; stats::dbeta(Y, #&gt; shape1 = shape1, #&gt; shape2 = shape1 * (1 - QAW) / QAW) #&gt; } #&gt; &lt;environment: 0xe822610&gt; It appears that the conditional law of \\(Y\\) given \\(A\\) and \\(W\\) is the Beta law with conditional mean and variance characterized by the Qbar feature of experiment (see below) and the shape10 and shape11 parameters. Qbar As for the Qbar feature, it describes the conditional mean of \\(Y\\) given \\(A\\) and \\(W\\). relevant_features$Qbar #&gt; function(AW) { #&gt; A &lt;- AW[, &quot;A&quot;] #&gt; W &lt;- AW[, &quot;W&quot;] #&gt; A * (cos((-1/2 + W) * pi) * 2/5 + 1/5 + #&gt; (1/3 &lt;= W &amp; W &lt;= 1/2) / 5 + #&gt; (W &gt;= 3/4) * (W - 3/4) * 2) + #&gt; (1 - A) * (sin(4 * W^2 * pi) / 4 + 1/2) #&gt; } #&gt; &lt;bytecode: 0xf3afad8&gt; #&gt; &lt;environment: 0xe822610&gt; We denote \\(\\Qbar_0(A,W) = \\Exp_{P_{0}}(Y|A,W)\\) the conditional mean of \\(Y\\) given \\(A\\) and \\(W\\). Note how \\(\\Qbar_0(A,W)\\) does depend heavily on \\(A\\) and \\(W\\). We refer the reader to Section 1.3 for a visualization of \\(\\Qbar_{0}\\). sample_from Finally, the sample_from feature is the function called by method sample_from when it is applied to an object of class LAW, like experiment. relevant_features$sample_from #&gt; function(n, ideal = FALSE) { #&gt; ## preliminary #&gt; n &lt;- R.utils::Arguments$getInteger(n, c(1, Inf)) #&gt; ideal &lt;- R.utils::Arguments$getLogical(ideal) #&gt; ## ## &#39;Gbar&#39; and &#39;Qbar&#39; factors #&gt; Gbar &lt;- experiment$.Gbar #&gt; Qbar &lt;- experiment$.Qbar #&gt; ## sampling #&gt; ## ## context #&gt; params &lt;- formals(experiment$.QW) #&gt; mixture_weights &lt;- eval(params$mixture_weights) #&gt; mins &lt;- eval(params$mins) #&gt; maxs &lt;- eval(params$maxs) #&gt; W &lt;- sample_from_mixture_of_uniforms(n, mixture_weights, #&gt; mins, maxs) #&gt; ## ## counterfactual rewards #&gt; zeroW &lt;- cbind(A = 0, W) #&gt; oneW &lt;- cbind(A = 1, W) #&gt; Qbar_zeroW &lt;- Qbar(zeroW) #&gt; Qbar_oneW &lt;- Qbar(oneW) #&gt; Yzero &lt;- stats::rbeta(n, #&gt; shape1 = 2, #&gt; shape2 = 2 * (1 - Qbar_zeroW) / Qbar_zeroW) #&gt; Yone &lt;- stats::rbeta(n, #&gt; shape1 = 3, #&gt; shape2 = 3 * (1 - Qbar_oneW) / Qbar_oneW) #&gt; ## ## action undertaken #&gt; A &lt;- stats::rbinom(n, size = 1, prob = Gbar(W)) #&gt; ## ## actual reward #&gt; Y &lt;- A * Yone + (1 - A) * Yzero #&gt; ## ## observation #&gt; if (ideal) { #&gt; obs &lt;- cbind(W = W, Yzero = Yzero, Yone = Yone, A = A, Y = Y) #&gt; } else { #&gt; obs &lt;- cbind(W = W, A = A, Y = Y) #&gt; } #&gt; return(obs) #&gt; } #&gt; &lt;bytecode: 0xd2a6a80&gt; #&gt; &lt;environment: 0xe822610&gt; We will comment upon the ideal argument in the above sample_from feature in Section 2.1. 1.3 ⚙ Visualization Run the following chunk of code. It visualizes the conditional mean \\(\\Qbar_{0}\\). Gbar &lt;- relevant_features$Gbar Qbar &lt;- relevant_features$Qbar QW &lt;- relevant_features$QW features &lt;- tibble(w = seq(0, 1, length.out = 1e3)) %&gt;% mutate(Qw = QW(w), Gw = Gbar(w), Q1w = Qbar(cbind(A = 1, W = w)), Q0w = Qbar(cbind(A = 0, W = w)), blip_Qw = Q1w - Q0w) features %&gt;% select(-Qw, -Gw) %&gt;% rename(&quot;Q(1,.)&quot; = Q1w, &quot;Q(0,.)&quot; = Q0w, &quot;Q(1,.) - Q(0,.)&quot; = blip_Qw) %&gt;% gather(&quot;f&quot;, &quot;value&quot;, -w) %&gt;% ggplot() + geom_line(aes(x = w, y = value, color = f), size = 1) + labs(y = &quot;f(w)&quot;, title = bquote(&quot;Visualizing&quot; ~ bar(Q)[0])) + ylim(NA, 1) Adapt the above chunk of code to visualize the marginal density \\(Q_{0,W}\\) and conditional probability \\(\\Gbar_{0}\\). 1.4 ⚙ Make your own experiment You can easily make your own experiment. Check out the man page of method alter by running ?alter. Run the following chunk of code: my_experiment &lt;- LAW() ## creates an object of class &#39;LAW&#39; alter(my_experiment, ## characterize its relevant features QW = function(W) { out &lt;- rep_len(0, length(W)) out[W == 0] &lt;- 1/4 out[W == 1] &lt;- 3/4 return(out) }, Gbar = function(W) { out &lt;- rep_len(0, length(W)) out[W == 0] &lt;- 1/3 out[W == 1] &lt;- 3/5 return(out) }, Qbar = function(AW) { probs &lt;- matrix(c(1/2, 2/3, 7/8, 4/5), ncol = 2, dimnames = list(c(&quot;A=0&quot;, &quot;A=1&quot;), c(&quot;W=0&quot;, &quot;W=1&quot;))) probs[cbind(AW[, &quot;A&quot;] + 1, AW[, &quot;W&quot;] + 1)] }, qY = function(obs) { probs &lt;- matrix(c(1/2, 2/3, 7/8, 4/5), ncol = 2, dimnames = list(c(&quot;A=0&quot;, &quot;A=1&quot;), c(&quot;W=0&quot;, &quot;W=1&quot;))) probs &lt;- probs[cbind(obs[, &quot;A&quot;] + 1, obs[, &quot;W&quot;] + 1)] obs[, &quot;Y&quot;] * probs + (1 - obs[, &quot;Y&quot;]) * (1 - probs) }, sample_from = function(n) { ## preliminary n &lt;- R.utils::Arguments$getInteger(n, c(1, Inf)) ## &#39;QW&#39;, &#39;Gbar&#39; and &#39;Qbar&#39; features QW &lt;- my_experiment$.QW Gbar &lt;- my_experiment$.Gbar Qbar &lt;- my_experiment$.Qbar ## sampling W &lt;- rbinom(n, size = 1, prob = QW(1)) A &lt;- rbinom(n, size = 1, prob = Gbar(W)) AW &lt;- cbind(W = W, A = A) Y &lt;- rbinom(n, size = 1, Qbar(AW)) return(cbind(AW, Y = Y)) }) What does the next chunk do? (sample_from(my_experiment, 3)) #&gt; W A Y #&gt; [1,] 0 0 1 #&gt; [2,] 1 1 1 #&gt; [3,] 1 0 1 Characterize entirely the law of my_experiment. Hint: obs &lt;- sample_from(my_experiment, 1e4) obs %&gt;% as.tibble %&gt;% group_by(W, A, Y) %&gt;% summarize(how_many = n()) #&gt; # A tibble: 8 x 4 #&gt; # Groups: W, A [?] #&gt; W A Y how_many #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 0 0 0 826 #&gt; 2 0 0 1 808 #&gt; 3 0 1 0 285 #&gt; 4 0 1 1 556 #&gt; 5 1 0 0 419 #&gt; 6 1 0 1 2665 #&gt; # ... with 2 more rows obs %&gt;% as.tibble %&gt;% group_by(W, A) %&gt;% summarize(prob = mean(Y)) #&gt; # A tibble: 4 x 3 #&gt; # Groups: W [?] #&gt; W A prob #&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 0 0 0.494 #&gt; 2 0 1 0.661 #&gt; 3 1 0 0.864 #&gt; 4 1 1 0.798 Now, make your own experiment. A summary of the notation used throughout the text is presented here.↩ We fine-tuned the marginal law \\(Q_{0,W}\\) of \\(W\\) to make it easier later on to drive home important messages.↩ "],
["2-parameter.html", "Section 2 The parameter of interest 2.1 The parameter of interest 2.2 ⚙ An alternative parameter of interest 2.3 The statistical mapping of interest 2.4 ⚙ Alternative statistical mapping 2.5 Representations 2.6 ⚙ Alternative representation", " Section 2 The parameter of interest 2.1 The parameter of interest 2.1.1 Definition It happens that we especially care for a finite-dimensional feature of \\(P_{0}\\) that we denote by \\(\\psi_{0}\\). Its definition involves two of the aforementioned infinite-dimensional features, the marginal law \\(Q_{0,W}\\) of \\(W\\) and the conditional mean \\(\\Qbar_{0}\\) of \\(Y\\) given \\(A\\) and \\(W\\): \\[\\begin{align} \\psi_{0} &amp;\\equiv \\int \\left(\\Qbar_{0}(1, w) - \\Qbar_{0}(0, w)\\right) dQ_{0,W}(w) \\tag{2.1}\\\\ \\notag &amp;= \\Exp_{P_{0}} \\left(\\Exp_{P_0}(Y \\mid A = 1, W) - \\Exp_{P_0}(Y \\mid A = 0, W) \\right). \\end{align}\\] Acting as oracles, we can compute explicitly the numerical value of \\(\\psi_{0}\\). The evaluate_psi method makes it very easy (simply run ?estimate_psi to see the man page of the method): (psi_zero &lt;- evaluate_psi(experiment)) #&gt; [1] 0.0832 2.1.2 A causal interpretation Our interest in \\(\\psi_{0}\\) is of causal nature. Taking a closer look at the sample_from feature of experiment reveals indeed that the random making of an observation \\(O\\) drawn from \\(P_{0}\\) can be summarized by the following causal graph: dagify( Y ~ A + Y1 + Y0, A ~ W, Y1 ~ W, Y0 ~ W, labels = c(Y = &quot;Actual reward&quot;, A = &quot;Action&quot;, Y1 = &quot;Counterfactual reward\\n of action 1&quot;, Y0 = &quot;Counterfactual reward\\n of action 0&quot;, W = &quot;Context of action&quot;), coords = list( x = c(W = 0, A = -1, Y1 = 1.5, Y0 = 0.25, Y = 1), y = c(W = 0, A = -1, Y1 = -0.5, Y0 = -0.5, Y = -1)), outcome = &quot;Y&quot;, exposure = &quot;A&quot;, latent = c(&quot;Y0&quot;, &quot;Y1&quot;)) %&gt;% tidy_dagitty %&gt;% ggdag(text = TRUE, use_labels = &quot;label&quot;) + theme_dag_grey() Figure 2.1: Causal graph summarizing the inner causal mechanism at play in experiment. In words, the experiment unfolds like this (see also Section 11.1): a context of action \\(W \\in [0,1]\\) is randomly generated; two counterfactual rewards \\(Y_{0}\\in [0,1]\\) and \\(Y_{1}\\in [0,1]\\) are generated conditionally on \\(W\\); an action \\(A \\in \\{0,1\\}\\) (among two possible actions called \\(a=0\\) and \\(a=1\\)) is undertaken, (i) knowing the context but not the counterfactual rewards, and (ii) in such a way that both actions can always be considered; the action yields a reward \\(Y\\), which equals either \\(Y_{0}\\) or \\(Y_{1}\\) depending on whether action \\(a=0\\) or \\(a=1\\) has been undertaken; summarize the course of the experiment with \\(O \\equiv (W, A, Y)\\), thus concealing \\(Y_{0}\\) and \\(Y_{1}\\). The above description of the experiment is useful to reinforce what it means to run the “ideal” experiment by setting argument ideal to TRUE in a call to sample_from for experiment (see Section 2.1.3). Doing so triggers a modification of the nature of the experiment, enforcing that the counterfactual rewards \\(Y_{0}\\) and \\(Y_{1}\\) be part of the summary of the experiment eventually. In light of the above enumeration, \\[\\begin{equation*} \\bbO \\equiv (W, Y_{0}, Y_{1}, A, Y) \\end{equation*}\\] is output, as opposed to its summary measure \\(O\\). This defines another experiment and its law, that we denote \\(\\bbP_{0}\\). It is straightforward to show that \\[\\begin{align} \\psi_{0} &amp;= \\Exp_{\\bbP_{0}} \\left(Y_{1} - Y_{0}\\right) \\tag{2.2} \\\\ &amp;= \\Exp_{\\bbP_{0}}(Y_1) - \\Exp_{\\bbP_{0}}(Y_0). \\notag \\end{align}\\] Thus, \\(\\psi_{0}\\) describes the average difference of the two counterfactual rewards. In other words, \\(\\psi_{0}\\) quantifies the difference in average of the reward one would get in a world where one would always enforce action \\(a=1\\) with the reward one would get in a world where one would always enforce action \\(a=0\\). This said, it is worth emphasizing that \\(\\psi_{0}\\) is a well-defined parameter beyond its causal interpretation, and that it describes a standardized association between the action \\(A\\) and reward \\(Y\\). 2.1.3 A causal computation We can use our position as oracles to sample observations from the ideal experiment. We call sample_from for experiment with its argument ideal set to TRUE in order to numerically approximate \\(\\psi_{0}\\). By the law of large numbers, the following code approximates \\(\\psi_{0}\\) and shows it approximate value. B &lt;- 1e5 ideal_obs &lt;- sample_from(experiment, B, ideal = TRUE) (psi_approx &lt;- mean(ideal_obs[, &quot;Yone&quot;] - ideal_obs[, &quot;Yzero&quot;])) #&gt; [1] 0.0841 The object psi_approx contains an approximation to \\(\\psi_0\\) based on B observations from the ideal experiment. The random sampling of observations results in uncertainty in the numerical approximation of \\(\\psi_0\\). This uncertainty can be quantified by constructing a 95% confidence interval for \\(\\psi_0\\). The central limit theorem and Slutsky’s lemma allow us to build such an interval as follows. sd_approx &lt;- sd(ideal_obs[, &quot;Yone&quot;] - ideal_obs[, &quot;Yzero&quot;]) alpha &lt;- 0.05 (psi_approx_CI &lt;- psi_approx + c(-1, 1) * qnorm(1 - alpha / 2) * sd_approx / sqrt(B)) #&gt; [1] 0.0822 0.0860 We note that the interpretation of this confidence interval is that in 95% of draws of size B from the ideal data generating experiment, the true value of \\(\\psi_0\\) will be contained in the generated confidence interval. 2.2 ⚙ An alternative parameter of interest Equality (2.2) shows that parameter \\(\\psi_0\\) (2.1) is the difference in average rewards if we enforce action \\(a = 1\\) rather than \\(a = 0\\). An alternative way to describe the rewards under different actions involves quantiles as opposed to averages. Let \\[\\begin{equation*} Q_{0,Y}(y, A, W) \\equiv \\int_{0}^y q_{0,Y}(u, A, W) du \\end{equation*}\\] be the conditional cumulative distribution of reward \\(Y\\) given \\(A\\) and \\(W\\), evaluated at \\(y \\in ]0,1[\\), that is implied by \\(P_0\\). For each action \\(a \\in \\{0,1\\}\\) and \\(c \\in ]0,1[\\), introduce \\[\\begin{equation} \\gamma_{0,a,c} \\equiv \\inf \\left\\{y \\in ]0,1[ : \\int Q_{0,Y}(y, a, w) dQ_{0,W}(w) \\ge c \\right\\}. \\tag{2.3} \\end{equation}\\] It is not very difficult to check (see Problem 1 below) that \\[\\begin{equation}\\gamma_{0,a,c} = \\inf\\left\\{y \\in ]0,1[ : \\Pr_{\\bbP_{0}}(Y_a \\leq y) \\geq c\\right\\}. \\tag{2.4}\\end{equation}\\] Thus, \\(\\gamma_{0,a,c}\\) can be interpreted as a covariate-adjusted \\(c\\)-th quantile reward when action \\(a\\) is enforced. The difference \\[\\begin{equation}\\delta_{0,c} \\equiv \\gamma_{0,1,c} - \\gamma_{0,0,c} \\tag{2.5}\\end{equation}\\] is the \\(c\\)-th quantile counterpart to parameter \\(\\psi_{0}\\) (2.1). ☡ Prove (2.4). ☡ Compute the numerical value of \\(\\gamma_{0,a,c}\\) for each \\((a,c) \\in \\{0,1\\} \\times \\{1/4, 1/2, 3/4\\}\\) using the appropriate features of experiment (see relevant_features). Based on these results, report the numerical value of \\(\\delta_{0,c}\\) for each \\(c \\in \\{1/4, 1/2, 3/4\\}\\). Approximate the numerical values of \\(\\gamma_{0,a,c}\\) for each \\((a,c) \\in \\{0,1\\} \\times \\{1/4, 1/2, 3/4\\}\\) by drawing a large sample from the “ideal” data experiment and using empirical quantile estimates. Deduce from these results a numerical approximation to \\(\\delta_{0,c}\\) for \\(c \\in \\{1/4, 1/2, 3/4\\}\\). Confirm that your results closely match those obtained in the previous problem. 2.3 The statistical mapping of interest The noble way to define a statistical parameter is to view it as the value of a statistical mapping at the law of the experiment of interest. Beyond the elegance, this has paramount statistical implications. 2.3.1 Opening discussion Oftentimes, the premise of a statistical analysis is presented like this. One assumes that the law \\(P_{0}\\) of the experiment of interest belongs to a statistical model \\[\\begin{equation*}\\{P_{\\theta} : \\theta \\in T\\}.\\end{equation*}\\] The statistical model is identifiable, meaning that if two elements \\(P_{\\theta}\\) and \\(P_{\\theta&#39;}\\) coincide, then necessarily \\(\\theta = \\theta&#39;\\). Therefore, there exists a unique \\(\\theta_{0} \\in T\\) such that \\(P_{0} = P_{\\theta_{0}}\\), and one wishes to estimate \\(\\theta_{0}\\). For instance, each \\(P_{\\theta}\\) could be the Gaussian law with mean \\(\\theta \\in T \\equiv \\bbR\\) and variance 1, and one could wish to estimate the mean \\(\\theta_{0}\\) of \\(P_{0}\\). To do so, one could rely on \\(n\\) observations \\(X_{1}\\), , \\(X_{n}\\) drawn independently from \\(P_{0}\\). The empirical mean \\[\\begin{equation*}\\theta_{n} \\equiv \\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\end{equation*}\\] estimates \\(\\theta_{0}\\). If we assume that \\(\\Var_{P_{0}} (X_{1})\\) is finite, then \\(\\theta_{n}\\) satisfies many useful properties. In particular, it can be used to construct confidence intervals. Of course, the mean of a law is defined beyond the small model \\(\\{P_{\\theta} : \\theta \\in \\bbR\\}\\). Let \\(\\calM\\) be the set of laws \\(P\\) on \\(\\bbR\\) such that \\(\\Var_{P}(X)\\) is finite. In particular, \\(P_{0} \\in \\calM\\). For every \\(P \\in \\calM\\), the mean \\(E_{P}(X)\\) is well defined. Thus, we can introduce the statistical mapping \\(\\Theta : \\calM \\to \\bbR\\) given by \\[\\begin{equation*}\\Theta(P) \\equiv E_{P}(X).\\end{equation*}\\] Interestingly, the empirical measure \\(P_{n}\\)3 is an element of \\(\\calM\\). Therefore, the statistical mapping \\(\\Theta\\) can be evaluated at \\(P_{n}\\): \\[\\begin{equation*}\\Theta(P_{n}) = \\frac{1}{n} \\sum_{i=1}^{n} X_{i} = \\theta_{n}.\\end{equation*}\\] We recover the empirical mean, and understand that it is a substitution estimator of the mean: in order to estimate \\(\\Theta(P_{0})\\), we substitute \\(P_{n}\\) for \\(P_{0}\\) within \\(\\Theta\\). Substitution-based estimators are particularly valuable notably because they, by construction, satisfy all the constraints to which the targeted parameter is subjected. Some of the estimators that we will build together are substitution-based, some are not. 2.3.2 The parameter as the value of a statistical mapping at the experiment We now go back to our main topic of interest. Suppose we know beforehand that \\(O\\) drawn from \\(P_{0}\\) takes its values in \\(\\calO \\equiv [0,1] \\times \\{0,1\\} \\times [0,1]\\) and that \\(\\Gbar_{0}(W) \\equiv _Pr_{P_{0}}(A=1|W)\\) is bounded away from zero and one \\(Q_{0,W}\\)-almost surely (this is the case indeed). Then we can define model \\(\\calM\\) as the set of all laws \\(P\\) on \\(\\calO\\) such that \\[\\begin{equation*}\\Gbar(W) \\equiv \\Pr_{P}(A=1|W)\\end{equation*}\\] is bounded away from zero and one \\(Q_{W}\\)-almost surely, where \\(Q_{W}\\) is the marginal law of \\(W\\) under \\(P\\). Let us also define generically \\(\\Qbar\\) as \\[\\begin{equation*}\\Qbar (A,W) \\equiv \\Exp_{P} (Y|A, W).\\end{equation*}\\] Note how we have suppressed the dependence of \\(\\Gbar\\) and \\(\\Qbar\\) on \\(P\\) for notational simplicity. Central to our approach is viewing \\(\\psi_{0}\\) as the value at \\(P_{0}\\) of the statistical mapping \\(\\Psi\\) from \\(\\calM\\) to \\([0,1]\\) characterized by \\[\\begin{align} \\Psi(P) &amp;\\equiv \\int \\left(\\Qbar(1, w) - \\Qbar(0, w)\\right) dQ_{W}(w) \\tag{2.6}\\\\ &amp;= \\Exp_{P} \\left(\\Qbar(1, W) - \\Qbar(0, W)\\right), \\notag \\end{align}\\] a clear extension of (2.1). 2.3.3 The value of the statistical mapping at another experiment When we ran example(tlrider) earlier, we created an object called another_experiment: another_experiment #&gt; A law for (W,A,Y) in [0,1] x {0,1} x [0,1]. #&gt; #&gt; If the law is fully characterized, you can use method #&gt; &#39;sample_from&#39; to sample from it. #&gt; #&gt; If you built the law, or if you are an _oracle_, you can #&gt; also use methods &#39;reveal&#39; to reveal its relevant features #&gt; (QW, Gbar, Qbar, qY -- see &#39;?reveal&#39;), and &#39;alter&#39; to change #&gt; some of them. #&gt; #&gt; If all its relevant features are characterized, you can #&gt; use methods &#39;evaluate_psi&#39; to obtain the value of &#39;Psi&#39; at #&gt; this law (see &#39;?evaluate_psi&#39;) and &#39;evaluate_eic&#39; to obtain #&gt; the efficient influence curve of &#39;Psi&#39; at this law (see &#39;? #&gt; evaluate_eic&#39;). reveal(another_experiment) #&gt; $QW #&gt; function (x, min = 1/10, max = 9/10) #&gt; { #&gt; stats::dunif(x, min = min, max = max) #&gt; } #&gt; &lt;environment: 0xf61e700&gt; #&gt; #&gt; $Gbar #&gt; function (W) #&gt; { #&gt; sin((1 + W) * pi/6) #&gt; } #&gt; &lt;environment: 0xf61e700&gt; #&gt; #&gt; $Qbar #&gt; function (AW, h) #&gt; { #&gt; A &lt;- AW[, &quot;A&quot;] #&gt; W &lt;- AW[, &quot;W&quot;] #&gt; expit(logit(A * W + (1 - A) * W^2) + h * 10 * sqrt(W) * A) #&gt; } #&gt; &lt;environment: 0xf61e700&gt; #&gt; #&gt; $qY #&gt; function (obs, Qbar, shape1 = 4) #&gt; { #&gt; AW &lt;- obs[, c(&quot;A&quot;, &quot;W&quot;)] #&gt; QAW &lt;- Qbar(AW) #&gt; stats::gdbeta(Y, shape1 = shape1, shape2 = shape1 * (1 - #&gt; QAW)/QAW) #&gt; } #&gt; &lt;environment: 0xf61e700&gt; #&gt; #&gt; $sample_from #&gt; function (n, h) #&gt; { #&gt; n &lt;- R.utils::Arguments$getInteger(n, c(1, Inf)) #&gt; h &lt;- R.utils::Arguments$getNumeric(h) #&gt; Gbar &lt;- another_experiment$.Gbar #&gt; Qbar &lt;- another_experiment$.Qbar #&gt; params &lt;- formals(another_experiment$.QW) #&gt; W &lt;- stats::runif(n, min = eval(params$min), max = eval(params$max)) #&gt; A &lt;- stats::rbinom(n, size = 1, prob = Gbar(W)) #&gt; params &lt;- formals(another_experiment$.qY) #&gt; shape1 &lt;- eval(params$shape1) #&gt; QAW &lt;- Qbar(cbind(A = A, W = W), h = h) #&gt; Y &lt;- stats::rbeta(n, shape1 = shape1, shape2 = shape1 * (1 - #&gt; QAW)/QAW) #&gt; obs &lt;- cbind(W = W, A = A, Y = Y) #&gt; return(obs) #&gt; } #&gt; &lt;environment: 0xf61e700&gt; (two_obs_another_experiment &lt;- sample_from(another_experiment, 2, h = 0)) #&gt; W A Y #&gt; [1,] 0.585 1 0.507 #&gt; [2,] 0.347 1 0.345 By taking an oracular look at the output of reveal(another_experiment), we discover that the law \\(\\Pi_{0} \\in \\calM\\) encoded by default (i.e., with h=0) in another_experiment differs starkly from \\(P_{0}\\). However, the parameter \\(\\Psi(\\Pi_{0})\\) is well defined. Straightforward algebra shows that \\(\\Psi(\\Pi_{0}) = 59/300\\). The numeric computation below confirms the equality. (psi_Pi_zero &lt;- evaluate_psi(another_experiment, h = 0)) #&gt; [1] 0.197 2.4 ⚙ Alternative statistical mapping We now resume the exercize of Section 2.2. Like we did in Section 2.3, we introduce a generic version of the relevant features \\(q_{0,Y}\\) and \\(Q_{0,Y}\\). Specifically, we define \\(q_{Y}(y,A,W)\\) to be the conditional density of \\(Y\\) given \\(A\\) and \\(W\\), evaluated at \\(y\\), that is implied by a generic \\(P \\in \\calM\\). Similarly, we use \\(Q_{Y}\\) to denote the corresponding cumulative distribution function. The covariate-adjusted \\(c\\)-th quantile reward for action \\(a \\in \\{0,1\\}\\), \\(\\gamma_{0,a,c}\\) (2.3), may be viewed as the value at \\(P_{0}\\) of a mapping \\(\\Gamma_{a,c}\\) from \\(\\calM\\) to \\([0,1]\\) characterized by \\[\\begin{equation*} \\Gamma_{a,c}(P) = \\inf\\left\\{y \\in ]0,1[ : \\int Q_{Y}(y,a,w) dQ_W(w) \\ge c \\right\\}. \\end{equation*}\\] The difference in \\(c\\)-th quantile rewards, \\(\\delta_{0,c}\\) (2.5), may similarly be viewed as the value at \\(P_{0}\\) of a mapping \\(\\Delta_c\\) from \\(\\calM\\) to \\([0,1]\\), characterized by \\[\\begin{equation*} \\Delta_c(P) \\equiv \\Gamma_{1,c}(P) - \\Gamma_{0,c}(P). \\end{equation*}\\] Compute the numerical value of \\(\\Gamma_{a,c}(\\Pi_0)\\) for \\((a,c) \\in \\{0,1\\} \\times \\{1/4, 1/2, 3/4\\}\\) using the relevant features of another_experiment. Based on these results, report the numerical value of \\(\\Delta_c(\\Pi_0)\\) for each \\(c \\in \\{1/4, 1/2, 3/4\\}\\). Approximate the value of \\(\\Gamma_{0,a,c}(\\Pi_{0})\\) for \\((a,c) \\in \\{0,1\\} \\times \\{1/4, 1/2, 3/4\\}\\) by drawing a large sample from the “ideal” data experiment and using empirical quantile estimates. Deduce from these results a numerical approximation to \\(\\Delta_{0,c} (\\Pi_{0})\\) for each \\(c \\in \\{1/4, 1/2, 3/4\\}\\). Confirm that your results closely match those obtained in the previous problem. Building upon the code you wrote to solve the previous problem, construct a confidence interval with asymptotic level \\(95\\%\\) for \\(\\Delta_{0,c} (\\Pi_{0})\\), with \\(c \\in \\{1/4, 1/2, 3/4\\}\\). 2.5 Representations In Section 2.3, we reoriented our view of the target parameter to be that of a statistical functional of the law of the observed data. Specifically, we viewed the parameter as a function of specific features of the observed data law, namely \\(Q_{W}\\) and \\(\\Qbar\\). 2.5.1 Yet another representation It is straightforward to show an equivalent representation of the parameter as \\[\\begin{align} \\notag \\psi_{0} &amp;= \\int \\frac{2a - 1}{\\ell\\Gbar_0(a,w)} y dP_0(w,a,y) \\\\ \\tag{2.7} &amp;= \\Exp_{P_0} \\left( \\frac{2A - 1}{\\ell\\Gbar_{0}(A,W)} Y \\right). \\end{align}\\] Viewing again the parameter as a statistical mapping from \\(\\calM\\) to \\([0,1]\\), it also holds that \\[\\begin{align} \\notag \\Psi(P) &amp;= \\int \\frac{2a-1}{\\ell\\Gbar(a,w)} y dP(w,a,y) \\\\ \\tag{2.8} &amp;= \\Exp_{P}\\left(\\frac{2A - 1}{\\ell\\Gbar_{0}(A,W)} Y \\right). \\end{align}\\] 2.5.2 From representations to estimation strategies Our reason for introducing this alternative view of the target parameter will become clear when we discuss estimation of the target parameter. Specifically, the representations (2.1) and (2.7) naturally suggest different estimation strategies for \\(\\psi_0\\), as hinted in Section 2.3.1. The former suggests building an estimator of \\(\\psi_0\\) using estimators of \\(\\Qbar_0\\) and of \\(Q_{W,0}\\). The latter suggests building an estimator of \\(\\psi_0\\) using estimators of \\(\\ell\\Gbar_0\\) and of \\(P_0\\). We return to these ideas in later sections. 2.6 ⚙ Alternative representation ☡ Show that for \\(a&#39; = 0,1\\), \\(\\gamma_{0,a&#39;,c}\\) as defined in (2.3) can be equivalently expressed as \\[\\begin{equation*}\\inf \\left\\{z \\in ]0,1[ : \\int \\frac{\\one\\{a = a&#39;\\}}{\\ell\\Gbar(a&#39;,W)} \\one\\{y \\le z\\} dP_0(w,a,y) \\ge c \\right\\}.\\end{equation*}\\] The empirical measure \\(P_{n}\\) is the law such that (i) \\(X\\) drawn from \\(P_{n}\\) takes its values in \\(\\{X_{1}, \\ldots, X_{n}\\}\\), and (ii) \\(X=X_{i}\\) with probability \\(n^{-1}\\)↩ "],
["3-smooth.html", "Section 3 Smoothness 3.1 Fluctuating smoothly 3.2 ⚙ Yet another experiment 3.3 ☡ More on fluctuations and smoothness 3.4 A fresh look at another_experiment 3.5 ☡ Asymptotic linearity and statistical efficiency 3.6 ⚙ Cramér-Rao bounds", " Section 3 Smoothness 3.1 Fluctuating smoothly Within our view of the target parameter as a statistical mapping evaluated at the law of the experiment, it is natural to inquire of properties this functional enjoys. For example, we may be interested in asking how the value of \\(\\Psi(P)\\) changes as we consider laws that get nearer to \\(P\\) in \\(\\calM\\). If small deviations from \\(P_0\\) result in large changes in \\(\\Psi(P_0)\\), then we might hypothesize that it will be difficult to produce stable estimators of \\(\\psi_0\\). Fortunately, this turns out not to be the case for the mapping \\(\\Psi\\), and so we say that \\(\\Psi\\) is a smooth statistical mapping. To discuss how \\(\\Psi(P)\\) changes for distributions that get nearer to \\(P\\) in the model, we require a more concrete notion of getting-nearness. The notion hinges on fluctuations (or fluctuating models). 3.1.1 The another_experiment fluctuation In Section 2.3.3, we discussed the nature of the object called another_experiment that was created when we ran example(tlrider): another_experiment #&gt; A law for (W,A,Y) in [0,1] x {0,1} x [0,1]. #&gt; #&gt; If the law is fully characterized, you can use method #&gt; &#39;sample_from&#39; to sample from it. #&gt; #&gt; If you built the law, or if you are an _oracle_, you can #&gt; also use methods &#39;reveal&#39; to reveal its relevant features #&gt; (QW, Gbar, Qbar, qY -- see &#39;?reveal&#39;), and &#39;alter&#39; to change #&gt; some of them. #&gt; #&gt; If all its relevant features are characterized, you can #&gt; use methods &#39;evaluate_psi&#39; to obtain the value of &#39;Psi&#39; at #&gt; this law (see &#39;?evaluate_psi&#39;) and &#39;evaluate_eic&#39; to obtain #&gt; the efficient influence curve of &#39;Psi&#39; at this law (see &#39;? #&gt; evaluate_eic&#39;). The message is a little misleading. Indeed, another_experiment is not a law but, rather, a collection of laws indexed by a real-valued parameter h. This oracular statement (we built the object!) is evident when one looks again at the sample_from feature of another_experiment: reveal(another_experiment)$sample_from #&gt; function(n, h) { #&gt; ## preliminary #&gt; n &lt;- R.utils::Arguments$getInteger(n, c(1, Inf)) #&gt; h &lt;- R.utils::Arguments$getNumeric(h) #&gt; ## ## &#39;Gbar&#39; and &#39;Qbar&#39; factors #&gt; Gbar &lt;- another_experiment$.Gbar #&gt; Qbar &lt;- another_experiment$.Qbar #&gt; ## sampling #&gt; ## ## context #&gt; params &lt;- formals(another_experiment$.QW) #&gt; W &lt;- stats::runif(n, min = eval(params$min), #&gt; max = eval(params$max)) #&gt; ## ## action undertaken #&gt; A &lt;- stats::rbinom(n, size = 1, prob = Gbar(W)) #&gt; ## ## reward #&gt; params &lt;- formals(another_experiment$.qY) #&gt; shape1 &lt;- eval(params$shape1) #&gt; QAW &lt;- Qbar(cbind(A = A, W = W), h = h) #&gt; Y &lt;- stats::rbeta(n, #&gt; shape1 = shape1, #&gt; shape2 = shape1 * (1 - QAW) / QAW) #&gt; ## ## observation #&gt; obs &lt;- cbind(W = W, A = A, Y = Y) #&gt; return(obs) #&gt; } #&gt; &lt;bytecode: 0xa6a2180&gt; #&gt; &lt;environment: 0xf61e700&gt; Let us call \\(\\Pi_{h} \\in \\calM\\) the law encoded by another_experiment for a given h taken in \\(]-1,1[\\). Note that \\[\\begin{equation*}\\calP \\equiv \\{\\Pi_h : h \\in ]-1,1[\\}\\end{equation*}\\] defines a collection of laws, i.e., a statistical model. We say that \\(\\calP\\) is a submodel of \\(\\calM\\) because \\(\\calP \\subset \\calM\\). Moreover, we say that this submodel is through \\(\\Pi_0\\) since \\(\\Pi_{h} = \\Pi_{0}\\) when \\(h = 0\\). We also say that \\(\\calP\\) is a fluctuation of \\(\\Pi_{0}\\). One could enumerate many possible submodels in \\(\\calM\\) through \\(\\Pi_0\\). It turns out that all that matters for our purposes is the form of the submodel in a neighborhood of \\(\\Pi_0\\). We informally say that this local behavior describes the direction of a submodel through \\(\\Pi_0\\). We formalize this notion Section 3.3. We now have a notion of how to move through the model space \\(P \\in \\calM\\) and can study how the value of the parameter changes as we move away from a law \\(P\\). Above, we said that \\(\\Psi\\) is a smooth parameter if it does not change “too much” as we move towards \\(P\\) in any particular direction. That is, we should hope that \\(\\Psi\\) is differentiable along our submodel at \\(P\\). This idea too is formalized in Section 3.3. We now turn to illustrating this idea numerically. 3.1.2 Numerical illustration The code below evaluates how the parameter changes for laws in \\(\\calP\\), and approximates the derivative of the parameter along the submodel \\(\\calP\\) at \\(\\Pi_0\\). Recall that the numerical value of \\(\\Psi(\\Pi_{0})\\) has already been computed and is stored in object psi_Pi_zero. approx &lt;- seq(-1, 1, length.out = 1e2) psi_Pi_h &lt;- sapply(approx, function(t) { evaluate_psi(another_experiment, h = t) }) slope_approx &lt;- (psi_Pi_h - psi_Pi_zero) / approx slope_approx &lt;- slope_approx[min(which(approx &gt; 0))] ggplot() + geom_point(data = data.frame(x = approx, y = psi_Pi_h), aes(x, y), color = &quot;#CC6666&quot;) + geom_segment(aes(x = -1, y = psi_Pi_zero - slope_approx, xend = 1, yend = psi_Pi_zero + slope_approx), arrow = arrow(length = unit(0.03, &quot;npc&quot;)), color = &quot;#9999CC&quot;) + geom_vline(xintercept = 0, color = &quot;#66CC99&quot;) + geom_hline(yintercept = psi_Pi_zero, color = &quot;#66CC99&quot;) + labs(x = &quot;h&quot;, y = expression(Psi(Pi[h]))) Figure 3.1: Evolution of statistical mapping \\(\\Psi\\) along fluctuation \\(\\{\\Pi_{h} : h \\in H\\}\\). The dotted curve represents the function \\(h \\mapsto \\Psi(\\Pi_{h})\\). The blue line represents the tangent to the previous curve at \\(h=0\\), which indeed appears to be differentiable around \\(h=0\\). In Section 3.4, we derive a closed-form expression for the slope of the blue curve. 3.2 ⚙ Yet another experiment Adapt the code from Problem 1 in Section 1.3 to visualize \\(w \\mapsto \\Exp_{\\Pi_h}(Y | A = 1, W = w)\\), \\(w \\mapsto \\Exp_{\\Pi_h}(Y | A = 0, W=w)\\), and \\(w \\mapsto \\Exp_{\\Pi_h}(Y | A = 1, W=w) - \\Exp_{\\Pi_h}(Y | A = 0, W=w)\\), for \\(h \\in \\{-1/2, 0, 1/2\\}\\). Run the following chunk of code. yet_another_experiment &lt;- copy(another_experiment) alter(yet_another_experiment, Qbar = function(AW, h){ A &lt;- AW[, &quot;A&quot;] W &lt;- AW[, &quot;W&quot;] expit( logit( A * W + (1 - A) * W^2 ) + h * (2*A - 1) / ifelse(A == 1, sin((1 + W) * pi / 6), 1 - sin((1 + W) * pi / 6)) * (Y - A * W + (1 - A) * W^2)) }) Justify that yet_another_fluctuation characterizes another fluctuation of \\(\\Pi_{0}\\). Comment upon the similarities and differences between \\(\\{\\Pi_{h} : h \\in ]-1,1[\\}\\) and \\(\\{\\Pi_{h}&#39; : h \\in ]-1,1[\\}\\). Repeat Problem 1 above with \\(\\Pi_{h}&#39;\\) substituted for \\(\\Pi_{h}\\). Re-produce Figure 3.1 for the \\(\\{\\Pi_h&#39; : h \\in ]-1,1[\\}\\) fluctuation. Comment on the similarities and differences between the resulting figure and Figure 3.1. In particular, how does the behavior of the target parameter around \\(h = 0\\) compare between laws \\(\\Pi_0\\) and \\(\\Pi_0&#39;\\)? 3.3 ☡ More on fluctuations and smoothness 3.3.1 Fluctuations Let us now formally define what it means for statistical mapping \\(\\Psi\\) to be smooth at every \\(P \\in \\calM\\). For every \\(h \\in H \\equiv ]-M^{-1},M^{-1}[\\), we can define a law \\(P_{h} \\in \\calM\\) by setting \\(P_{h} \\ll P\\)4 and \\[\\begin{equation} \\frac{dP_h}{dP} \\equiv 1 + h s, \\tag{3.1} \\end{equation}\\] where \\(s : \\calO\\to \\bbR\\) is a (measurable) function of \\(O\\) such that \\(s(O)\\) is not equal to zero \\(P\\)-almost surely, \\(\\Exp_{P} (s(O)) = 0\\), and \\(s\\) bounded by \\(M\\). We make the observation that \\[\\begin{equation} (i) \\quad P_h|_{h=0} = P,\\quad (ii) \\quad \\left.\\frac{d}{dh} \\log \\frac{dP_h}{dP}(O)\\right|_{h=0} =s(O). \\tag{3.2} \\end{equation}\\] Because of (i), \\(\\{P_{h} : h \\in H\\}\\) is a submodel through \\(P\\), also referred to as a fluctuation of \\(P\\). The fluctuation is a one-dimensional submodel of \\(\\calM\\) with univariate parameter \\(h \\in H\\). We note that (ii) indicates that the score of this submodel at \\(h = 0\\) is \\(s\\). Thus, we say that the fluctuation is in the direction of \\(s\\). Fluctuations of \\(P\\) do not necessarily take the same form as in (3.1). No matter how the fluctuation is built, for our purposes the most important feature of the fluctuation is its local shape in a neighborhood of \\(P\\). 3.3.2 Smoothness and gradients We are now prepared to provide a formal definition of smoothness of statistical mappings. We say that a statistical mapping \\(\\Psi\\) is smooth at every \\(P \\in \\calM\\) if for each \\(P \\in \\calM\\), there exists a (measurable) function \\(D^{*}(P) : \\calO \\to \\bbR\\) such that \\(\\Exp_{P}(D^{*}(P)(O)) = 0\\), \\(\\Var_{P}(D^{*}(P)(O)) &lt; \\infty\\), and, for every fluctuation \\(\\{P_{h} : h \\in H\\}\\) with score \\(s\\) at \\(h = 0\\), the real-valued mapping \\(h \\mapsto \\Psi(P_{h})\\) is differentiable at \\(h=0\\), with a derivative equal to \\[\\begin{equation} \\Exp_{P} \\left(D^{*}(P)\\tag{3.3} (#eq:derivative) \\end{equation}\\] The object \\(D^*(P)\\) in (3.3) is called a gradient of \\(\\Psi\\) at \\(P\\).5 3.3.3 A Euclidean perspective This terminology has a direct parallel to directional derivatives in the calculus of Euclidean geometry. Recall that if \\(f\\) is a differentiable mapping from \\(\\bbR^p\\) to \\(\\bbR\\), then the directional derivative of \\(f\\) at a point \\(x\\) (an element of \\(\\bbR^p\\)) in direction \\(u\\) (a unit vector in \\(\\bbR^p\\)) is the scalar product of the gradient of \\(f\\) and \\(u\\). In words, the directional derivative of \\(f\\) at \\(x\\) can be represented as a scalar product of the direction that we approach \\(x\\) and the change of the function’s value at \\(x\\). In the present problem, the law \\(P\\) is the point at which we evaluate the function \\(\\Psi\\), the score \\(s\\) of the fluctuation is the direction in which we approach the point, and the gradient describes the change in the function’s value at the point. 3.3.4 The canonical gradient In general, it is possible for many gradients to exist6. Yet, in the special case that the model is nonparametric, only a single gradient exists. The unique gradient is then referred to as the canonical gradient or the efficient influence curve, for reasons that will be clarified in Section 3.5. In the more general setting, the canonical gradient may be defined as the minimizer of \\(D\\mapsto \\Var_{P} (D(O))\\) over the set of all gradients. It is not difficult to check that the efficient influence curve of statistical mapping \\(\\Psi\\) (2.6) at \\(P \\in \\calM\\) can be written as \\[\\begin{align} D^{*}(P) &amp; \\equiv D_{1}^{*} (P) + D_{2}^{*} (P), \\quad \\text{where} \\tag{3.4}\\\\ D_{1}^{*}(P) (O) &amp;\\equiv \\Qbar(1,W) - \\Qbar(0,W) - \\Psi(P), \\notag\\\\ D_{2}^{*}(P) (O) &amp;\\equiv \\frac{2A-1}{\\ell\\Gbar(A,W)}(Y - \\Qbar(A,W)).\\notag \\end{align}\\] A method from package tlrider evaluates the efficient influence curve at a law described by an object of class LAW. It is called evaluate_eic. For instance, the next chunk of code evaluates the efficient influence curve \\(D^{*}(P_{0})\\) of \\(\\Psi\\) (2.6) at \\(P_{0} \\in \\calM\\) that is characterized by experiment: eic_experiment &lt;- evaluate_eic(experiment) The efficient influence curve \\(D^{*}(P_{0})\\) is a function from \\(\\bbO\\) to \\(\\bbR\\). As such, it can be evaluated at the five independent observations drawn from \\(P_{0}\\) in Section 1.2.2. This is what the next chunk of code does: (eic_experiment(five_obs)) #&gt; [1] 0.260 0.161 -0.387 -0.186 0.110 Finally, the efficient influence curve can be visualized as two images that represent \\((w,y) \\mapsto D^{*}(P_{0})(w,a,y)\\) for \\(a = 0,1\\), respectively: crossing(w = seq(0, 1, length.out = 2e2), a = c(0, 1), y = seq(0, 1, length.out = 2e2)) %&gt;% mutate(eic = eic_experiment(cbind(Y=y,A=a,W=w))) %&gt;% ggplot(aes(x = w, y = y, fill = eic)) + geom_raster(interpolate = TRUE) + geom_contour(aes(z = eic), color = &quot;white&quot;) + facet_wrap(~ a, nrow = 1, labeller = as_labeller(c(`0` = &quot;a = 0&quot;, `1` = &quot;a = 1&quot;))) + labs(fill = expression(paste(D^&quot;*&quot;, (P[0])(w,a,y)))) Figure 3.2: Visualizing the efficient influence curve \\(D^{*}(P_{0})\\) of \\(\\Psi\\) (2.6) at \\(P_{0}\\), the law described by experiment. 3.4 A fresh look at another_experiment We can give a fresh look at Section 3.1.2 now. 3.4.1 Deriving the efficient influence curve It is not difficult (though cumbersome) to verify that, up to a constant, \\(\\{\\Pi_{h} : h \\in [-1,1]\\}\\) is a fluctuation of \\(\\Pi_{0}\\) in the direction (in the sense of (3.1)) of \\[\\begin{align} \\notag\\sigma_{0}(O) \\equiv - 10 \\sqrt{W} A \\times \\beta_{0} &amp; (A,W)\\\\ &amp;\\times\\left(\\log(1 - Y) + \\sum_{k=0}^{3} \\left(k + \\beta_{0} (A,W)\\right)^{-1}\\right) + \\text{constant},\\\\ \\text{where} \\; \\beta_{0}(A,W)&amp;\\equiv \\frac{1 -\\Qbar_{\\Pi_{0}}(A,W)}{\\Qbar_{\\Pi_{0}}(A,W)}. \\tag{3.5}\\end{align}\\] Consequently, the slope of the dotted curve in Figure 3.1 is equal to \\[\\begin{equation} \\Exp_{\\Pi_{0}} (D^{*}(\\Pi_{0}) (O) \\sigma_{0}(O)). \\tag{3.6} \\end{equation}\\] Since \\(D^{*}(\\Pi_{0})\\) is centered under \\(\\Pi_{0}\\), knowing \\(\\sigma_{0}\\) up to a constant is not problematic. 3.4.2 Numerical validation In the following code, we check the above fact numerically. When we ran example(tlrider), we created a function sigma0. The function implements \\(\\sigma_{0}\\) defined in (3.5): sigma0 #&gt; function(obs, law = another_experiment) { #&gt; ## preliminary #&gt; Qbar &lt;- get_feature(law, &quot;Qbar&quot;, h = 0) #&gt; QAW &lt;- Qbar(obs[, c(&quot;A&quot;, &quot;W&quot;)]) #&gt; params &lt;- formals(get_feature(law, &quot;qY&quot;, h = 0)) #&gt; shape1 &lt;- eval(params$shape1) #&gt; ## computations #&gt; betaAW &lt;- shape1 * (1 - QAW) / QAW #&gt; out &lt;- log(1 - obs[, &quot;Y&quot;]) #&gt; for (int in 1:shape1) { #&gt; out &lt;- out + 1/(int - 1 + betaAW) #&gt; } #&gt; out &lt;- - out * shape1 * (1 - QAW) / QAW * #&gt; 10 * sqrt(obs[, &quot;W&quot;]) * obs[, &quot;A&quot;] #&gt; ## no need to center given how we will use it #&gt; return(out) #&gt; } The next chunk of code approximates (3.6) pointwise and with a confidence interval of asymptotic level 95%: eic_another_experiment &lt;- evaluate_eic(another_experiment, h = 0) obs_another_experiment &lt;- sample_from(another_experiment, B, h = 0) vars &lt;- eic_another_experiment(obs_another_experiment) * sigma0(obs_another_experiment) sd_hat &lt;- sd(vars) (slope_hat &lt;- mean(vars)) #&gt; [1] 1.35 (slope_CI &lt;- slope_hat + c(-1, 1) * qnorm(1 - alpha / 2) * sd_hat / sqrt(B)) #&gt; [1] 1.33 1.36 Equal to 1.349 (rounded to three decimal places — hereafter, all rounding will be to three decimal places as well), the first numerical approximation slope_approx is not too off! 3.5 ☡ Asymptotic linearity and statistical efficiency 3.5.1 Asymptotic linearity Suppose that \\(O_{1}, \\ldots, O_{n}\\) are drawn independently from \\(P\\in \\calM\\). If an estimator \\(\\psi_n\\) of \\(\\Psi(P)\\) can be written as \\[\\begin{equation*} \\psi_n = \\Psi(P) + \\frac{1}{n}\\sum_{i=1}^n \\IC(O_i) + o_{P}(1/\\sqrt{n})\\end{equation*}\\] for some function \\(\\IC : \\calO \\to \\bbR\\) such that \\(\\Exp_P(\\IC(O)) = 0\\) and \\(\\Var_{P}(\\IC(O)) &lt; \\infty\\), then we say that \\(\\psi_n\\) is asymptotically linear with influence curve \\(\\IC\\). Asymptotically linear estimators are weakly convergent. Specifically, if \\(\\psi_n\\) is asymptotically linear with influence curve \\(\\IC\\), then \\[\\begin{equation} \\sqrt{n} (\\psi_n - \\Psi(P)) = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\IC(O_i) + o_P(1) \\tag{3.7} \\end{equation}\\] and, by the central limit theorem (recall that \\(O_{1}, \\ldots, O_{n}\\) are independent), \\(\\sqrt{n} (\\psi_n - \\Psi(P))\\) converges in law to a centered Gaussian distribution with variance \\(\\Var_P(\\IC(O))\\). 3.5.2 Influence curves and gradients As it happens, influence curves of regular7 estimators are intimately related to gradients. In fact, if \\(\\psi_n\\) is a regular, asymptotically linear estimator of \\(\\Psi(P)\\) with influence curve \\(\\IC\\), then it must be true that \\(\\Psi\\) is a smooth at \\(P\\) and that \\(\\IC\\) is a gradient of \\(\\Psi\\) at \\(P\\). 3.5.3 Asymptotic efficiency Now recall that, in Section 3.3.4, we defined the canonical gradient as the minimizer of \\(D \\mapsto \\Var_{P}(D(O))\\) over the set of all gradients. Therefore, if \\(\\psi_{n}\\) is a regular, asymptotically linear estimator of \\(\\Psi(P)\\) (built from \\(n\\) independent observations drawn from \\(P\\)), then the asymptotic variance of \\(\\sqrt{n} (\\psi_{n} - \\Psi(P))\\) cannot be smaller than the variance of the canonical gradient of \\(\\Psi\\) at \\(P\\), i.e., \\[\\begin{equation} \\tag{3.8}\\Var_{P}(D^{*}(P)(O)). \\end{equation}\\] In other words, (3.8) is the lower bound on the asymptotic variance of any regular, asymptotically linear estimator of \\(\\Psi(P)\\). This bound is referred to as the Cramér-Rao bound. Any regular estimator that achieves this variance bound is said to be asymptotically efficient at \\(P\\). Because the canonical gradient is the influence curve of an asymptotically efficient estimator, it is often referred to as the efficient influence curve. 3.6 ⚙ Cramér-Rao bounds What does the following chunk do? obs &lt;- sample_from(experiment, B) (cramer_rao_hat &lt;- var(eic_experiment(obs))) #&gt; [1] 0.287 Same question about this one. obs_another_experiment &lt;- sample_from(another_experiment, B, h = 0) (cramer_rao_Pi_zero_hat &lt;- var(eic_another_experiment(obs_another_experiment))) #&gt; [1] 0.098 With a large independent sample drawn from \\(\\Psi(P_0)\\) (or \\(\\Psi(\\Pi_0)\\)), is it possible to construct a regular estimator \\(\\psi_{n}\\) of \\(\\Psi(P_0)\\) (or \\(\\Psi(\\Pi_0)\\)) such that the asymptotic variance of \\(\\sqrt{n}\\) times \\(\\psi_{n}\\) minus its target be smaller than the Cramér-Rao bound? Is it easier to estimate \\(\\Psi(P_{0})\\) or \\(\\Psi(\\Pi_{0})\\) (from independent observations drawn from either law)? In what sense? (Hint: you may want to compute a ratio.) That is, \\(P_{h}\\) is dominated by \\(P\\): if an event \\(A\\) satisfies \\(P(A) = 0\\), then necessarily \\(P_{h} (A) = 0\\) too.↩ Interestingly, if a fluctuation \\(\\{P_{h} : h \\in H\\}\\) satisfies (3.2) for a direction \\(s\\) such that \\(s\\neq 0\\), \\(\\Exp_{P}(s(O)) = 0\\) and \\(\\Var_{P} (s(O)) &lt; \\infty\\), then \\(h \\mapsto \\Psi(P_{h})\\) is still differentiable at \\(h=0\\) with a derivative equal to (3.3) beyond fluctuations of the form (3.1).↩ This may be at first surprising given the parallel drawn in Section 3.3.3 to Euclidean geometry. However, it is important to remember that the model dictates fluctuations of \\(P\\) that are valid submodels with respect to the full model. In turn, this determines the possible directions from which we may approach \\(P\\). Thus, depending on the direction, (3.3) may hold with different choices of \\(D^*\\).↩ We can view \\(\\psi_{n}\\) as the by product of an algorithm \\(\\Psihat\\) trained on independent observations \\(O_{1}, \\ldots, O_{n}\\) drawn from \\(P\\). We say that the estimator is regular at \\(P\\) if, for any direction \\(s\\neq 0\\) such that \\(\\Exp_{P} (s(O)) = 0\\) and \\(\\Var_{P} (s(O)) &lt; \\infty\\) and fluctuation \\(\\{P_{h} : h \\in H\\}\\) satisfying (3.2), the estimator \\(\\psi_{n,1/\\sqrt{n}}\\) of \\(\\Psi(P_{1/\\sqrt{n}})\\) obtained by training \\(\\Psihat\\) on independent observations \\(O_{1}\\), , \\(O_{n}\\) drawn from \\(P_{1/\\sqrt{n}}\\) is such that \\(\\sqrt{n} (\\psi_{n,1/\\sqrt{n}} - \\Psi(P_{1/\\sqrt{n}}))\\) converges in law to a limit that does not depend on \\(s\\).↩ "],
["4-double-robustness.html", "Section 4 Double-robustness 4.1 Linear approximations of parameters 4.2 ⚙ The remainder term 4.3 ☡ Double-robustness 4.4 ⚙ Double-robustness", " Section 4 Double-robustness 4.1 Linear approximations of parameters 4.1.1 From gradients to estimators We learned in Section 3 that the stochastic behavior of a regular, asymptotically linear estimator of \\(\\Psi(P)\\) can be characterized by its influence curve. Moreover, we said that this influence curve must in fact be a gradient of \\(\\Psi\\) at \\(P\\). In this section, we show that the converse is also true: given a gradient \\(D^*\\) of \\(\\Psi\\) at \\(P\\), under so-called regularity conditions, it is possible to construct an estimator with influence curve equal to \\(D^*(P)\\). This fact will suggest concrete strategies for generating efficient estimators of smooth parameters. We take here the first step towards generating such estimators: linearizing the parameter. 4.1.2 A Euclidean perspective As in Section 3.3.3, drawing a parallel to Euclidean geometry is helpful. We recall that if \\(f\\) is a differentiable mapping from \\(\\bbR^p\\) to \\(\\bbR\\), then a Taylor series approximates \\(f\\) at a point \\(x_0 \\in \\bbR^p\\): \\[\\begin{equation*} f(x_0) \\approx f(x) + \\langle(x_0 - x), \\nabla f(x)\\rangle,\\end{equation*}\\] where \\(x\\) is a point in \\(\\bbR^p\\), \\(\\nabla f(x)\\) is the gradient of \\(f\\) evaluated at \\(x\\) and \\(\\langle u,v\\rangle\\) is the scalar product of \\(u,v \\in \\bbR^{p}\\). As the squared distance \\(\\|x-x_{0}\\|^{2} = \\langle x-x_{0}, x-x_{0}\\rangle\\) between \\(x\\) and \\(x_0\\) decreases, the linear approximation to \\(f(x_0)\\) becomes more accurate. 4.1.3 The remainder term Returning to the present problem with this in mind, we find that indeed a similar approximation strategy may be applied. For clarity, let us introduce a new shorthand notation. For any measurable function \\(f\\) of the observed data \\(O\\), we may write from now on \\(P f \\equiv \\Exp_P(f(O))\\). One may argue that the notation is valuable beyong the gain of space. For instance, (3.7) \\[\\begin{equation*} \\sqrt{n} (\\psi_n - \\Psi(P)) = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\IC(O_i) + o_P(1) \\end{equation*}\\] can be rewritten as \\[\\begin{equation*} \\sqrt{n} (\\psi_n - \\Psi(P)) = \\sqrt{n} (P_{n} - P) \\IC + o_P(1), \\end{equation*}\\] thus suggesting more clearly the importance of the so-called empirical process \\(\\sqrt{n} (P_{n} - P)\\). In particular, if \\(\\Psi\\) is smooth uniformly over directions, then for any given \\(P \\in \\calM\\), we can write \\[\\begin{equation} \\Psi(P_0) = \\Psi(P) + (P_0 - P) D^*(P) - \\Rem_{P_0}(P), \\tag{4.1} \\end{equation}\\] where \\(\\Rem_{P_0}(P)\\) is a remainder term satisfying that \\[\\begin{equation*} \\frac{\\Rem_{P_0}(P)}{d(P, P_0)} \\rightarrow 0 \\ \\mbox{as} \\ d(P, P_0) \\rightarrow 0 , \\end{equation*}\\] where \\(d\\) is a measure of discrepancy for distributions in \\(\\calM\\). Note that (4.1) can be equivalently written as \\[\\begin{equation*} \\Psi(P_0) = \\Psi(P) + \\Exp_{P_0}(D^*(P)(O)) - \\Exp_P(D^*(P)(O)) - \\Rem_{P_0}(P). \\end{equation*}\\] The remainder term formalizes the notion that if \\(P\\) is close to \\(P_0\\) (i.e., if \\(d(P,P_0)\\) is small), then the linear approximation of \\(\\Psi(P_0)\\) is more accurate. 4.1.4 Expressing the remainder term as a function of the relevant features The equations for the definition of the parameter (2.6), form of the canonical gradient (3.4), and linearization of parameter (4.1) combine to determine the remainder: \\[\\begin{equation} \\Rem_{P_0}(P) \\equiv \\Psi(P) - \\Psi(P_0) - (P_0 - P)D^*(P) \\tag{4.2} \\end{equation}\\] hence \\[\\begin{multline} \\Rem_{P_0}(P)= \\Exp_{P_0} \\Bigg[ \\left(\\Gbar_0(W) - \\Gbar(W)\\right) \\\\ \\times \\left(\\frac{\\Qbar_0(1,W) - \\Qbar(1,W)}{\\ell\\Gbar(1,W)} + \\frac{\\Qbar_0(0,W) - \\Qbar(0,W)}{\\ell\\Gbar(0,W)} \\right) \\Bigg]. \\tag{4.3} \\end{multline}\\] Acting as oracles, we can compute explicitly the remainder term \\(\\Rem_{P_0}(P)\\). The evaluate_remainder method makes it very easy (simply run ?evaluate_remainder to see the man page of the method): (evaluate_remainder(experiment, experiment)) #&gt; [1] 0 (rem &lt;- evaluate_remainder(experiment, another_experiment, list(list(), list(h = 0)))) #&gt; [1] 0.199 We recover the equality \\(\\Rem_{P_{0}} (P_{0}) = 0\\), which is fairly obvious given (4.1). In addition, we learn that \\(\\Rem_{P_{0}} (\\Pi_{0})\\) equals 0.199. In the next subsection, we invite you to make better acquaintance with the remainder term by playing around with it numerically. 4.2 ⚙ The remainder term Compute numerically \\(\\Rem_{\\Pi_0}(\\Pi_h)\\) for \\(h \\in [-1,1]\\) and plot your results. What do you notice? ☡ Approximate \\(\\Rem_{P_{0}} (\\Pi_{0})\\) numerically without relying on method evaluate_remainder and compare the value you get with that of rem. (Hint: use (4.2) and a large sample of observations drawn independently from \\(P_{0}\\).) 4.3 ☡ Double-robustness 4.3.1 The key property Let us denote by \\(\\|f\\|_{P}^{2}\\) the square of the \\(L^{2}(P)\\)-norm of any function \\(f\\) from \\(\\bbO\\) to \\(\\bbR\\) i.e., using a recently introduced notation, \\(\\|f\\|_{P}^{2} \\equiv Pf^{2}\\). For instance, \\(\\|\\Qbar_{1} - \\Qbar_{0}\\|_{P}\\) or \\(\\|\\Gbar_{1} - \\Gbar_{0}\\|_{P}\\) is a distance separating the features \\(\\Qbar_{1}\\) and \\(\\Qbar_{0}\\) or \\(\\Gbar_{1}\\) and \\(\\Gbar_{0}\\). The efficient influence curve \\(D^{*}(P)\\) at \\(P \\in \\calM\\) enjoys a rather remarkable property: it is double-robust. Specifically, for every \\(P \\in \\calM\\), the remainder term \\(\\Rem_{P_{0}} (P)\\) satisfies \\[\\begin{equation} \\Rem_{P_{0}} (P)^{2} \\leq \\|\\Qbar - \\Qbar_{0}\\|_{P}^{2} \\times \\|(\\Gbar - \\Gbar_{0})/\\ell\\Gbar_{0}\\|_{P}^{2}, \\tag{4.4} \\end{equation}\\] where \\(\\Qbar\\) and \\(\\Gbar\\) are the counterparts under \\(P\\) to \\(\\Qbar\\)_{0}$ and \\(\\Gbar_{0}\\). The proof consists in a straightforward application of the Cauchy-Schwarz inequality to the right-hand side expression in (4.2). 4.3.2 Its direct consequence It may not be clear yet why (4.4) is an important property, and why \\(D^{*}\\) is said double-robust because of it. To answer the latter question, let us consider a law \\(P\\in \\calM\\) such that either \\(\\Qbar = \\Qbar_{0}\\) or \\(\\Gbar = \\Gbar_{0}\\). It is then the case that either \\(\\|\\Qbar - \\Qbar_{0}\\|_{P} = 0\\) or \\(\\|\\Gbar - \\Gbar_{0}\\|_{P} = 0\\). Therefore, in light of (4.4), it also holds that \\(\\Rem_{P_{0}} (P) = 0\\).8 It thus appears that (4.1) simplifies to \\[\\begin{align*} \\Psi(P_0) &amp;= \\Psi(P) + (P_0 - P) D^*(P)\\\\ &amp;= \\Psi(P) + P_0 D^*(P),\\end{align*}\\] where the second equality holds because \\(PD^{*}(P) = 0\\) for all \\(P\\in \\calM\\) by definition of \\(D^{*}(P)\\). It is now clear that for such a law \\(P\\in \\calM\\), \\(\\Psi(P) = \\Psi(P_{0})\\) is equivalent to \\[\\begin{equation} P_{0} D^{*}(P) = 0. \\tag{4.5} \\end{equation}\\] Most importantly, in words, if \\(P\\) solves the so-called \\(P_{0}\\)-specific efficient influence curve equation (4.5) and if, in addition, \\(P\\) has the same \\(\\Qbar\\)-feature or \\(\\Gbar\\)-feature as \\(P_{0}\\), then \\(\\Psi(P) = \\Psi(P_{0})\\). The conclusion is valid no matter how \\(P\\) may differ from \\(P_{0}\\) otherwise, hence the notion of being double-robust. This property is useful to build consistent estimators of \\(\\Psi(P)\\), as we shall see in Section 5. 4.4 ⚙ Double-robustness Go back to Problem 1 in 4.2. In light of Section 4.3, what is happening? Create a copy of experiment and replace its Gbar feature with some other function of \\(W\\) (see ?copy, ?alter and Problem 2 in Section 3.2). Call \\(P&#39;\\) the element of model \\(\\calM\\) thus characterized. Can you guess the values of \\(\\Rem_{P_{0}}(P&#39;)\\), \\(\\Psi(P&#39;)\\) and \\(P_{0} D^{*}(P&#39;)\\)? Support your argument. Add more exercizes? This also trivially follows from (4.3).↩ "],
["5-inference.html", "Section 5 Inference 5.1 Where we stand 5.2 Where we go", " Section 5 Inference 5.1 Where we stand In the previous sections, we analyzed our target parameter and presented relevant theory for understanding the statistical properties of certain types of estimators of the parameter. The theory is also relevant for building and comparing a variety of estimators. We assume from now on that we have available a sample \\(O_{1}, \\ldots, O_{B}\\) of independent observations drawn from \\(P_{0}\\). This is literally the case!, and the observations are stored in obs that we created in Section 3.6. iter &lt;- 1e2 Equal to 0.1 million, the sample size B is very large. We will in fact use 100 disjoint subsamples composed of \\(n\\) independent observations among \\(O_{1}, \\ldots, O_{B}\\), where \\(n\\) equals B/iter, i.e., 1000. We will thus be in a position to investigate the statistical properties of every estimation procedure by replicating it independently 100 times. 5.2 Where we go The following sections explore different statistical paths to inferring \\(\\psi_{0}\\) or, rather (though equivalently), \\(\\Psi(P_{0})\\). Section 6 present a simple inference strategy. It can be carried out in situations where \\(\\Gbar_{0}\\) is already known to the statistician. Section 7 discusses the estimation of some infinite-dimensional features of \\(P_{0}\\). The resulting estimators are later used to infer \\(\\psi_{0}\\). Section… "],
["6-simple-strategy.html", "Section 6 A simple inference strategy 6.1 A cautionary detour 6.2 ⚙ Delta-method 6.3 IPTW estimator assuming the mechanism of action known", " Section 6 A simple inference strategy 6.1 A cautionary detour Let us introduce first the following estimator: \\[\\begin{align} \\notag \\psi_{n}^{a} &amp;\\equiv \\frac{\\Exp_{P_{n}} (AY)}{\\Exp_{P_{n}} (A)} - \\frac{\\Exp_{P_{n}} ((1-A)Y)}{\\Exp_{P_{n}}(1-A)} \\\\ &amp;= \\frac{\\sum_{i=1}^{n} \\one\\{A_{i}=Y_{i}=1\\}}{\\sum_{i=1}^{n} \\one\\{A_{i}=1\\}} - \\frac{\\sum_{i=1}^{n} \\one\\{A_{i}=0,Y_{i}=1\\}}{\\sum_{i=1}^{n}\\one\\{A_{i}=0\\}}. \\tag{6.1} \\end{align}\\] It estimates \\[\\begin{align*}\\Phi(P_{0}) &amp;\\equiv \\frac{\\Exp_{P_{0}} (AY)}{\\Exp_{P_{0}} (A)} - \\frac{\\Exp_{P_{0}} ((1-A)Y)}{\\Exp_{P_{0}} (1-A)}\\\\&amp;= \\Exp_{P_{0}} (Y | A=1) - \\Exp_{P_{0}} (Y | A=0).\\end{align*}\\] We seize this opportunity to demonstrate numerically the obvious fact that \\(\\psi_{n}^{a}\\) does not estimate \\(\\Psi(P_{0})\\) because, in general, \\(\\Psi(P_{0})\\) and \\(\\Phi(P_{0})\\) differ. This is apparent in the following alternative expression of \\(\\Phi(P_{0})\\): \\[\\begin{align*} \\Phi(P_{0}) &amp;= \\Exp_{P_{0}} \\left(\\Exp_{P_0}(Y \\mid A, W) |A=1) \\right) - \\Exp_{P_{0}} \\left(\\Exp_{P_0}(Y \\mid A, W) | A=0\\right)\\\\ &amp;= \\int \\Qbar_{0}(1, w) dP_{0,W|A=1}(w) - \\int \\Qbar_{0}(0, w) dP_{0,W|A=0}(w). \\end{align*}\\] Contrast the above equalities and (2.1). In the latter, the outer integral is against the marginal law of \\(W\\) under \\(P_{0}\\). In the former, the outer integrals are respectively against the conditional laws of \\(W\\) given \\(A=1\\) and \\(A=0\\) under \\(P_{0}\\). 6.2 ⚙ Delta-method Consider the next chunk of code: compute_irrelevant_estimator &lt;- function(obs) { Y &lt;- pull(obs, Y) A &lt;- pull(obs, A) psi_n &lt;- mean(A * Y) / mean(A) - mean((1 - A) * Y) / mean(1 - A) Var_n &lt;- cov(cbind(A * Y, A, (1 - A) * Y, (1 - A))) phi_n &lt;- c(1 / mean(A), -mean(A * Y) / mean(A)^2, -1 / mean(1 - A), mean((1 - A) * Y) / mean(1 - A)^2) var_n &lt;- as.numeric(t(phi_n) %*% Var_n %*% phi_n) sig_n &lt;- sqrt(var_n / nrow(obs)) tibble(psi_n = psi_n, sig_n = sig_n) } Function compute_irrelevant_estimator computes the estimator \\(\\psi_{n}^{a}\\) (6.1) based on the data set in obs. Introduce \\(X_{n} \\equiv n^{-1}\\sum_{i=1}^{n} \\left(A_{i}Y_{i}, A_{i}, (1-A_{i})Y_{i}, 1-A_{i}\\right)^{\\top}\\) and \\(X \\equiv \\left(AY, A, (1-A)Y, 1-A\\right)^{\\top}\\). It happens that \\(X_{n}\\) is asymptotically Gaussian: as \\(n\\) goes to infinity, \\[\\begin{equation*}\\sqrt{n} \\left(X_{n} - \\Exp_{P_{0}} (X)\\right)\\end{equation*}\\] converges in law to the centered Gaussian law with covariance matrix \\[\\begin{equation*}V_{0} \\equiv \\Exp_{P_{0}} \\left((X - \\Exp_{P_{0}} (X)) \\times (X- \\Exp_{P_{0}} (X))^{\\top}\\right).\\end{equation*}\\] Let \\(f:\\bbR\\times \\bbR^{*} \\times \\bbR\\times \\bbR^{*}\\) be given by \\(f(r,s,t,u) = r/s - t/u\\). The function is differentiable. Check that \\(\\psi_{n}^{a} = f(X_{n})\\). Point out to the line where \\(\\psi_{n}^{a}\\) is computed in the body of compute_irrelevant_estimator. Also point out to the line where the above asymptotic variance of \\(X_{n}\\) is estimated with its empirical counterpart, say \\(V_{n}\\). ☡ Argue how the delta-method yields that \\(\\sqrt{n}(\\psi_{n}^{a} - \\Phi(P_{0}))\\) converges in law to the centered Gaussian law with a variance that can be estimated with \\[\\begin{equation} v_{n}^{a} \\equiv \\nabla f(X_{n}) \\times V_{n} \\times \\nabla f(X_{n})^{\\top}. \\tag{6.2} \\end{equation}\\] Check that the gradient \\(\\nabla f\\) of \\(f\\) is given by \\(\\nabla f(r,s,t,u) \\equiv (1/s, -r/s^{2}, -1/u, t/u^{2})\\). Point out to the line where the asymptotic variance of \\(\\psi_{n}^{a}\\) is estimated. 6.3 IPTW estimator assuming the mechanism of action known 6.3.1 A simple substitution estimator Let us assume for a moment that we know \\(\\Gbar_{0}\\). This would have been the case indeed if \\(P_{0}\\) were a controlled experiment. Note that, on the contrary, assuming \\(\\Qbar_{0}\\) known would be difficult to justify. Gbar &lt;- get_feature(experiment, &quot;Gbar&quot;) The alternative expression (2.7) suggests to estimate \\(\\Psi(P_{0})\\) with \\[\\begin{align} \\psi_{n}^{b} &amp;\\equiv \\Exp_{P_{n}} \\left( \\frac{2A-1}{\\ell \\Gbar_{0}(A,W)} Y \\right) \\\\ &amp; = \\frac{1}{n} \\sum_{i=1}^{n} \\left(\\frac{2A_{i}-1}{ \\ell\\Gbar_{0}(A_{i},W_{i})}Y_{i} \\right). \\tag{6.3} \\end{align}\\] Note how \\(P_{n}\\) is substituted for \\(P_{0}\\) in (6.3) relative to (2.7). This justifies that we call \\(\\psi_{n}^{b}\\) a substitution estimator (for the same reason, \\(\\psi_{n}^{a}\\) is a substitution estimator of \\(\\Phi(P_{0})\\)). It is also dubbed an IPTW (inverse probability of treatment weighted) estimator because of the denominators \\(\\ell\\Gbar_{0}(A_{i},W_{i})\\) in its definition.9 In Section 8.2, we develop another IPTW estimator that does not assume that \\(\\Gbar_{0}\\) is known beforehand. 6.3.2 Elementary statistical properties It is easy to check that \\(\\psi_{n}^{b}\\) estimates \\(\\Psi(P_{0})\\) consistently, but this is too little to request from an estimator of \\(\\psi_{0}\\). Better, \\(\\psi_{n}^{b}\\) also satisfies a central limit theorem: \\(\\sqrt{n} (\\psi_{n}^{b} - \\psi_{0})\\) converges in law to a centered Gaussian law with asymptotic variance \\[\\begin{equation*}v^{b} \\equiv \\Var_{P_{0}} \\left(\\frac{2A-1}{\\ell\\Gbar_{0}(A,W)}Y\\right),\\end{equation*}\\] where \\(v^{b}\\) can be consistently estimated by its empirical counterpart \\[\\begin{align} \\tag{6.4} v_{n}^{b} &amp;\\equiv \\Var_{P_{n}} \\left(\\frac{2A-1}{\\ell\\Gbar_{0}(A,W)}Y\\right) \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^{n}\\left(\\frac{2A_{i}-1}{\\ell\\Gbar_{0} (A_{i},W_{i})} Y_{i} - \\psi_{n}^{b}\\right)^{2}. \\end{align}\\] We investigate empirically the statistical behavior of \\(\\psi_{n}^{b}\\) in Section 6.3.3. 6.3.3 Empirical investigation The next chunk of code investigates the empirical behaviors of estimators \\(\\psi_{n}^{a}\\) and \\(\\psi_{n}^{b}\\). As explained in Section 5, we first make iter data sets out of the obs data set (second line), then build the estimators on each of them (fourth and fifth lines). After the first series of commands the object psi_hat_ab, a tibble, contains 200 rows and four columns. For each smaller data set (identified by its id), two rows contain the values of either \\(\\psi_{n}^{a}\\) and \\(\\sqrt{v_{n}^{a}}/\\sqrt{n}\\) (if type equals a) or \\(\\psi_{n}^{b}\\) and \\(\\sqrt{v_{n}^{b}}/\\sqrt{n}\\) (if type equals b). After the second series of commands, the object psi_hat_ab contains, in addition, the values of the recentered (with respect to \\(\\psi_{0}\\)) and renormalized \\(\\sqrt{n}/\\sqrt{v_{n}^{a}} (\\psi_{n}^{a} - \\psi_{0})\\) and \\(\\sqrt{n}/\\sqrt{v_{n}^{b}} (\\psi_{n}^{b} - \\psi_{0})\\), where \\(v_{n}^{a}\\) (6.2) and \\(v_{n}^{b}\\) (6.4) estimate the asymptotic variances of \\(\\psi_{n}^{a}\\) and \\(\\psi_{n}^{b}\\), respectively. Finally, bias_ab reports amounts of bias (at the renormalized scale). psi_hat_ab &lt;- obs %&gt;% as_tibble() %&gt;% mutate(id = (seq_len(n()) - 1) %% iter) %&gt;% nest(-id, .key = &quot;obs&quot;) %&gt;% mutate(est_a = map(obs, ~ compute_irrelevant_estimator(.)), est_b = map(obs, ~ compute_iptw(as.matrix(.), Gbar))) %&gt;% gather(`est_a`, `est_b`, key = &quot;type&quot;, value = &quot;estimates&quot;) %&gt;% extract(type, &quot;type&quot;, &quot;_([ab])$&quot;) %&gt;% unnest(estimates) %&gt;% select(-obs) (psi_hat_ab) #&gt; # A tibble: 200 x 4 #&gt; id type psi_n sig_n #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 a 0.130 0.0174 #&gt; 2 1 a 0.126 0.0180 #&gt; 3 2 a 0.112 0.0161 #&gt; 4 3 a 0.116 0.0164 #&gt; 5 4 a 0.110 0.0187 #&gt; 6 5 a 0.140 0.0178 #&gt; # ... with 194 more rows psi_hat_ab &lt;- psi_hat_ab %&gt;% group_by(id) %&gt;% mutate(clt = (psi_n - psi_zero) / sig_n) (psi_hat_ab) #&gt; # A tibble: 200 x 5 #&gt; # Groups: id [100] #&gt; id type psi_n sig_n clt #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 a 0.130 0.0174 2.71 #&gt; 2 1 a 0.126 0.0180 2.40 #&gt; 3 2 a 0.112 0.0161 1.78 #&gt; 4 3 a 0.116 0.0164 2.01 #&gt; 5 4 a 0.110 0.0187 1.42 #&gt; 6 5 a 0.140 0.0178 3.19 #&gt; # ... with 194 more rows (bias_ab &lt;- psi_hat_ab %&gt;% group_by(type) %&gt;% summarise(bias = mean(clt))) #&gt; # A tibble: 2 x 2 #&gt; type bias #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 a 1.53 #&gt; 2 b 0.0922 fig_bias_ab &lt;- ggplot() + geom_line(aes(x = x, y = y), data = tibble(x = seq(-3, 3, length.out = 1e3), y = dnorm(x)), linetype = 1, alpha = 0.5) + geom_density(aes(clt, fill = type, colour = type), psi_hat_ab, alpha = 0.1) + geom_vline(aes(xintercept = bias, colour = type), bias_ab, size = 1.5, alpha = 0.5) fig_bias_ab + labs(y = &quot;&quot;, x = expression(paste(sqrt(n/v[n]^{list(a, b)})* (psi[n]^{list(a, b)} - psi[0])))) Figure 6.1: Kernel density estimators of the law of two estimators of \\(\\psi_{0}\\) (recentered with respect to \\(\\psi_{0}\\), and renormalized), one of them misconceived (a), the other assuming that \\(\\Gbar_{0}\\) is known (b). Built based on iter independent realizations of each estimator. By the above chunk of code, the averages of \\(\\sqrt{n/v_{n}^{a}} (\\psi_{n}^{a} - \\psi_{0})\\) and \\(\\sqrt{n/v_{n}^{b}} (\\psi_{n}^{b} - \\psi_{0})\\) computed across the realizations of the two estimators are respectively equal to 1.526 and 0.092 (see bias_ab). Interpreted as amounts of bias, those two quantities are represented by vertical lines in Figure 6.1. The red and blue bell-shaped curves represent the empirical laws of \\(\\psi_{n}^{a}\\) and \\(\\psi_{n}^{b}\\) (recentered with respect to \\(\\psi_{0}\\), and renormalized) as estimated by kernel density estimation. The latter is close to the black curve, which represents the standard normal density. We could have used the alternative expression IPAW, where A (like action) is substituted for T (like treatment).↩ "],
["7-nuisance.html", "Section 7 Nuisance parameters 7.1 Anatomy of an expression 7.2 An algorithmic stance 7.3 QW 7.4 Gbar 7.5 ⚙ Qbar, working model-based algorithms 7.6 Qbar 7.7 ⚙ ☡ Qbar, machine learning-based algorithms", " Section 7 Nuisance parameters 7.1 Anatomy of an expression From now, all the inference strategies that we will present unfold in two or three stages. For all of them, the first stage consists in estimating a selection of features of the law \\(P_{0}\\) of the experiment. Specifically, the features are chosen among \\(Q_{0,W}\\) (the marginal law of \\(W\\) under \\(P_{0}\\)), \\(\\Gbar_{0}\\) (the conditional probability that \\(A=1\\) given \\(W\\) under \\(P_{0}\\)) and \\(\\Qbar_{0}\\) (the conditional mean of \\(Y\\) given \\(A\\) and \\(W\\) under \\(P_{0}\\)). In this context, because they are not the parameter of primary interest (i.e., they are not the real-values feature \\(\\Psi(P_{0})\\)), they are often referred to as nuisance parameters of \\(P_{0}\\). The unflaterring expression conveys the notion that their estimation is merely an intermediate step along our path towards an inference of the target parameter. As for the reason why \\(Q_{0,W}\\), \\(\\Gbar_{0}\\) and \\(\\Qbar_{0}\\) are singled out, it is because of their role in the definition of \\(\\Psi\\) and the efficient influence curve \\(D^{*}(P_{0})\\). 7.2 An algorithmic stance In general, we can view an estimator of any feature \\(f_0\\) of \\(P_{0}\\) as the output of an algorithm \\(\\Algo\\) that maps any element of \\[\\begin{equation*} \\calM^{\\text{empirical}} \\equiv \\left\\{\\frac{1}{m} \\sum_{i=1}^{m} \\Dirac(o_{i}) : m \\geq 1, o_{1}, \\ldots, o_{m} \\in [0,1] \\times \\{0,1\\} \\times [0,1]\\right\\} \\end{equation*}\\] to the set \\(\\calF\\) where \\(f_{0}\\) is know to live. Here, \\(\\calM^{\\text{empirical}}\\) can be interpreted as the set of all possible empirical measures summarizing the outcomes of any number of replications of the experiment \\(P_{0}\\). In particular, \\(P_{n}\\) belongs to this set. The tlrider package includes such template algorithms for the estimation of \\(Q_{0,W}\\), \\(\\Gbar_{0}\\) and \\(\\Qbar_{0}\\). We illustrate how they work and their use in the next sections. 7.3 QW For instance, estimate_QW is an algorithm \\(\\Algo_{Q_{W}}\\) for the estimation of the marginal law of \\(W\\) under \\(P_{0}\\) (to see its man page, simply run ?estimate_QW). It is a map from \\(\\calM^{\\text{empirical}}\\) to the set of laws on \\([0,1]\\). The following chunk of code estimates \\(Q_{0,W}\\) based on the \\(n = 1000\\) first observations in obs: QW_hat &lt;- estimate_QW(head(obs, 1e3)) It is easy to sample independent observations from QW_hat. To do so, we create an object of class LAW then set its marginal law of \\(W\\) to that described by QW_hat and specify its sample_from feature: empirical_experiment &lt;- LAW() alter(empirical_experiment, QW = QW_hat) alter(empirical_experiment, sample_from = function(n) { QW &lt;- get_feature(empirical_experiment, &quot;QW&quot;) W &lt;- sample(pull(QW, &quot;value&quot;), n, prob = pull(QW, &quot;weight&quot;)) cbind(W = W, A = NA, Y = NA) }) W &lt;- sample_from(empirical_experiment, 1e3) %&gt;% as.tibble W %&gt;% ggplot() + geom_histogram(aes(x = W, y = stat(density)), bins = 40) + stat_function(fun = get_feature(experiment, &quot;QW&quot;), col = &quot;red&quot;) Figure 7.1: Histogram representing 1000 observations drawn independently from QW_hat. The superimposed red curve is the true density of \\(Q_{0,W}\\). Note that all the \\(W\\)s sampled from QW_hat fall in the set \\(\\{W_{1}, \\ldots, W_{n}\\}\\) of observed \\(W\\)s in obs (an obvious fact given the definition of the sample_from feature of empirical_experiment: (length(intersect(pull(W, W), head(obs[, &quot;W&quot;], 1e3)))) #&gt; [1] 1000 This is because estimate_QW estimates \\(Q_{0,W}\\) with its empirical counterpart, i.e., \\[\\begin{equation*}\\frac{1}{n} \\sum_{i=1}^{n} \\Dirac(W_{i}).\\end{equation*}\\] 7.4 Gbar Another template algorithm is built-in into tlrider: estimate_Gbar (to see its man page, simply run ?estimate_Gbar). Unlike estimate_QW, estimate_Gbar needs further specification of the algorithm. The package also includes examples of such specifications. There are two sorts of specifications, of which we say that they are either working model-based or machine learning-based. We discuss the former sort in the next subsection. The latter sort is discussed in Section 7.6. 7.4.1 Working model-based algorithms Let us take a look at working_model_G_one for instance: working_model_G_one #&gt; $model #&gt; function (...) #&gt; { #&gt; trim_glm_fit(glm(family = binomial(), ...)) #&gt; } #&gt; &lt;environment: 0xdea07a8&gt; #&gt; #&gt; $formula #&gt; A ~ I(W^0.5) + I(abs(W - 5/12)^0.5) + I(W^1) + I(abs(W - 5/12)^1) + #&gt; I(W^1.5) + I(abs(W - 5/12)^1.5) #&gt; &lt;environment: 0xdea07a8&gt; #&gt; #&gt; $type_of_preds #&gt; [1] &quot;response&quot; #&gt; #&gt; attr(,&quot;ML&quot;) #&gt; [1] FALSE and focus on its model and formula attributes. The former relies on the glm and binomial functions from base R, and on trim_glm_fit (which removes information that we do not need from the standard output of glm, simply run ?trim_glm_fit to see the function’s man page). The latter is a formula that characterizes what we call a working model for \\(\\Gbar_{0}\\). In words, by using working_model_G_one we implicitly choose the so-called logistic (or negative binomial) loss function \\(L_{a}\\) given by \\[\\begin{equation} \\tag{7.1} -L_{a}(f)(A,W) \\equiv A \\log f(W) + (1 - A) \\log (1 - f(W)) \\end{equation}\\] for any function \\(f : [0,1] \\to [0,1]\\) paired with the working model \\[\\begin{equation*} \\calF_{1} \\equiv \\left\\{f_{\\theta} : \\theta \\in \\bbR^{5}\\right\\} \\end{equation*}\\] where, for any \\(\\theta \\in \\bbR^{5}\\), \\[\\begin{equation*}\\logit f_{\\theta} (W) \\equiv \\theta_{0} + \\sum_{j=1}^{4} \\theta_{j} W^{j/2}.\\end{equation*}\\] We acted as oracles when we specified the working model: it is well-specified, i.e., it happens that \\(\\Gbar_{0}\\) is the unique minimizer of the risk entailed by \\(L_{a}\\) over \\(\\calF_{1}\\): \\[\\begin{equation*}\\Gbar_{0} = \\mathop{\\arg\\min}_{f_{\\theta} \\in \\calF_{1}} \\Exp_{P_{0}} \\left(L_{a}(f_{\\theta})(A,W)\\right).\\end{equation*}\\] Therefore, the estimator \\(\\Gbar_{n}\\) obtained by minimizing the empirical risk \\[\\begin{equation*} \\Exp_{P_{n}} \\left(L_{a}(f_{\\theta})(A,W)\\right) = \\frac{1}{n} \\sum_{i=1}^{n} L_{a}(f_{\\theta})(A_{i},W_{i}) \\end{equation*}\\] over \\(\\calF_{1}\\) estimates \\(\\Gbar_{0}\\) consistently. Of course, it is seldom certain in real life that the target feature, here \\(\\Gbar_{0}\\), belongs to the working model.10 Suppose for instance that we choose a small finite-dimensional working model \\(\\calF_{2}\\) without acting as an oracle. Then consistency certainly fails to hold. However, if \\(\\Gbar_{0}\\) can nevertheless be projected unambiguously onto \\(\\calF_{2}\\) (an assumption that cannot be checked), then the estimator might converge to the projection. 7.4.2 Visualization To illustrate the use of the algorithm \\(\\Algo_{\\Gbar,1}\\) obtained by combining estimate_Gbar and working_model_G_one, let us estimate \\(\\Gbar_{0}\\) based on the first \\(n = 1000\\) observations in obs: Gbar_hat &lt;- estimate_Gbar(head(obs, 1e3), algorithm = working_model_G_one) Using compute_Gbar_hat_W11 (simply run ?compute_Gbar_hat_W to see its man page) makes it is easy to compare visually the estimator \\(\\Gbar_{n} \\equiv \\Algo_{\\Gbar,1}(P_{n})\\) with its target \\(\\Gbar0\\): tibble(w = seq(0, 1, length.out = 1e3)) %&gt;% mutate(&quot;truth&quot; = Gbar(w), &quot;estimated&quot; = compute_Gbar_hatW(w, Gbar_hat)) %&gt;% gather(&quot;f&quot;, &quot;value&quot;, -w) %&gt;% ggplot() + geom_line(aes(x = w, y = value, color = f), size = 1) + labs(y = &quot;f(w)&quot;, title = bquote(&quot;Visualizing&quot; ~ bar(G)[0] ~ &quot;and&quot; ~ hat(G)[n])) + ylim(NA, 1) Figure 7.2: Comparing \\(\\Gbar_{n}\\equiv \\Algo_{\\Gbar,1}(P_{n})\\) and \\(\\Gbar_{0}\\). The estimator is consistent because the algorithm relies on a working model that is correctly specified. 7.5 ⚙ Qbar, working model-based algorithms A third template algorithm is built-in into tlrider: estimate_Qbar (to see its man page, simply run ?estimate_Qbar). Like estimate_Gbar, estimate_Qbar needs further specification of the algorithm. The package also includes examples of such specifications, which can also be either working model-based (see Section 7.4) or machine learning-based (see Sections 7.6 and 7.7). There are built-in specifications similar to working_model_G_one, e.g., working_model_Q_one #&gt; $model #&gt; function (...) #&gt; { #&gt; trim_glm_fit(glm(family = binomial(), ...)) #&gt; } #&gt; &lt;environment: 0xdea07a8&gt; #&gt; #&gt; $formula #&gt; Y ~ A * (I(W^0.5) + I(W^1) + I(W^1.5)) #&gt; &lt;environment: 0xdea07a8&gt; #&gt; #&gt; $type_of_preds #&gt; [1] &quot;response&quot; #&gt; #&gt; attr(,&quot;ML&quot;) #&gt; [1] FALSE #&gt; attr(,&quot;stratify&quot;) #&gt; [1] FALSE Drawing inspiration from Section 7.4, comment upon and use the algorithm \\(\\Algo_{\\Qbar,1}\\) obtained by combining estimate_Gbar and working_model_Q_one. 7.6 Qbar 7.6.1 Qbar, machine learning-based algorithms We explained how algorithm \\(\\Algo_{\\Gbar,1}\\) is based on a working model (and you did for \\(\\Algo_{\\Qbar,1}\\)). It is not the case that all algorithms are based on working models in the same (admittedly rather narrow) sense. We propose to say that those algorithms that are not based on working models like \\(\\Algo_{\\Gbar,1}\\), for instance, are instead machine learning-based. Typically, machine learning-based algorithms are more data-adaptive; they rely on larger working models, and/or fine-tune parameters that must be calibrated, e.g. by cross-validation. Furthermore, they call for being stacked, i.e., combined by means of another outer algorithm (involving cross-validation) into a more powerful machine learning-based meta-algorithm. The super learning methodology is a popular stacking algorithm. We will elaborate further on this important topic in another forthcoming part. Here, we merely illustrate the concept with two specifications built-in into tlrider. Based on the \\(k\\)-nearest neighbors non-parametric estimating methodology, the first one is discussed in the next subsection. Based on boosted trees, another non-parametric estimating methodology, the second one is used in the exercize that follows the next subsection. 7.6.2 Qbar, kNN algorithm Algorithm \\(\\Algo_{\\Qbar,\\text{kNN}}\\) is obtained by combining estimate_Qbar and kknn_algo. The training of \\(\\Algo_{\\Qbar,\\text{kNN}}\\) (i.e., the making of the output \\(\\Algo_{\\Qbar,\\text{kNN}} (P_{n})\\) is implemented based on function caret::train of the caret (classification and regression training) package (to see its man page, simply run ?caret::train). Some additional specifications are provided in kknn_grid and kknn_control. In a nutshell, \\(\\Algo_{\\Qbar,\\text{kNN}}\\) estimates \\(\\Qbar_{0}(1,\\cdot)\\) and \\(\\Qbar_{0}(0,\\cdot)\\) separately. Each of them is estimated by applying the \\(k\\)-nearest neighbors methodology as it is implemented in function kknn::train.kknn from the kknn package (to see its man page, simply run ?kknn::train.kknn).12 The following chunk of code trains algorithm \\(\\Algo_{\\Qbar,\\text{kNN}}\\) on \\(P_{n}\\): Qbar_hat_kknn &lt;- estimate_Qbar(head(obs, 1e3), algorithm = kknn_algo, trControl = kknn_control, tuneGrid = kknn_grid) Using compute_Qbar_hat_AW (simply run ?compute_Qbar_hat_AW to see its man page) makes it is easy to compare visually the estimator \\(\\Qbar_{n,\\text{kNN}} \\equiv \\Algo_{\\Qbar,\\text{kNN}}(P_{n})\\) with its target \\(\\Qbar0\\), see Figure 7.3. fig &lt;- tibble(w = seq(0, 1, length.out = 1e3), truth_1 = Qbar(cbind(A = 1, W = w)), truth_0 = Qbar(cbind(A = 0, W = w)), kNN_1 = compute_Qbar_hatAW(1, w, Qbar_hat_kknn), kNN_0 = compute_Qbar_hatAW(0, w, Qbar_hat_kknn)) 7.6.3 Qbar, boosted trees algorithm Algorithm \\(\\Algo_{\\Qbar,\\text{trees}}\\) is obtained by combining estimate_Qbar and bstTree_algo. The training of \\(\\Algo_{\\Qbar,\\text{trees}}\\) (i.e., the making of the output \\(\\Algo_{\\Qbar,\\text{trees}} (P_{n})\\) is implemented based on function caret::train of the caret package. Some additional specifications are provided in bstTree_grid and bstTree_control. In a nutshell, \\(\\Algo_{\\Qbar,\\text{trees}}\\) estimates \\(\\Qbar_{0}(1,\\cdot)\\) and \\(\\Qbar_{0}(0,\\cdot)\\) separately. Each of them is estimated by boosted trees as implemented in function bst::bst from the bst (gradient boosting) package (to see its man page, simply run ?bst::bst).13 The following chunk of code trains algorithm \\(\\Algo_{\\Qbar,\\text{trees}}\\) on \\(P_{n}\\), and reveals what are the optimal fine-tune parameters for the estimation of \\(\\Qbar_{0}(1,\\cdot)\\) and \\(\\Qbar_{0}(0,\\cdot)\\): Qbar_hat_trees &lt;- estimate_Qbar(head(obs, 1e3), algorithm = bstTree_algo, trControl = bstTree_control, tuneGrid = bstTree_grid) Qbar_hat_trees %&gt;% dplyr::filter(a == &quot;one&quot;) %&gt;% pull(fit) %&gt;% capture.output %&gt;% tail(3) %&gt;% str_wrap(width = 60) %&gt;% cat #&gt; The final values used for the model were mstop = 20, #&gt; maxdepth = 1 and nu = 0.2. Qbar_hat_trees %&gt;% dplyr::filter(a == &quot;zero&quot;) %&gt;% pull(fit) %&gt;% capture.output %&gt;% tail(3) %&gt;% str_wrap(width = 60) %&gt;% cat #&gt; The final values used for the model were mstop = 30, #&gt; maxdepth = 1 and nu = 0.1. We can compare visually the estimators \\(\\Qbar_{n,\\text{kNN}}\\), \\(\\Qbar_{n,\\text{trees}} \\equiv \\Algo_{\\Qbar,\\text{trees}}(P_{n})\\) with its target \\(\\Qbar0\\), see Figure 7.3. In summary, \\(\\Qbar_{n,\\text{kNN}}\\) is rather good, though very versatile at the vincinity of the break points. As for \\(\\Qbar_{n,\\text{trees}}\\), it does not seem to capture the shape of its target. fig %&gt;% mutate(trees_1 = compute_Qbar_hatAW(1, w, Qbar_hat_trees), trees_0 = compute_Qbar_hatAW(0, w, Qbar_hat_trees)) %&gt;% gather(&quot;f&quot;, &quot;value&quot;, -w) %&gt;% extract(f, c(&quot;f&quot;, &quot;a&quot;), &quot;([^_]+)_([01]+)&quot;) %&gt;% mutate(a = paste0(&quot;a=&quot;, a)) %&gt;% ggplot + geom_line(aes(x = w, y = value, color = f), size = 1) + labs(y = &quot;f(w)&quot;, title = bquote(&quot;Visualizing&quot; ~ bar(Q)[0] ~ &quot;and its estimators&quot;)) + ylim(NA, 1) + facet_wrap(~ a) Figure 7.3: Comparing to their target two (machine learning-based) estimators of \\(\\Qbar_{0}\\), one based on the \\(k\\)-nearest neighbors and the other on boosted trees. 7.7 ⚙ ☡ Qbar, machine learning-based algorithms Using estimate_Q, make your own machine learning-based algorithm for the estimation of \\(\\Qbar_{0}\\). Train your algorithm on the same data set as \\(\\Algo_{\\Qbar,\\text{kNN}}\\) and \\(\\Algo_{\\Qbar,\\text{trees}}\\). If, like \\(\\Algo_{\\Qbar,\\text{trees}}\\), your algorithm includes a fine-tuning procedure, comment upon the optimal, data-driven specification. Plot your estimators of \\(\\Qbar_{0}(1,\\cdot)\\) and \\(\\Qbar_{0}(0,\\cdot)\\) on Figure 7.3. In fact, if one knows nothing about the feature, then it is certain that it does not belong to whichever small finite-dimensional working model we may come up with.↩ See also the companion function compute_lGbar_hat_AW (run ?compute_lGbar_hat_AW to see its man page.↩ Specifically, argument kmax (maximum number of neighbors considered) is set to 5, argument distance (parameter of the Minkowski distance) is set to 2, and argument kernel is set to gaussian. The best value of \\(k\\) is chosen between 1 and kmax by leave-one-out. No outer cross-validation is needed.↩ Specifically, argument mstop (number of boosting iterations for prediction) is one among 10, 20 and 30; argument nu (stepsize of the shrinkage parameter) is one among 0.1 and 0.2; argument maxdepth (maximum depth of the base learner, a tree) is one among 1, 2 and 5. An outer 10-fold cross-validation is carried out to select the best combination of fine-tune parameters.↩ "],
["8-two-naive-plug-in-inference-strategies.html", "Section 8 Two “naive” plug-in inference strategies 8.1 Why “naive”? 8.2 IPTW estimator 8.3 ⚙ Investigating further the IPTW inference strategy 8.4 G-computation estimator", " Section 8 Two “naive” plug-in inference strategies 8.1 Why “naive”? In this section, we present and discuss two plug-in strategies for the inference of \\(\\Psi(P_{0})\\). In light of Section 7.1, both unfold in two stages. During the first stage, some features among \\(Q_{0,W}\\), \\(\\Gbar_{0}\\) and \\(\\Qbar_{0}\\) (the \\(\\Psi\\)-specific nuisance parameters, see Section 7) are estimated. During the second stage, the estimators are substituted for their theoretical counterparts in the definition of \\(\\Psi(P_{0})\\), thus yielding estimators of \\(\\Psi(P_{0})\\). Although the strategies sound well conceived, a theoretical analysis reveals that they lack a third stage trying to correct an inherent flaw. They are thus said naive. The analysis and a first modus operandi are presented in Section ??. 8.2 IPTW estimator 8.2.1 Construction and computation In Section 6.3, we developed an IPTW substitution estimator, \\(\\psi_{n}^{b}\\), assuming that we knew \\(\\Gbar_{0}\\) beforehand. What if we did not? Obviously, we could estimate it and substitute the estimator of \\(\\ell\\Gbar_{0}\\) for \\(\\ell\\Gbar_{0}\\) in (6.3). Let \\(\\Algo_{\\Gbar}\\) be an algorithm designed for the estimation of \\(\\Gbar_{0}\\) (see Section 7.4). We denote by \\(\\Gbar_{n} \\equiv \\Algo_{\\Gbar}(P_{n})\\) the output of the algorithm trained on \\(P_{n}\\), and by \\(\\ell\\Gbar_{n}\\) the resulting (empirical) function given by \\[\\begin{equation*} \\ell\\Gbar_{n}(A,W) \\equiv A \\Gbar_{n}(W) + (1-A) (1 - \\Gbar_{n}(W)). \\end{equation*}\\] In light of (6.3), introduce \\[\\begin{equation*} \\psi_{n}^{c} \\equiv \\frac{1}{n} \\sum_{i=1}^{n} \\left(\\frac{2A_{i} - 1}{\\ell\\Gbar_{n}(A_{i}, W_{i})} Y_{i}\\right). \\end{equation*}\\] From a computational point of view, the tlrider package makes it easy to build \\(\\psi_{n}^{c}\\). Recall that compute_iptw(head(obs, 1e3), Gbar) implements the computation of \\(\\psi_{n}^{b}\\) based on the \\(n=1000\\) first observations stored in obs, using the true feature \\(\\Gbar_{0}\\) stored in Gbar, see Section 6.3.3 and the construction of psi_hat_ab. Similarly, Ghat &lt;- estimate_Gbar(head(obs, 1e3), working_model_G_one) compute_iptw(head(obs, 1e3), wrapper(Ghat)) %&gt;% pull(psi_n) #&gt; [1] 0.0707 implements (i) the estimation of \\(\\Gbar_{0}\\) with \\(\\Gbar_{n}\\)/Ghat using algorithm \\(\\Algo_{\\Gbar,1}\\) (first line) then (ii) the computation of \\(\\psi_{n}^{c}\\) (second line), both based on the same observations as above. Note how we use function wrapper (simply run ?wrapper to see its man page). 8.2.2 Elementary statistical properties Because \\(\\Gbar_{n}\\) minimizes the empirical risk over a finite-dimensional, identifiable, and well-specified working model, \\(\\sqrt{n} (\\psi_{n}^{c} - \\psi_{0})\\) converges in law to a centered Gaussian law. Moreover, the asymptotic variance of \\(\\sqrt{n} (\\psi_{n}^{c} - \\psi_{0})\\) is conservatively14 estimated with \\[\\begin{align*} v_{n}^{c} &amp;\\equiv \\Var_{P_{n}} \\left(\\frac{2A-1}{\\ell\\Gbar_{n}(A,W)}Y\\right) \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^{n}\\left(\\frac{2A_{i}-1}{\\ell\\Gbar_{n} (A_{i},W_{i})} Y_{i} - \\psi_{n}^{c}\\right)^{2}. \\end{align*}\\] We investigate empirically the statistical behavior of \\(\\psi_{n}^{c}\\) in Section 8.2.3. For an analysis of the reason why \\(v_{n}^{c}\\) is a conservative estimator of the asymptotic variance of \\(\\sqrt{n} (\\psi_{n}^{c} - \\psi_{0})\\), see here. Before proceeding, let us touch upon what would have happened if we had used a less amenable algorithm \\(\\Algo_{\\Gbar}\\). For instance, \\(\\Algo_{\\Gbar}\\) could still be well-specified15 but so versatile/complex (as opposed to being based on well-behaved, finite-dimensional parametric model) that the estimator \\(\\Gbar_{n}\\), though still consistent, would converge slowly to its target. Then, root-\\(n\\) consistency would fail to hold. Or \\(\\Algo_{\\Gbar}\\) could be mis-specified and there would be no guarantee at all that the resulting estimator \\(\\psi_{n}^{c}\\) be even consistent. 8.2.3 Empirical investigation Let us compute \\(\\psi_{n}^{c}\\) on the same iter = 100 independent samples of independent observations drawn from \\(P_{0}\\) as in Section 6.3. As explained in Sections 5 and 6.3.3, we first make iter data sets out of the obs data set (third line), then train algorithm \\(\\Algo_{\\Gbar,1}\\) on each of them (fifth to seventh lines). After the first series of commands the object learned_features_fixed_sample_size, a tibble, contains 100 rows and three columns. We created learned_features_fixed_sample_size to store the estimators of \\(\\Gbar_{0}\\) for future use. We will at a later stage enrich the object, for instance by adding to it estimators of \\(\\Qbar_{0}\\) obtained by training different algorithms on each smaller data set. In the second series of commands, the object psi_hat_abc is obtained by adding to psi_hat_ab (see Section 6.3.3) an 100 by four tibble containing notably the values of \\(\\psi_{n}^{c}\\) and \\(\\sqrt{v_{n}^{c}}/\\sqrt{n}\\) computed by calling compute_iptw. The object also contains the values of the recentered (with respect to \\(\\psi_{0}\\)) and renormalized \\(\\sqrt{n}/\\sqrt{v_{n}^{c}} (\\psi_{n}^{c} - \\psi_{0})\\). Finally, bias_abc reports amounts of bias (at the renormalized scale). learned_features_fixed_sample_size &lt;- obs %&gt;% as_tibble() %&gt;% mutate(id = (seq_len(n()) - 1) %% iter) %&gt;% nest(-id, .key = &quot;obs&quot;) %&gt;% mutate(Gbar_hat = map(obs, ~ estimate_Gbar(., algorithm = working_model_G_one))) psi_hat_abc &lt;- learned_features_fixed_sample_size %&gt;% mutate(est_c = map2(obs, Gbar_hat, ~ compute_iptw(as.matrix(.x), wrapper(.y, FALSE)))) %&gt;% unnest(est_c) %&gt;% select(-Gbar_hat, -obs) %&gt;% mutate(clt = (psi_n - psi_zero) / sig_n, type = &quot;c&quot;) %&gt;% full_join(psi_hat_ab) (bias_abc &lt;- psi_hat_abc %&gt;% group_by(type) %&gt;% summarise(bias = mean(clt))) #&gt; # A tibble: 3 x 2 #&gt; type bias #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 a 1.53 #&gt; 2 b 0.0922 #&gt; 3 c 0.0254 By the above chunk of code, the average of \\(\\sqrt{n/v_{n}^{c}} (\\psi_{n}^{c} - \\psi_{0})\\) computed across the realizations is equal to 0.025 (see bias_abc). In words, the average bias of \\(\\psi_{n}^{c}\\) is of the same magnitude as that of \\(\\psi_{n}^{b}\\) despite the fact that the construction of \\(\\psi_{n}^{c}\\) hinges on the estimation of \\(\\Gbar_{0}\\) (based on the well-specified algorithm \\(\\Algo_{\\Gbar,1}\\)). We represent the empirical laws of the recentered (with respect to \\(\\psi_{0}\\)) and renormalized \\(\\psi_{n}^{a}\\), \\(\\psi_{n}^{b}\\) and \\(\\psi_{n}^{c}\\) in Figures 8.1 (kernel density estimators) and 8.2 (quantile-quantile plots). fig_bias_ab + geom_density(aes(clt, fill = type, colour = type), psi_hat_abc, alpha = 0.1) + geom_vline(aes(xintercept = bias, colour = type), bias_abc, size = 1.5, alpha = 0.5) + xlim(-3, 4) + labs(y = &quot;&quot;, x = expression(paste(sqrt(n/v[n]^{list(a, b, c)})* (psi[n]^{list(a, b, c)} - psi[0])))) Figure 8.1: Kernel density estimators of the law of three estimators of \\(\\psi_{0}\\) (recentered with respect to \\(\\psi_{0}\\), and renormalized), one of them misconceived (a), one assuming that \\(\\Gbar_{0}\\) is known (b) and one that hinges on the estimation of \\(\\Gbar_{0}\\) (c). The present figure includes Figure 6.1 (but the colors differ). Built based on iter independent realizations of each estimator. ggplot(psi_hat_abc, aes(sample = clt, fill = type, colour = type)) + geom_abline(intercept = 0, slope = 1, alpha = 0.5) + geom_qq(alpha = 1) Figure 8.2: Quantile-quantile plot of the standard normal law against the empirical laws of three estimators of \\(\\psi_{0}\\), one of them misconceived (a), one assuming that \\(\\Gbar_{0}\\) is known (b) and one that hinges on the estimation of \\(\\Gbar_{0}\\) (c). Built based on iter independent realizations of each estimator. Figures 8.1 and 8.2 confirm that \\(\\psi_{n}^{c}\\) behaves as well as \\(\\psi_{n}^{b}\\) in terms of bias — but remember that we acted as oracles when we chose the well-specified algorithm \\(\\Algo_{\\Gbar,1}\\). They also corroborate that \\(v_{n}^{c}\\), the estimator of the asymptotic variance of \\(\\sqrt{n} (\\psi_{n}^{c} - \\psi_{0})\\), is conservative: for instance, the corresponding bell-shaped blue curve is too much concentrated around its axis of symmetry. The actual asymptotic variance of \\(\\sqrt{n} (\\psi_{n}^{c} - \\psi_{0})\\) can be estimated with the empirical variance of the iter replications of the construction of \\(\\psi_{n}^{c}\\). (emp_sig_n &lt;- psi_hat_abc %&gt;% filter(type == &quot;c&quot;) %&gt;% summarize(sd(psi_n)) %&gt;% pull) #&gt; [1] 0.0191 (summ_sig_n &lt;- psi_hat_abc %&gt;% filter(type == &quot;c&quot;) %&gt;% select(sig_n) %&gt;% summary) #&gt; sig_n #&gt; Min. :0.0531 #&gt; 1st Qu.:0.0550 #&gt; Median :0.0557 #&gt; Mean :0.0561 #&gt; 3rd Qu.:0.0570 #&gt; Max. :0.0616 The empirical standard deviation is approximately 2.939 times smaller than the average estimated standard deviation. The estimator is conservative indeed! Furthermore, note the better fit with the density of the standard normal density of the kernel density estimator of the law of \\(\\sqrt{n} (\\psi_{n}^{c} - \\psi_{0})\\) renormalized with emp_sig_n. clt_c &lt;- psi_hat_abc %&gt;% filter(type == &quot;c&quot;) %&gt;% mutate(clt = clt * sig_n / emp_sig_n) fig_bias_ab + geom_density(aes(clt, fill = type, colour = type), clt_c, alpha = 0.1) + geom_vline(aes(xintercept = bias, colour = type), bias_abc, size = 1.5, alpha = 0.5) + xlim(-3, 4) + labs(y = &quot;&quot;, x = expression(paste(sqrt(n/v[n]^{list(a, b, c)})* (psi[n]^{list(a, b, c)} - psi[0])))) Figure 8.3: Kernel density estimators of the law of three estimators of \\(\\psi_{0}\\) (recentered with respect to \\(\\psi_{0}\\), and renormalized), one of them misconceived (a), one assuming that \\(\\Gbar_{0}\\) is known (b) and one that hinges on the estimation of \\(\\Gbar_{0}\\) and an estimator of the asymptotic variance computed across the replications (c). The present figure includes Figure 6.1 (but the colors differ) and it should be compared to Figure 8.2. Built based on iter independent realizations of each estimator. Workaround. In a real world data-analysis, one could correct the estimation of the asymptotic variance of \\(\\sqrt{n} (\\psi_{n}^{c} - \\psi_{0})\\). We could for instance derive the influence function as it is described here and use the corresponding influence function-based estimator of the variance. Or one could rely on the bootstrap.16 This, however, would only make sense if one knew for sure that the algorithm for the estimation of \\(\\Gbar_{0}\\) is well-specified. 8.3 ⚙ Investigating further the IPTW inference strategy Building upon the chunks of code devoted to the repeated computation of \\(\\psi_{n}^{b}\\) and its companion quantities, construct confidence intervals for \\(\\psi_{0}\\) of (asymptotic) level \\(95\\%\\), and check if the empirical coverage is satisfactory. Note that if the coverage was exactly \\(95\\%\\), then the number of confidence intervals that would contain \\(\\psi_{0}\\) would follow a binomial law with parameters iter and 0.95, and recall that function binom.test performs an exact test of a simple null hypothesis about the probability of success in a Bernoulli experiment against its three one-sided and two-sided alternatives. Discuss what happens when the dimension of the (still well-specified) working model grows. Start with the built-in working model working_model_G_two. The following chunk of code re-defines working_model_G_two ## make sure &#39;1/2&#39; and &#39;1&#39; belong to &#39;powers&#39; powers &lt;- rep(seq(1/4, 3, by = 1/4), each = 2) working_model_G_two &lt;- list( model = function(...) {trim_glm_fit(glm(family = binomial(), ...))}, formula = stats::as.formula( paste(&quot;A ~&quot;, paste(c(&quot;I(W^&quot;, &quot;I(abs(W - 5/12)^&quot;), powers, sep = &quot;&quot;, collapse = &quot;) + &quot;), &quot;)&quot;) ), type_of_preds = &quot;response&quot; ) attr(working_model_G_two, &quot;ML&quot;) &lt;- FALSE Play around with argument powers (making sure that 1/2 and 1 belong to it), and plot graphics similar to those presented in Figures 8.1 and 8.2. Discuss what happens when the working model is mis-specified. You could use the built-in working model working_model_G_three. Repeat the analysis developed in response to problem 1 above but for \\(\\psi_{n}^{c}\\). What can you say about the coverage of the confidence intervals? ☡ (Follow-up to problem 5). Implement the bootstrap procedure evoked at the end of Section 8.2.3. Repeat the analysis developed in response to problem 1. Compare your results with those to problem 5. ☡ Is it legitimate to infer the asymptotic variance of \\(\\psi_{n}^{c}\\) with \\(v_{n}^{c}\\) when one relies on a very data-adaptive/versatile algorithm to estimate \\(\\Gbar_{0}\\)? 8.4 G-computation estimator 8.4.1 Construction and computation Let \\(\\Algo_{Q_{W}}\\) be an algorithm designed for the estimation of \\(Q_{0,W}\\) (see Section 7.3). We denote by \\(Q_{n,W} \\equiv \\Algo_{Q_{W}}(P_{n})\\) the output of the algorithm trained on \\(P_{n}\\). Let \\(\\Algo_{\\Qbar}\\) be an algorithm designed for the estimation of \\(\\Qbar_{0}\\) (see Section 7.6). We denote by \\(\\Qbar_{n} \\equiv \\Algo_{\\Qbar}(P_{n})\\) the output of the algorithm trained on \\(P_{n}\\). Equation (2.1) suggests the following, simple estimator of \\(\\Psi(P_0)\\): \\[\\begin{equation} \\psi_{n} \\equiv \\int \\left(\\Qbar_{n}(1, w) - \\Qbar_{n}(0,w)\\right) dQ_{n,W}(w). \\tag{8.1} \\end{equation}\\] In words, this estimator is implemented by first regressing \\(Y\\) on \\((A,W)\\), then by marginalizing with respect to the estimated law of \\(W\\). The resulting estimator is referred to as a G-computation (or standardization) estimator. From a computational point of view, the tlrider package makes it easy to build the G-computation estimator. Recall that we have already estimated the marginal law \\(Q_{0,W}\\) of \\(W\\) under \\(P_{0}\\) by training the algorithm \\(\\Algo_{Q_{W}}\\) as it is implemented in estimate_QW on the \\(n = 1000\\) first observations in obs (see Section 7.3): QW_hat &lt;- estimate_QW(head(obs, 1e3)) Recall that \\(\\Algo_{\\Qbar,1}\\) is the algorithm for the estimation of \\(\\Qbar_{0}\\) as it is implemented in estimate_Qbar with its argument algorithm set to the built-in working_model_Q_one (see Section 7.5). Recall also that \\(\\Algo_{\\Qbar,\\text{kNN}}\\) is the algorithm for the estimation of \\(\\Qbar_{0}\\) as it is implemented in estimate_Qbar with its argument algorithm set to the built-in kknn_algo (see Section 7.6.2). We have already trained the latter on the \\(n=1000\\) first observations in obs. Let us train the former on the same data set: Qbar_hat_kknn &lt;- estimate_Qbar(head(obs, 1e3), algorithm = kknn_algo, trControl = kknn_control, tuneGrid = kknn_grid) Qbar_hat_d &lt;- estimate_Qbar(head(obs, 1e3), working_model_Q_one) With these estimators handy, computing the G-computation estimator is as simple as running the following chunk of code: (compute_gcomp(QW_hat, wrapper(Qbar_hat_kknn, FALSE), 1e3)) #&gt; # A tibble: 1 x 2 #&gt; psi_n sig_n #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.0722 0.00487 (compute_gcomp(QW_hat, wrapper(Qbar_hat_d, FALSE), 1e3)) #&gt; # A tibble: 1 x 2 #&gt; psi_n sig_n #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.0742 0.00215 Note how we use function wrapper again, and that it is necessary to provide the number of observations upon which the estimation of the \\(Q_{W}\\) and \\(\\Qbar\\) features of \\(P_{0}\\). 8.4.2 Elementary statistical properties This subsection is very similar to its counterpart for the IPTW estimator, see Section 8.2.2. Let us denote by \\(\\Qbar_{n,1}\\) the output of algorithm \\(\\Algo_{\\Qbar,1}\\) trained on \\(P_{n}\\), and recall that \\(\\Qbar_{n,\\text{kNN}}\\) is the output of algorithm \\(\\Algo_{\\Qbar,\\text{kNN}}\\) trained on \\(P_{n}\\). Let \\(\\psi_{n}^{d}\\) and \\(\\psi_{n}^{e}\\) be the G-computation estimators obtained by substituting \\(\\Qbar_{n,1}\\) and \\(\\Qbar_{n,\\text{kNN}}\\) for \\(\\Qbar_{n}\\) in (8.1), respectively. If \\(\\Qbar_{n,\\bullet}\\) minimized the empirical risk over a finite-dimensional, identifiable, and well-specified working model, then \\(\\sqrt{n} (\\psi_{n}^{\\bullet} - \\psi_{0})\\) would converge in law to a centered Gaussian law (here \\(\\psi_{n}^{\\bullet}\\) represents the G-computation estimator obtained by substituting \\(\\Qbar_{n,\\bullet}\\) for \\(\\Qbar_{n}\\) in (8.1)). Moreover, the asymptotic variance of \\(\\sqrt{n} (\\psi_{n}^{\\bullet} - \\psi_{0})\\) would be estimated anti-conservatively17 with \\[\\begin{align} v_{n}^{d} &amp;\\equiv \\Var_{P_{n}} \\left(\\Qbar_{n,1}(1,\\cdot) - \\Qbar_{n,1}(0,\\cdot)\\right) \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^{n}\\left(\\Qbar_{n,1}(1,W_{i}) - \\Qbar_{n,1}(0,W_{i}) -\\psi_{n}^{d}\\right)^{2}. \\tag{8.2} \\end{align}\\] Unfortunately, algorithm \\(\\Algo_{\\Qbar,1}\\) is mis-specified and \\(\\Algo_{\\Qbar,\\text{kNN}}\\) is not based on a finite-dimensional working model. Nevertheless, function compute_gcomp estimates (in general, very poorly) the asymptotic variance with (8.2). We investigate empirically the statistical behavior of \\(\\psi_{n}^{d}\\) in Section 8.4.3. For an analysis of the reason why \\(v_{n}^{d}\\) is an anti-conservative estimator of the asymptotic variance of \\(\\sqrt{n} (\\psi_{n}^{d} - \\psi_{0})\\), see here. We wish to emphasize that anti-conservativeness is even more embarrassing that conservativeness (both being contingent on the fact that the algorithms are well-specified, fact that cannot be true in the present case in real world situations). What would happen if we used a less amenable algorithm \\(\\Algo_{\\Qbar}\\). For instance, \\(\\Algo_{\\Qbar}\\) could still be well-specified but so versatile/complex (as opposed to being based on well-behaved, finite-dimensional parametric model) that the estimator \\(\\Qbar_{n}\\), though still consistent, would converge slowly to its target. Then, root-\\(n\\) consistency would fail to hold. We can explore empirically this situation with estimator \\(\\psi_{n}^{e}\\) that hinges on algorithm \\(\\Algo_{\\Qbar,\\text{kNN}}\\). Or \\(\\Algo_{\\Qbar}\\) could be mis-specified and there would be no guarantee at all that the resulting estimator \\(\\psi_{n}\\) be even consistent. 8.4.3 Empirical investigation Fixed sample size Let us compute \\(\\psi_{n}^{d}\\) and \\(\\psi_{n}^{e}\\) on the same iter = 100 independent samples of independent observations drawn from \\(P_{0}\\) as in Sections 6.3 and 8.2.3. We first enrich object learned_features_fixed_sample_size that was created in Section 8.2.3, adding to it estimators of \\(\\Qbar_{0}\\) obtained by training algorithms \\(\\Algo_{\\Qbar,1}\\) and \\(\\Algo_{\\Qbar,\\text{kNN}}\\) on each smaller data set. The second series of commands creates object psi_hat_de, an 100 by six tibble containing notably the values of \\(\\psi_{n}^{d}\\) and \\(\\sqrt{v_{n}^{d}}/\\sqrt{n}\\) computed by calling compute_gcomp, and those of the recentered (with respect to \\(\\psi_{0}\\)) and renormalized \\(\\sqrt{n}/\\sqrt{v_{n}^{d}} (\\psi_{n}^{d} - \\psi_{0})\\). Because we know beforehand that \\(v_{n}^{d}\\) under-estimates the actual asymptotic variance of \\(\\sqrt{n} (\\psi_{n}^{d} - \\psi_{0})\\), the tibble also includes the values of \\(\\sqrt{n}/\\sqrt{v^{d*}} (\\psi_{n}^{d} - \\psi_{0})\\) where the estimator \\(v^{d*}\\) of the asymptotic variance is computed across the replications of \\(\\psi_{n}^{d}\\). The tibble includes the same quantities pertaining to \\(\\psi_{n}^{e}\\), although there is no theoretical guarantee that the central limit theorem does hold and, even if it did, that the counterpart \\(v_{n}^{e}\\) to \\(v_{n}^{d}\\) estimates in any way the asymptotic variance of \\(\\sqrt{n} (\\psi_{n}^{e} - \\psi_{0})\\). Finally, bias_de reports amounts of bias (at the renormalized scales — plural). There is one value of bias for each combination of (i) type of the estimator (d or e) and (ii) how the renormalization is carried out, either based on \\(v_{n}^{d}\\) and \\(v_{n}^{e}\\) (auto_renormalization is TRUE) or on the estimator of the asymptotic variance computed across the replications of \\(\\psi_{n}^{d}\\) and \\(\\psi_{n}^{e}\\) (auto_renormalization is FALSE). learned_features_fixed_sample_size &lt;- learned_features_fixed_sample_size %&gt;% mutate(Qbar_hat_d = map(obs, ~ estimate_Qbar(., algorithm = working_model_Q_one)), Qbar_hat_e = map(obs, ~ estimate_Qbar(., algorithm = kknn_algo, trControl = kknn_control, tuneGrid = kknn_grid))) %&gt;% mutate(QW = map(obs, estimate_QW), est_d = pmap(list(QW, Qbar_hat_d, n()), ~ compute_gcomp(..1, wrapper(..2, FALSE), ..3)), est_e = pmap(list(QW, Qbar_hat_e, n()), ~ compute_gcomp(..1, wrapper(..2, FALSE), ..3))) psi_hat_de &lt;- learned_features_fixed_sample_size %&gt;% select(est_d, est_e) %&gt;% gather(`est_d`, `est_e`, key = &quot;type&quot;, value = &quot;estimates&quot;) %&gt;% extract(type, &quot;type&quot;, &quot;_([de])$&quot;) %&gt;% unnest(estimates) %&gt;% group_by(type) %&gt;% mutate(sig_alt = sd(psi_n)) %&gt;% mutate(clt_ = (psi_n - psi_zero) / sig_n, clt_alt = (psi_n - psi_zero) / sig_alt) %&gt;% gather(`clt_`, `clt_alt`, key = &quot;key&quot;, value = &quot;clt&quot;) %&gt;% extract(key, &quot;key&quot;, &quot;_(.*)$&quot;) %&gt;% mutate(key = ifelse(key == &quot;&quot;, TRUE, FALSE)) %&gt;% rename(&quot;auto_renormalization&quot; = key) (bias_de &lt;- psi_hat_de %&gt;% group_by(type, auto_renormalization) %&gt;% summarize(bias = mean(clt))) #&gt; # A tibble: 4 x 3 #&gt; # Groups: type [?] #&gt; type auto_renormalization bias #&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; #&gt; 1 d FALSE 0.356 #&gt; 2 d TRUE 1.08 #&gt; 3 e FALSE 0.130 #&gt; 4 e TRUE 0.150 fig &lt;- ggplot() + geom_line(aes(x = x, y = y), data = tibble(x = seq(-4, 4, length.out = 1e3), y = dnorm(x)), linetype = 1, alpha = 0.5) + geom_density(aes(clt, fill = type, colour = type), psi_hat_de, alpha = 0.1) + geom_vline(aes(xintercept = bias, colour = type), bias_de, size = 1.5, alpha = 0.5) + facet_wrap(~ auto_renormalization, labeller = as_labeller(c(`TRUE` = &quot;auto-renormalization: TRUE&quot;, `FALSE` = &quot;auto-renormalization: FALSE&quot;)), scales = &quot;free&quot;) fig + labs(y = &quot;&quot;, x = expression(paste(sqrt(n/v[n]^{list(d, e)})* (psi[n]^{list(d, e)} - psi[0])))) Figure 8.4: Kernel density estimators of the law of two estimators of \\(\\psi_{0}\\) (recentered with respect to \\(\\psi_{0}\\), and renormalized). The estimators respectively hinge on algorithms \\(\\Algo_{\\Qbar,1}\\) (d) and \\(\\Algo_{\\Qbar,\\text{kNN}}\\) (e) to estimate \\(\\Qbar_{0}\\). Two renormalization schemes are considered, either based on an estimator of the asymptotic variance (left) or on the empirical variance computed across the iter independent replications of the estimators (right). We emphasize that the \\(x\\)-axis ranges differ starkly between the left and right plots. We represent the empirical laws of the recentered (with respect to \\(\\psi_{0}\\)) and renormalized \\(\\psi_{n}^{d}\\) and \\(\\psi_{n}^{e}\\) in Figure 8.4 (kernel density estimators). Two renormalization schemes are considered, either based on an estimator of the asymptotic variance (left) or on the empirical variance computed across the iter independent replications of the estimators (right). We emphasize that the \\(x\\)-axis ranges differ starkly between the left and right plots. Two important comments are in order. First, on the one hand, the G-computation estimator \\(\\psi_{n}^{d}\\) is biased. Specifically, by the above chunk of code, the averages of \\(\\sqrt{n/v_{n}^{d}} (\\psi_{n}^{d} - \\psi_{0})\\) and \\(\\sqrt{n/v_{n}^{d*}} (\\psi_{n}^{d} - \\psi_{0})\\) computed across the realizations are equal to 1.076 and 0.356 (see bias_de). On the other hand, the G-computation estimator \\(\\psi_{n}^{e}\\) is biased too, though slightly less than \\(\\psi_{n}^{d}\\). Specifically, by the above chunk of code, the averages of \\(\\sqrt{n/v_{n}^{e}} (\\psi_{n}^{e} - \\psi_{0})\\) and \\(\\sqrt{n/v^{e*}} (\\psi_{n}^{e} - \\psi_{0})\\) computed across the realizations are equal to 0.15 and 0.13 (see bias_de). We can provide an oracular explanation. Estimator \\(\\psi_{n}^{d}\\) suffers from the poor approximation of \\(\\Qbar_{0}\\) by \\(\\Algo_{\\Qbar,1}(P_{n})\\), a result of the algorithm’s mis-specification. As for \\(\\psi_{n}^{e}\\), it behaves better because \\(\\Algo_{\\Qbar,\\text{kNN}} (P_{n})\\) approximates \\(\\Qbar_{0}\\) better than \\(\\Algo_{\\Qbar,1}(P_{n})\\), an apparent consequence of the greater versatility of the algorithm. Second, we get a visual confirmation that \\(v_{n}^{d}\\) under-estimates the actual asymptotic variance of \\(\\sqrt{n} (\\psi_{n}^{d} - \\psi_{0})\\): the right-hand side red bell-shaped curve is too dispersed. In contrast, the right-hand side blue bell-shaped curve is much closer to the black curve that represents the density of the standard normal law. Looking at the left-hand side plot reveals that the empirical law of \\(\\sqrt{n/v^{d*}} (\\psi_{n}^{d} - \\psi_{0})\\), once translated to compensate for the bias, is rather close to the black curve. This means that the random variable is approximately distributed like a Gaussian random variable. On the contrary, the empirical law of \\(\\sqrt{n/v^{e*}} (\\psi_{n}^{e} - \\psi_{0})\\) does not strike us as being as closely Gaussian-like as that of \\(\\sqrt{n/v^{d*}} (\\psi_{n}^{d} - \\psi_{0})\\). By being more data-adaptive than \\(\\Algo_{\\Qbar,1}\\), algorithm \\(\\Algo_{\\Qbar,\\text{kNN}}\\) yields a better estimator of \\(\\Qbar_{0}\\). However, the rate of convergence of \\(\\Algo_{\\Qbar,\\text{kNN}}(P_{n})\\) to its limit may be slower than root-\\(n\\), invalidating a central limit theorem. How do the estimated variances of \\(\\psi_{n}^{d}\\) and \\(\\psi_{n}^{e}\\) compare with their empirical counterparts (computed across the iter replications of the construction of the two estimators)? ## psi_n^d (psi_hat_de %&gt;% ungroup %&gt;% filter(type == &quot;d&quot; &amp; auto_renormalization) %&gt;% pull(sig_n) %&gt;% summary) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 0.00269 0.00539 0.00641 0.00648 0.00753 0.01012 ## psi_n^e (psi_hat_de %&gt;% ungroup %&gt;% filter(type == &quot;e&quot; &amp; auto_renormalization) %&gt;% pull(sig_n) %&gt;% summary) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 0.0133 0.0149 0.0156 0.0157 0.0163 0.0188 The empirical standard deviation of \\(\\psi_{n}^{d}\\) is approximately 2.933 times larger than the average estimated standard deviation. The estimator is anti-conservative indeed! As for the empirical standard deviation of \\(\\psi_{n}^{e}\\), it is approximately 1.214 times larger than the average estimated standard deviation. Varying sample size ## ## not updated yet ## sample_size &lt;- c(2e3, 3e3) block_size &lt;- sum(sample_size) learned_features_varying_sample_size &lt;- obs %&gt;% as.tibble %&gt;% head(n = (nrow(.) %/% block_size) * block_size) %&gt;% mutate(block = label(1:nrow(.), sample_size)) %&gt;% nest(-block, .key = &quot;obs&quot;) First, we cut the data set into independent sub-data sets of sample size \\(n\\) in \\(\\{\\) 2000, 3000 \\(\\}\\). Second, we infer \\(\\psi_{0}\\) as shown two chunks earlier. We thus obtain 20 independent realizations of each estimator derived on data sets of 2, increasing sample sizes. learned_features_varying_sample_size &lt;- learned_features_varying_sample_size %&gt;% mutate(Qbar_hat_d = map(obs, ~ estimate_Qbar(., algorithm = working_model_Q_one)), Qbar_hat_e = map(obs, ~ estimate_Qbar(., algorithm = kknn_algo, trControl = kknn_control, tuneGrid = kknn_grid))) %&gt;% mutate(QW = map(obs, estimate_QW), est_d = pmap(list(QW, Qbar_hat_d, n()), ~ compute_gcomp(..1, wrapper(..2, FALSE), ..3)), est_e = pmap(list(QW, Qbar_hat_e, n()), ~ compute_gcomp(..1, wrapper(..2, FALSE), ..3))) root_n_bias &lt;- learned_features_varying_sample_size %&gt;% mutate(block = unlist(map(strsplit(block, &quot;_&quot;), ~.x[2])), sample_size = sample_size[as.integer(block)]) %&gt;% select(block, sample_size, est_d, est_e) %&gt;% gather(`est_d`, `est_e`, key = &quot;type&quot;, value = &quot;estimates&quot;) %&gt;% extract(type, &quot;type&quot;, &quot;_([de])$&quot;) %&gt;% unnest(estimates) %&gt;% group_by(block, type) %&gt;% mutate(sig_alt = sd(psi_n)) %&gt;% mutate(clt_ = (psi_n - psi_zero) / sig_n, clt_alt = (psi_n - psi_zero) / sig_alt) %&gt;% gather(`clt_`, `clt_alt`, key = &quot;key&quot;, value = &quot;clt&quot;) %&gt;% extract(key, &quot;key&quot;, &quot;_(.*)$&quot;) %&gt;% mutate(key = ifelse(key == &quot;&quot;, TRUE, FALSE)) %&gt;% rename(&quot;auto_renormalization&quot; = key) The tibble called root_n_bias reports root-\\(n\\) times bias for all combinations of estimator and sample size. The next chunk of code presents visually our findings, see Figure 8.5. Note how we include the realizations of the estimators derived earlier and contained in psi_hat_de (thus breaking the independence between components of root_n_bias, a small price to pay in this context). root_n_bias &lt;- learned_features_fixed_sample_size %&gt;% mutate(block = &quot;0&quot;, sample_size = B/iter) %&gt;% # because *fixed* sample size select(block, sample_size, est_d, est_e) %&gt;% gather(`est_d`, `est_e`, key = &quot;type&quot;, value = &quot;estimates&quot;) %&gt;% extract(type, &quot;type&quot;, &quot;_([de])$&quot;) %&gt;% unnest(estimates) %&gt;% group_by(block, type) %&gt;% mutate(sig_alt = sd(psi_n)) %&gt;% mutate(clt_ = (psi_n - psi_zero) / sig_n, clt_alt = (psi_n - psi_zero) / sig_alt) %&gt;% gather(`clt_`, `clt_alt`, key = &quot;key&quot;, value = &quot;clt&quot;) %&gt;% extract(key, &quot;key&quot;, &quot;_(.*)$&quot;) %&gt;% mutate(key = ifelse(key == &quot;&quot;, TRUE, FALSE)) %&gt;% rename(&quot;auto_renormalization&quot; = key) %&gt;% full_join(root_n_bias) root_n_bias %&gt;% ggplot() + stat_summary(aes(x = sample_size, y = clt, group = interaction(sample_size, type), color = type), fun.data = mean_se, fun.args = list(mult = 2), position = position_dodge(width = 250), cex = 1) + stat_summary(aes(x = sample_size, y = clt, group = interaction(sample_size, type), color = type), fun.data = mean_se, fun.args = list(mult = 2), position = position_dodge(width = 250), cex = 1, geom = &quot;errorbar&quot;, width = 750) + stat_summary(aes(x = sample_size, y = clt, color = type), fun.y = mean, position = position_dodge(width = 250), geom = &quot;polygon&quot;, fill = NA) + geom_point(aes(x = sample_size, y = clt, group = interaction(sample_size, type), color = type), position = position_dodge(width = 250), alpha = 0.1) + scale_x_continuous(breaks = unique(c(B / iter, sample_size))) + labs(x = &quot;sample size n&quot;, y = expression(paste(sqrt(n) * (psi[n]^{list(d, e)} - psi[0])))) + facet_wrap(~ auto_renormalization, labeller = as_labeller(c(`TRUE` = &quot;auto-renormalization: TRUE&quot;, `FALSE` = &quot;auto-renormalization: FALSE&quot;)), scales = &quot;free&quot;) Figure 8.5: Evolution of root-\\(n\\) times bias versus sample size for two inference methodology of \\(\\psi_{0}\\) based on the estimation of \\(\\Qbar_{0}\\). Big dots represent the average biases and vertical lines represent twice the standard error. In words, \\(v_{n}^{c}\\) converges to an upper-bound of the true asymptotic variance.↩ Well-specified e.g. in the sense that the target \\(\\Gbar_{0}\\) of \\(\\Algo_{\\Gbar}\\) belongs to the closure of the algorithm’s image \\(\\Algo_{\\Gbar}(\\calM^{\\text{empirical}})\\) or, in other words, can be approximated arbitrarily well by an output of the algorithm.↩ That is, replicate the construction of \\(\\psi_{n}^{c}\\) many times based on data sets obtained by resampling from the original data set, then estimate the asymptotic variance with the empirical variance of \\(\\psi_{n}^{c}\\) computed across the replications.↩ In words, \\(v_{n}^{d}\\) converges to a lower-bound of the true asymptotic variance.↩ "],
["9-work-in-progress.html", "Section 9 Work in progress", " Section 9 Work in progress (…) "],
["10-notation.html", "Section 10 Notation", " Section 10 Notation \\(O = (W,A,Y)\\), the generic summary of how one realization of the experiments of interest unfold, our generic observation; \\(W \\in [0,1]\\) is the context of action, \\(A \\in \\{0,1\\}\\) is the action undertaken, and \\(Y\\in [0,1]\\) is the reward of action \\(A\\) in context \\(W\\). We denote by \\(\\bbO \\equiv [0,1] \\times \\{0,1\\} \\times [0,1]\\) the set where a generic \\(O\\) takes its values. \\(P\\), \\(P_{0}\\), \\(\\Pi_{0}\\), \\(\\Pi_{h}\\), \\(\\Pi_{0}&#39;\\), \\(\\Pi_{h}&#39;\\), laws (on \\(\\bbO\\)) for \\(O\\). \\(Pf \\equiv \\Exp_{P} (f(O))\\) for any law \\(P\\) for \\(O\\) and function \\(f\\) from \\(\\bbO\\) to \\(\\bbR^{p}\\). \\(\\|f\\|_{P}^{2} \\equiv Pf^{2} = \\Exp_{P} (f(O)^{2}) = \\int f(o)^{2} dP(o)\\), the square of the \\(L^{2}(P)\\)-norm of \\(f\\), a function from \\(\\bbO\\) to \\(\\bbR\\). \\(P_{n}\\), the empirical measure. If the observations are \\(O_{1}\\), , \\(O_{n}\\), then \\(P_{n}\\) is a law such that the generic random variable \\(O\\) drawn from \\(P_{n}\\) takes its values in \\(\\{O_{1}, \\ldots, O_{n}\\}\\) in such a way that \\(O = O_{i}\\) with probability \\(n^{-1}\\) for each \\(1 \\leq i \\leq n\\). \\(\\sqrt{n} (P_{n} - P)\\), where \\(P_{n}\\) is the empirical measure associated to \\(O_{1}, \\ldots, O_{n}\\) drawn independently from \\(P\\), the empirical process. \\(\\calM\\), the model, that is, the collection of all laws from which \\(O\\) can be drawn and that meet some constraints. \\(\\calM^{\\text{empirical}}\\), the collection of all discrete laws on \\([0,1] \\times \\{0,1\\} \\times [0,1]\\), of which \\(P_{n}\\) is a distinguished element. \\(Q_{W}\\), \\(Q_{0,W}\\), marginal laws for \\(W\\) (under \\(P\\) and \\(P_{0}\\), respectively). \\(\\Gbar(W) \\equiv \\Pr_{P}(A = 1 | W)\\), \\(\\Gbar_0(W) \\equiv \\Pr_{P_0}(A = 1 | W)\\), conditional probabilities of action \\(A = 1\\) given \\(W\\) (under \\(P\\) and \\(P_{0}\\), respectively). For each \\(a \\in \\{0,1\\}\\), \\(\\ell\\Gbar(a,W) \\equiv \\Pr_{P}(A = a | W)\\) and \\(\\ell\\Gbar_0(a,W) \\equiv \\Pr_{P_0}(A = a | W)\\). \\(\\Qbar(A,W) = \\Exp_{P}(Y|A,W)\\), \\(\\Qbar_0(A,W) = \\Exp_{P_{0}}(Y|A,W)\\), the conditional means of \\(Y\\) given \\(A\\) and \\(W\\) (under \\(P\\) and \\(P_{0}\\), respectively). \\(q_{Y}\\), \\(q_{0,Y}\\), conditional densities of \\(Y\\) given \\(A\\) and \\(W\\) (under \\(P\\) and \\(P_{0}\\), respectively). \\(\\Psi : \\calM \\to [0,1]\\), given by \\(\\Psi(P) \\equiv \\int \\left(\\Qbar(1, w) - \\Qbar(0, w)\\right)dQ_{W}(w)\\), the statistical mapping of interest. \\(\\psi \\equiv \\Psi(P)\\), \\(\\psi_{0} \\equiv \\Psi(P_{0})\\). \\(\\Algo\\), \\(\\Algo_{\\Gbar,1}\\), \\(\\Algo_{\\Qbar,1}\\), algorithms to be trained on \\(P_{n}\\), i.e., mappings from \\(\\calM^{\\text{empirical}}\\) to the set where lives the feature targeted by the algorithm. "],
["11-proofs.html", "Section 11 Basic results and their proofs 11.1 NPSEM 11.2 Identification 11.3 Building a confidence interval 11.4 Another representation of the parameter of interest 11.5 The delta-method 11.6 Asymptotic negligibility of the remainder term", " Section 11 Basic results and their proofs 11.1 NPSEM The experiment can also be summarized by a nonparametric system of structural equations: for some deterministic functions \\(f_w\\), \\(f_a\\), \\(f_y\\) and independent sources of randomness \\(U_w\\), \\(U_a\\), \\(U_y\\), sample the context where the counterfactual rewards will be generated, the action will be undertaken and the actual reward will be obtained, \\(W = f_{w}(U_w)\\); sample the two counterfactual rewards of the two actions that can be undertaken, \\(Y_{0} = f_{y}(0, W, U_y)\\) and \\(Y_{1} = f_{y}(1, W, U_y)\\); sample which action is carried out in the given context, \\(A = f_{a} (W, U_a)\\); define the corresponding reward, \\(Y = A Y_{1} + (1-A) Y_{0}\\); summarize the course of the experiment with the observation \\(O = (W, A, Y)\\), thus concealing \\(Y_{0}\\) and \\(Y_{1}\\). 11.2 Identification Let \\(\\bbP_{0}\\) be an experiment that generates \\(\\bbO \\equiv (W, Y_{0}, Y_{1}, A, Y)\\). We think of \\(W\\) as the context where an action is undertaken, of \\(Y_{0}\\) and \\(Y_{1}\\) as the counterfactual (potential) rewards that actions \\(a=0\\) and \\(a=1\\) would entail, of \\(A\\) as the action carried out, and of \\(Y\\) as the reward received in response to action \\(A\\). Consider the following assumptions: Randomization: under \\(\\bbP_{0}\\), the counterfactual rewards \\((Y_0,Y_1)\\) and action \\(A\\) are conditionally independent given \\(W\\), i.e., \\((Y_0,Y_1) \\perp A \\mid W\\). Consistency: under \\(\\bbP_{0}\\), if action \\(A\\) is undertaken then reward \\(Y_{A}\\) is received, i.e., \\(Y = Y_{A}\\) (or \\(Y=Y_{a}\\) given that \\(A=a\\)). Positivity: under \\(\\bbP_{0}\\), both actions \\(a=0\\) and \\(a=1\\) have (\\(\\bbP_{0}\\)-almost surely) a positive probability to be undertaken given \\(W\\), i.e., \\(\\Pr_{\\bbP_0}(\\ell\\Gbar_0(a,W) &gt; 0) = 1\\) for \\(a=0,1\\). Proposition 11.1 (Identification) Under the above assumptions, it holds that \\[\\begin{equation*} \\psi_{0} = \\Exp_{\\bbP_{0}} \\left(Y_{1} - Y_{0}\\right) = \\Exp_{\\bbP_{0}}(Y_1) - \\Exp_{\\bbP_{0}}(Y_0). \\end{equation*}\\] Proof. Set arbitrarily \\(a \\in \\{0,1\\}\\). By the randomization assumption on the one hand (second equality) and by the consistency and positivity assumptions on the other hand (third equality), it holds that \\[\\begin{align*} \\Exp_{\\bbP_0}(Y_a) &amp;= \\int \\Exp_{\\bbP_0}(Y_a \\mid W = w) dQ_{0,W}(w) = \\int \\Exp_{\\bbP_0}(Y_a \\mid A = a, W = w) dQ_{0,W}(w) \\\\ &amp;= \\int \\Exp_{P_0}(Y \\mid A = a, W = w) dQ_{0,W}(w) = \\int \\Qbar_0(a,W) dQ_{0,W}(w). \\end{align*}\\] The stated result easily follows. Remark. The positivity assumption is needed for \\(\\Exp_{P_0}(Y \\mid A = a, W) \\equiv \\Qbar_{0}(a,W)\\) to be well-defined. 11.3 Building a confidence interval Let \\(\\Phi\\) be the standard normal distribution function. Let \\(X_{1}\\), \\(\\ldots\\), \\(X_{n}\\) be independently drawn from a given law. 11.3.1 CLT &amp; Slutsky’s lemma Assume that \\(\\sigma^{2} \\equiv \\Var(X_{1})\\) is finite. Let \\(m \\equiv \\Exp(X_{1})\\) be the mean of \\(X_{1}\\) and \\(\\bar{X}_{n} \\equiv n^{-1} \\sum_{i=1}^{n} X_{i}\\) be the empirical mean. By the central limit theorem (CLT), it holds that \\(\\sqrt{n} (\\bar{X}_{n} - m)\\) converges in law as \\(n\\) grows to the centered Gaussian law with variance \\(\\sigma^{2}\\). Moreover, if \\(\\sigma_{n}^{2}\\) is a (positive) consistent estimator of \\(\\sigma^{2}\\) then, by Slutsky’s lemma, \\(\\sqrt{n}/\\sigma_{n} (\\bar{X}_{n} - m)\\) converges in law to the standard normal law. The empirical variance \\(n^{-1} \\sum_{i=1}^{n}(X_{i} - \\bar{X}_{n})^{2}\\) is such an estimator. Proposition 11.2 Under the above assumptions, \\[\\begin{equation*} \\left[\\bar{X}_{n} \\pm \\Phi^{-1}(1-\\alpha) \\frac{\\sigma_{n}}{\\sqrt{n}}\\right] \\end{equation*}\\] is a confidence interval for \\(m\\) with asymptotic level \\((1-2\\alpha)\\). 11.3.2 CLT and order statistics Suppose that the law of \\(X_{1}\\) admits a continuous distribution function \\(F\\). Set \\(p \\in ]0,1[\\) and, assuming that \\(n\\) is large, find \\(k\\geq 1\\) and \\(l \\geq 1\\) such that \\[\\begin{equation*} \\frac{k}{n} \\approx p - \\Phi^{-1}(1-\\alpha) \\sqrt{\\frac{p(1-p)}{n}} \\end{equation*}\\] and \\[\\begin{equation*} \\frac{l}{n} \\approx p + \\Phi^{-1}(1-\\alpha) \\sqrt{\\frac{p(1-p)}{n}}. \\end{equation*}\\] Proposition 11.3 Under the above assumptions, \\([X_{(k)},X_{(l)}]\\) is a confidence interval for \\(F^{-1}(p)\\) with asymptotic level \\(1 - 2\\alpha\\). 11.4 Another representation of the parameter of interest For notational simplicitiy, note that \\((2a-1)\\) equals 1 if \\(a=1\\) and \\(-1\\) if \\(a=0\\). Now, for each \\(a = 0,1\\), \\[\\begin{align*} \\Exp_{P_{0}}\\left(\\frac{\\one\\{A = a\\}Y}{\\ell\\Gbar_{0}(a,W)}\\right) &amp;= \\Exp_{P_{0}}\\left(\\Exp_{P_{0}}\\left(\\frac{\\one\\{A = a\\}Y}{\\ell\\Gbar_{0}(a,W)} \\middle| A, W \\right) \\right) \\\\ &amp;= \\Exp_{P_{0}}\\left(\\frac{\\one\\{A = a\\}}{\\ell\\Gbar_{0}(a,W)} \\Qbar_{0}(A, W) \\right) \\\\ &amp;= \\Exp_{P_{0}}\\left(\\frac{\\one\\{A = a\\}}{\\ell\\Gbar_{0}(a,W)} \\Qbar_{0}(a, W)\\right) \\\\ &amp;= \\Exp_{P_{0}}\\left(\\Exp_{P_{0}}\\left(\\frac{\\one\\{A = a\\}}{\\ell\\Gbar_{0}(a,W)} \\Qbar_{0}(a, W) \\middle| W \\right) \\right) \\\\&amp; = \\Exp_{P_{0}}\\left(\\frac{\\ell\\Gbar_{0}(a,W)}{\\ell\\Gbar_{0}(a,W)} \\Qbar_{0}(a, W) \\middle| W \\right) \\\\&amp; = \\Exp_{P_{0}} \\left( \\Qbar_{0}(a, W) \\right), \\end{align*}\\] where the first, fourth and sixth equalities follow from the tower rule18, and the second and fifth hold by definition of the conditional expectation. This completes the proof. 11.5 The delta-method Let \\(f\\) be a map from \\(\\Theta \\subset \\bbR^{p}\\) to \\(\\bbR^{q}\\) that is differentiable at \\(\\theta\\in \\Theta\\). Let \\(X_{n}\\) be a random vector taking its values in \\(\\Theta\\). Proposition 11.4 If \\(\\sqrt{n} (X_{n} - \\theta)\\) converges in law to the Gaussian law with mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\), then \\(\\sqrt{n} (f(X_{n}) - f(\\theta))\\) converge in law to the Gaussian law with mean \\(\\nabla f(\\theta) \\times \\mu\\) and covariance matrix \\(\\nabla f(\\theta) \\times \\Sigma \\times \\nabla f(\\theta)^{\\top}\\). In addition, if \\(\\Sigma_{n}\\) estimates \\(\\Sigma\\) consistently then, by Slutsky’s lemma, the asymptotic variance of \\(\\sqrt{n} (f(X_{n}) - f(\\theta))\\) is consistently estimated with \\(\\nabla f(X_{n}) \\times \\Sigma_{n} \\times \\nabla f(X_{n})^{\\top}\\). 11.6 Asymptotic negligibility of the remainder term Recall that \\(\\|f\\|_{P}^{2} \\equiv \\Exp_{P} \\left( f(O)^{2} \\right)\\) is the \\(L_2(P)\\)-norm of \\(f\\), a measurable function from \\(\\calO\\) to \\(\\bbR\\). Assume that for \\(a= 0,1\\), \\(\\ell\\Gbar_{n}(a,W) \\geq \\delta &gt; 0\\) \\(Q_{0,W}\\)-almost everywhere. The Cauchy-Schwarz inequality then implies that, for \\(a = 0,1\\), \\[\\begin{equation*}\\Rem_{P_0}(\\hat{P}_n) \\le \\frac{2}{\\delta} \\max_{a=0,1} \\left( \\|\\Qbar_n (a,\\cdot) - \\Qbar_0 (a,\\cdot)\\|_{P_0} \\right) \\times \\|\\Gbar_n - \\Gbar_0\\|_{P_0}.\\end{equation*}\\] Therefore, if for \\(a=0,1\\), \\[\\begin{equation*}\\|\\Qbar_n(a,\\cdot) - \\Qbar_0(a,\\cdot)\\|_{P_0} = o_{P_0}(n^{-1/4})\\end{equation*}\\] and \\[\\begin{equation*}\\|\\Gbar_n - \\Gbar_0\\|_{P_0} = o_{P_0}(n^{-1/4}),\\end{equation*}\\] then \\[\\begin{equation*}\\Rem_{P_0}(\\hat{P}_n) = o_{P_0}(n^{-1/2}).\\end{equation*}\\] For any random variable \\((U,V)\\) such that \\(\\Exp(U|V)\\) and \\(\\Exp(U)\\) are well defined, it holds that \\(\\Exp(\\Exp(U|V)) = \\Exp(U)\\).↩ "],
["12-references.html", "Section 12 References", " Section 12 References "],
["13-more-proofs.html", "Section 13 More results and their proofs 13.1 Estimation of the asymptotic variance of an estimator", " Section 13 More results and their proofs Written too quickly. Checks needed! 13.1 Estimation of the asymptotic variance of an estimator 13.1.1 IPTW estimator based on a well-specified model Sketch (to extend later on) The IPTW estimator \\(\\psi_{n}^{b}\\) relies on algorithm \\(\\Algo_{\\Gbar,1}\\), which is “well-specified” in the sense that its output \\(\\Gbar_{n}\\equiv \\Algo_{\\Gbar,1}(P_{n})\\) minimizes the empirical risk over a finite-dimensional, identifiable, well-specified working model for \\(\\Gbar_{0}\\). If one introduces \\(D\\) given by \\[\\begin{equation*} D(O) \\equiv \\frac{(2A-1)}{\\ell\\Gbar_{0}(A,W)} Y, \\end{equation*}\\] then the influence curve of \\(\\psi_{n}^{b}\\) equals \\(D - \\Psi(P_{0})\\) minus the projection of \\(D\\) onto the tangent space of the above parametric model for \\(\\Gbar_{0}\\). The variance of the influence curve is thus smaller than that of \\(D\\), hence the conservativeness. 13.1.2 G-computation estimator based on a well-specified model Sketch (to extend later on, see (Laan and Rose 2011) page 527) Consider a G-computation estimator \\(\\psi_{n}\\) that relies on an algorithm \\(\\Algo_{\\Qbar}\\) that is “well-specified” in the sense that its output \\(\\Qbar_{n}\\equiv \\Algo_{\\Qbar}(P_{n})\\) minimizes the empirical risk over a finite-dimensional, identifiable, well-specified working model for \\(\\Qbar_{0}\\). If one introduces \\(D\\) given by \\[\\begin{equation*} D(O) \\equiv \\Qbar_{0}(1,W) - \\Qbar_{0}(0,W) \\end{equation*}\\] then the influence curve of \\(\\psi_{n}\\) equals \\(D - \\Psi(P_{0})\\) plus a function of \\(O\\) that is orthogonal to \\(D - \\Psi(P_{0})\\). Thus the variance of the influence curve is larger than that of \\(D\\), hence the anti-conservativeness. "],
["14-references-1.html", "Section 14 References", " Section 14 References "]
]
